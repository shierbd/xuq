"""
Phase 5: Tokenæå–é¡µé¢
"""
import streamlit as st
import subprocess
import sys
from pathlib import Path
import pandas as pd

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from storage.repository import TokenRepository, PhraseRepository


def render():
    st.markdown('<div class="main-header">ğŸ·ï¸ Phase 5: Tokenæå–</div>', unsafe_allow_html=True)

    st.markdown("""
    ### åŠŸèƒ½è¯´æ˜

    ä»çŸ­è¯­ä¸­æå–é«˜é¢‘å…³é”®è¯ï¼ˆtokensï¼‰ï¼Œå¹¶ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†ç±»ã€‚

    **ç›®çš„**: å»ºç«‹éœ€æ±‚æ¡†æ¶è¯åº“ï¼Œç”¨äºéœ€æ±‚åˆ†æå’ŒSEO

    **åˆ†ç±»æ ‡å‡†**:
    - **intent**: æ„å›¾è¯ï¼ˆbest, top, how to, cheap, freeï¼‰
    - **action**: åŠ¨ä½œè¯ï¼ˆdownload, buy, make, create, installï¼‰
    - **object**: å¯¹è±¡è¯ï¼ˆshoes, phone, tutorial, recipeï¼‰
    - **other**: å…¶ä»–ï¼ˆå“ç‰Œåã€åœ°åã€æ•°å­—ç­‰ï¼‰

    **è¾“å‡º**: 500-2000ä¸ªåˆ†ç±»tokens
    """)

    st.markdown("---")

    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ğŸ“Š å½“å‰çŠ¶æ€")

        try:
            with TokenRepository() as token_repo:
                all_tokens = token_repo.get_all_tokens()
                st.metric("Tokenæ€»æ•°", len(all_tokens))

                if all_tokens:
                    by_type = {}
                    verified_count = 0
                    for t in all_tokens:
                        by_type[t.token_type] = by_type.get(t.token_type, 0) + 1
                        if t.verified:
                            verified_count += 1

                    st.metric("å·²éªŒè¯", verified_count)

                    st.markdown("**æŒ‰ç±»å‹åˆ†å¸ƒ:**")
                    for ttype, count in sorted(by_type.items(), key=lambda x: x[1], reverse=True):
                        st.text(f"  {ttype}: {count}")

            with PhraseRepository() as phrase_repo:
                stats = phrase_repo.get_statistics()
                st.metric("çŸ­è¯­æ€»æ•°", f"{stats.get('total_count', 0):,}")

        except Exception as e:
            st.error(f"æ— æ³•è·å–çŠ¶æ€: {str(e)}")

    with col2:
        st.markdown("### âš™ï¸ æå–å‚æ•°")

        sample_size = st.number_input(
            "é‡‡æ ·çŸ­è¯­æ•°é‡",
            min_value=0,
            max_value=100000,
            value=10000,
            step=1000,
            help="0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨çŸ­è¯­ï¼Œå»ºè®®é¦–æ¬¡ä½¿ç”¨10000"
        )

        min_frequency = st.slider(
            "æœ€å°è¯é¢‘",
            min_value=1,
            max_value=20,
            value=3,
            help="tokenåœ¨çŸ­è¯­ä¸­è‡³å°‘å‡ºç°çš„æ¬¡æ•°"
        )

        st.markdown("### ğŸ§ª æµ‹è¯•é€‰é¡¹")

        skip_llm = st.checkbox(
            "è·³è¿‡LLMåˆ†ç±»",
            value=False,
            help="ä»…æå–tokensï¼Œä¸è¿›è¡Œåˆ†ç±»"
        )

        round_id = st.number_input(
            "Round ID",
            min_value=1,
            value=1,
            help="æ•°æ®è½®æ¬¡æ ‡è¯†"
        )

    st.markdown("---")

    # é¢„æœŸç»“æœæç¤º
    st.markdown("### ğŸ¯ é¢„æœŸç»“æœ")

    try:
        with PhraseRepository() as phrase_repo:
            stats = phrase_repo.get_statistics()
            total_phrases = stats.get('total_count', 0)

            actual_sample = sample_size if sample_size > 0 else total_phrases

            # ä¼°ç®—tokenæ•°é‡
            estimated_tokens = actual_sample * 3  # æ¯ä¸ªçŸ­è¯­çº¦3ä¸ªè¯
            estimated_unique = estimated_tokens // 10  # å»é‡åçº¦10%
            estimated_after_filter = estimated_unique // 3  # é¢‘æ¬¡è¿‡æ»¤åçº¦1/3

            col1, col2, col3 = st.columns(3)
            with col1:
                st.info(f"**é‡‡æ ·çŸ­è¯­**: {actual_sample:,}")
            with col2:
                st.info(f"**é¢„è®¡tokens**: {estimated_after_filter}")
            with col3:
                if not skip_llm:
                    estimated_batches = estimated_after_filter // 50 + 1
                    estimated_cost = estimated_batches * 0.002  # æ¯æ‰¹$0.002
                    st.warning(f"**é¢„è®¡æˆæœ¬**: ${estimated_cost:.2f}")
                else:
                    st.success("**æˆæœ¬**: $0ï¼ˆè·³è¿‡LLMï¼‰")

    except Exception as e:
        st.error(f"æ— æ³•ä¼°ç®—: {str(e)}")

    st.markdown("---")

    # æ“ä½œæŒ‰é’®
    col1, col2, col3 = st.columns([1, 1, 2])

    with col1:
        start_button = st.button("ğŸš€ å¼€å§‹æå–", type="primary", use_container_width=True)

    with col2:
        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", use_container_width=True):
            st.rerun()

    # æ‰§è¡ŒPhase 5
    if start_button:
        st.markdown("### ğŸ“ æ‰§è¡Œæ—¥å¿—")

        # æ„å»ºå‘½ä»¤
        script_path = project_root / "scripts" / "run_phase5_tokens.py"

        cmd = [
            sys.executable,
            str(script_path),
            f"--sample-size={sample_size}",
            f"--min-frequency={min_frequency}",
            f"--round-id={round_id}"
        ]

        if skip_llm:
            cmd.append("--skip-llm")

        # æ˜¾ç¤ºå‘½ä»¤
        st.code(" ".join(cmd), language="bash")

        # æ‰§è¡Œ
        with st.spinner("æ­£åœ¨æå–tokens..."):
            try:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,
                    universal_newlines=True
                )

                # åˆ›å»ºæ—¥å¿—è¾“å‡ºåŒºåŸŸ
                log_container = st.empty()
                log_lines = []

                # å®æ—¶è¯»å–è¾“å‡º
                for line in process.stdout:
                    log_lines.append(line.strip())
                    log_container.text_area(
                        "è¾“å‡ºæ—¥å¿—",
                        "\n".join(log_lines[-50:]),
                        height=400
                    )

                process.wait()

                if process.returncode == 0:
                    st.success("âœ… Phase 5 å®Œæˆï¼")
                    st.balloons()

                    # æ˜¾ç¤ºç»“æœç»Ÿè®¡
                    with TokenRepository() as token_repo:
                        all_tokens = token_repo.get_all_tokens()
                        st.metric("æå–Tokenæ•°", len(all_tokens))

                        if all_tokens:
                            by_type = {}
                            for t in all_tokens:
                                by_type[t.token_type] = by_type.get(t.token_type, 0) + 1

                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("intent", by_type.get('intent', 0))
                            with col2:
                                st.metric("action", by_type.get('action', 0))
                            with col3:
                                st.metric("object", by_type.get('object', 0))
                            with col4:
                                st.metric("other", by_type.get('other', 0))

                    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                    output_file = project_root / "data" / "output" / "tokens_extracted.csv"
                    if output_file.exists():
                        st.info(f"ğŸ“„ Token CSV: {output_file}")
                        st.markdown("**ä¸‹ä¸€æ­¥**: æ‰“å¼€CSVè¿›è¡Œäººå·¥å®¡æ ¸")
                else:
                    st.error(f"âŒ æ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºä»£ç : {process.returncode}")

            except Exception as e:
                st.error(f"âŒ æ‰§è¡Œå‡ºé”™: {str(e)}")

    st.markdown("---")

    # æŸ¥çœ‹å·²æå–çš„tokens
    st.markdown("## ğŸ“‹ å·²æå–çš„Tokens")

    try:
        with TokenRepository() as token_repo:
            all_tokens = token_repo.get_all_tokens()

            if all_tokens:
                st.markdown(f"**æ‰¾åˆ° {len(all_tokens)} ä¸ªtokens**")

                # æ˜¾ç¤ºtokenè¡¨æ ¼
                token_data = []
                for t in all_tokens:
                    token_data.append({
                        "Token": t.token_text,
                        "ç±»å‹": t.token_type,
                        "é¢‘æ¬¡": t.in_phrase_count,
                        "å·²éªŒè¯": "âœ…" if t.verified else "âŒ"
                    })

                df = pd.DataFrame(token_data)

                # ç­›é€‰å™¨
                col1, col2, col3 = st.columns(3)

                with col1:
                    type_filter = st.multiselect(
                        "æŒ‰ç±»å‹ç­›é€‰",
                        options=["å…¨éƒ¨"] + list(df["ç±»å‹"].unique()),
                        default=["å…¨éƒ¨"]
                    )

                with col2:
                    verified_filter = st.selectbox(
                        "æŒ‰éªŒè¯çŠ¶æ€ç­›é€‰",
                        options=["å…¨éƒ¨", "å·²éªŒè¯", "æœªéªŒè¯"]
                    )

                with col3:
                    min_freq_filter = st.number_input(
                        "æœ€å°é¢‘æ¬¡",
                        min_value=0,
                        value=0
                    )

                # åº”ç”¨ç­›é€‰
                filtered_df = df.copy()

                if "å…¨éƒ¨" not in type_filter:
                    filtered_df = filtered_df[filtered_df["ç±»å‹"].isin(type_filter)]

                if verified_filter == "å·²éªŒè¯":
                    filtered_df = filtered_df[filtered_df["å·²éªŒè¯"] == "âœ…"]
                elif verified_filter == "æœªéªŒè¯":
                    filtered_df = filtered_df[filtered_df["å·²éªŒè¯"] == "âŒ"]

                if min_freq_filter > 0:
                    filtered_df = filtered_df[filtered_df["é¢‘æ¬¡"] >= min_freq_filter]

                # æ’åº
                sort_by = st.selectbox(
                    "æ’åºæ–¹å¼",
                    ["æŒ‰é¢‘æ¬¡é™åº", "æŒ‰Tokenå­—æ¯åº", "æŒ‰ç±»å‹"]
                )

                if sort_by == "æŒ‰é¢‘æ¬¡é™åº":
                    filtered_df = filtered_df.sort_values("é¢‘æ¬¡", ascending=False)
                elif sort_by == "æŒ‰Tokenå­—æ¯åº":
                    filtered_df = filtered_df.sort_values("Token")
                elif sort_by == "æŒ‰ç±»å‹":
                    filtered_df = filtered_df.sort_values("ç±»å‹")

                # æ˜¾ç¤ºè¡¨æ ¼
                st.dataframe(
                    filtered_df,
                    use_container_width=True,
                    height=400
                )

                # è¯äº‘
                if st.checkbox("æ˜¾ç¤ºè¯äº‘"):
                    try:
                        import matplotlib.pyplot as plt
                        from wordcloud import WordCloud

                        # å‡†å¤‡è¯é¢‘æ•°æ®
                        word_freq = {row["Token"]: row["é¢‘æ¬¡"] for _, row in filtered_df.iterrows()}

                        # ç”Ÿæˆè¯äº‘
                        wordcloud = WordCloud(
                            width=800,
                            height=400,
                            background_color='white',
                            colormap='viridis'
                        ).generate_from_frequencies(word_freq)

                        # æ˜¾ç¤º
                        fig, ax = plt.subplots(figsize=(10, 5))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)

                    except ImportError:
                        st.warning("âš ï¸ éœ€è¦å®‰è£… wordcloud å’Œ matplotlib: pip install wordcloud matplotlib")

            else:
                st.info("â„¹ï¸ è¿˜æ²¡æœ‰æå–tokensï¼Œè¯·å…ˆè¿è¡Œ Phase 5")

    except Exception as e:
        st.error(f"âŒ åŠ è½½tokenså¤±è´¥: {str(e)}")

    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### Phase 5 æµç¨‹

        1. **åŠ è½½çŸ­è¯­**: ä»æ•°æ®åº“åŠ è½½çŸ­è¯­ï¼ˆå¯é‡‡æ ·ï¼‰
        2. **æå–tokens**: åˆ†è¯ã€æ¸…ç†ã€ç»Ÿè®¡é¢‘æ¬¡
        3. **åœç”¨è¯è¿‡æ»¤**: ç§»é™¤å¸¸è§åœç”¨è¯ï¼ˆ~70ä¸ªï¼‰
        4. **æå–bigrams**: è¯†åˆ«å¸¸è§äºŒå…ƒè¯ç»„
        5. **LLMåˆ†ç±»**: æ‰¹é‡åˆ†ç±»ï¼ˆ50ä¸ª/æ‰¹ï¼‰
        6. **ä¿å­˜ç»“æœ**: æ›´æ–°æ•°æ®åº“å’Œå¯¼å‡ºCSV

        ### å‚æ•°è¯´æ˜

        **é‡‡æ ·çŸ­è¯­æ•°é‡**:
        - 0 = ä½¿ç”¨å…¨éƒ¨çŸ­è¯­
        - 10000 = é‡‡æ ·1ä¸‡æ¡ï¼ˆæ¨èé¦–æ¬¡ä½¿ç”¨ï¼‰
        - å½±å“tokensæ•°é‡å’Œæˆæœ¬

        **æœ€å°è¯é¢‘**:
        - é»˜è®¤: 3
        - å¢å¤§ â†’ tokensæ›´å°‘ï¼Œä½†æ›´æœ‰ä»£è¡¨æ€§
        - å‡å° â†’ tokensæ›´å¤šï¼ŒåŒ…å«é•¿å°¾è¯

        ### åœç”¨è¯åˆ—è¡¨

        ç³»ç»Ÿä¼šè‡ªåŠ¨è¿‡æ»¤ä»¥ä¸‹ç±»å‹çš„è¯ï¼š
        - å† è¯: a, an, the
        - ä»‹è¯: in, on, at, by, for, with
        - è¿è¯: and, or, but
        - ä»£è¯: i, you, he, she, it
        - åŠ©è¯: is, are, was, were, do, does

        ### äººå·¥å®¡æ ¸

        1. æ‰“å¼€ `data/output/tokens_extracted.csv`
        2. æ£€æŸ¥ `token_type` æ˜¯å¦æ­£ç¡®
        3. ä¿®æ”¹é”™è¯¯çš„åˆ†ç±»
        4. æ ‡è®° `verified=TRUE` è¡¨ç¤ºå·²å®¡æ ¸

        ### è¾“å‡ºæ–‡ä»¶

        - `data/output/tokens_extracted.csv` - Token CSV
        - `data/output/phase5_tokens_report.txt` - ç»Ÿè®¡æŠ¥å‘Š
        """)

    # æ•…éšœæ’æŸ¥
    with st.expander("ğŸ”§ æ•…éšœæ’æŸ¥"):
        st.markdown("""
        ### å¸¸è§é—®é¢˜

        **Q: æå–çš„tokensæ•°é‡å¤ªå°‘**
        - å‡å°"æœ€å°è¯é¢‘"ï¼ˆä»3å‡åˆ°2ï¼‰
        - å¢å¤§"é‡‡æ ·çŸ­è¯­æ•°é‡"
        - æ£€æŸ¥æ•°æ®åº“ä¸­çš„çŸ­è¯­æ•°é‡

        **Q: æå–çš„tokensæ•°é‡å¤ªå¤š**
        - å¢å¤§"æœ€å°è¯é¢‘"ï¼ˆä»3å¢åˆ°5æˆ–æ›´é«˜ï¼‰
        - æ£€æŸ¥åœç”¨è¯åˆ—è¡¨æ˜¯å¦å®Œæ•´

        **Q: LLMåˆ†ç±»é”™è¯¯ç‡é«˜**
        - è¿™æ˜¯æ­£å¸¸çš„ï¼ŒAIåˆ†ç±»å‡†ç¡®ç‡çº¦70-80%
        - éœ€è¦äººå·¥å®¡æ ¸å’Œä¿®æ­£
        - è€ƒè™‘è°ƒæ•´Promptï¼ˆåœ¨ ai/client.pyï¼‰

        **Q: é‡å¤è¿è¡Œä¼šé‡å¤æå–å—**
        - ä¸ä¼šï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å»é‡
        - å·²å­˜åœ¨çš„tokensä¸ä¼šé‡å¤æ’å…¥
        - å¯ä»¥å®‰å…¨åœ°å¤šæ¬¡è¿è¡Œ

        **Q: å†…å­˜ä¸è¶³**
        - å‡å°é‡‡æ ·æ•°é‡ï¼ˆä»10000å‡åˆ°5000ï¼‰
        - ä½¿ç”¨æ›´å°çš„batch_sizeï¼ˆåœ¨ä»£ç ä¸­è°ƒæ•´ï¼‰

        **Q: æŸäº›é‡è¦è¯è¢«è¿‡æ»¤**
        - æ£€æŸ¥åœç”¨è¯åˆ—è¡¨ï¼ˆåœ¨ utils/token_extractor.pyï¼‰
        - å¯ä»¥è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨
        - å‡å°æœ€å°è¯é¢‘
        """)
