"""
Phase 0: å…³é”®è¯æ‰©å±•ä¸åˆ†è¯å·¥å…·
ä»æ•°æ®åº“è¯»å–å·²å¯¼å…¥çš„å…³é”®è¯è¿›è¡Œåˆ†è¯ã€åœç”¨è¯ç®¡ç†å’Œé¢‘æ¬¡åˆ†æ
"""
import streamlit as st
import pandas as pd
from pathlib import Path
import sys
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from utils.keyword_segmentation import (
    segment_keywords,
    segment_keywords_with_seed_tracking,
    get_sorted_words,
    clean_keywords,
    get_statistics
)
from utils.stopwords import (
    load_stopwords,
    save_stopwords,
    add_stopword,
    remove_stopword,
    reset_to_default,
    get_stopwords_info
)
from utils.translation import translate_words_batch, TRANSLATION_AVAILABLE
from utils.pos_tagging import (
    tag_words_batch,
    get_pos_statistics,
    get_available_categories,
    POS_TAGGING_AVAILABLE
)
from storage.repository import PhraseRepository, SeedWordRepository
from storage.word_segment_repository import WordSegmentRepository
from ui.components.seed_word_tracking import render_seed_word_tracking
from utils.user_preferences import (
    load_phase0_preferences,
    update_phase0_preference,
    get_preferences_manager
)


# é…ç½®
STOPWORDS_FILE = PROJECT_ROOT / "config" / "stopwords_en.txt"


def main():
    st.title("ğŸ“ Phase 0: å…³é”®è¯æ‰©å±•ä¸åˆ†è¯å·¥å…·")

    # åˆ›å»ºTabå¯¼èˆª
    tab1, tab2 = st.tabs(["ğŸŒ± è¯æ ¹è¿½æº¯", "âœ‚ï¸ åˆ†è¯ä¸ç­›é€‰"])

    # ========== Tab 1: è¯æ ¹è¿½æº¯ ==========
    with tab1:
        render_seed_word_tracking()

    # ========== Tab 2: åˆ†è¯ä¸ç­›é€‰ ==========
    with tab2:
        render_segmentation_tab()


def render_segmentation_tab():
    """æ¸²æŸ“åˆ†è¯ä¸ç­›é€‰tabçš„å†…å®¹"""
    st.markdown("""
    ---
    **ä½¿ç”¨è¯´æ˜**:

    æœ¬å·¥å…·ç”¨äºå…³é”®è¯æ‰©å±•çš„è¿­ä»£å¾ªç¯ä¸­ï¼Œå·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
    1. **Phase 1: æ•°æ®å¯¼å…¥** - å¯¼å…¥åˆå§‹å…³é”®è¯CSV
    2. **Phase 0: å…³é”®è¯æ‰©å±•**ï¼ˆæœ¬é¡µé¢ï¼‰- åˆ†è¯ã€ç­›é€‰é«˜é¢‘è¯
    3. **æ‰‹åŠ¨æ“ä½œ** - ä½¿ç”¨é«˜é¢‘è¯å»å¤–éƒ¨å·¥å…·ï¼ˆGoogle Autocompleteã€ç›¸å…³æœç´¢ç­‰ï¼‰è·å–æ‰©å±•è¯
    4. **Phase 1: æ•°æ®å¯¼å…¥** - å¯¼å…¥æ‰©å±•åçš„å…³é”®è¯
    5. **Phase 0: å…³é”®è¯æ‰©å±•** - å†æ¬¡åˆ†è¯ç­›é€‰
    6. é‡å¤æ­¥éª¤3-5ï¼Œç›´åˆ°æ»¡æ„
    7. **Phase 2: å¤§ç»„èšç±»** - å¼€å§‹æ­£å¼çš„èšç±»åˆ†æ

    **æ ¸å¿ƒåŠŸèƒ½**:
    - ä»æ•°æ®åº“è¯»å–å·²å¯¼å…¥çš„å…³é”®è¯è¿›è¡Œåˆ†è¯
    - äº¤äº’å¼åœç”¨è¯ç®¡ç†ï¼ˆæ·»åŠ ã€åˆ é™¤ã€é‡ç½®ï¼‰
    - æ”¯æŒæ’åºã€ç¿»è¯‘ï¼ˆå¯é€‰ï¼‰ã€å¯¼å‡ºï¼ˆHTML/CSV/å¤åˆ¶ï¼‰
    - å•è½®æ¬¡å¤„ç†ï¼Œç”±ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶è¿­ä»£è½®æ¬¡
    - **âœ¨ è‡ªåŠ¨ä¿å­˜é…ç½®**ï¼šæ‰€æœ‰å‚æ•°è®¾ç½®ä¼šè‡ªåŠ¨ä¿å­˜ï¼Œä¸‹æ¬¡æ‰“å¼€æ—¶æ¢å¤
    """)

    # åˆå§‹åŒ– Session Stateï¼ˆå¿…é¡»åœ¨é…ç½®ç®¡ç†åŒºåŸŸä¹‹å‰ï¼‰
    if 'stopwords' not in st.session_state:
        st.session_state.stopwords = load_stopwords(STOPWORDS_FILE)

    if 'word_counter' not in st.session_state:
        st.session_state.word_counter = None

    if 'translations' not in st.session_state:
        st.session_state.translations = {}

    if 'pos_tags' not in st.session_state:
        st.session_state.pos_tags = {}

    if 'selected_words' not in st.session_state:
        st.session_state.selected_words = set()

    if 'word_to_seeds' not in st.session_state:
        st.session_state.word_to_seeds = {}

    if 'keywords_cache' not in st.session_state:
        st.session_state.keywords_cache = None

    if 'phrases_cache' not in st.session_state:
        st.session_state.phrases_cache = None

    if 'ngram_counter' not in st.session_state:
        st.session_state.ngram_counter = None

    if 'ngram_to_seeds' not in st.session_state:
        st.session_state.ngram_to_seeds = {}

    if 'ngram_translations' not in st.session_state:
        st.session_state.ngram_translations = {}

    # åŠ è½½ç”¨æˆ·åå¥½è®¾ç½®
    if 'preferences' not in st.session_state:
        st.session_state.preferences = load_phase0_preferences()

    # æ ‡è®°æ˜¯å¦å·²ä»æ•°æ®åº“åŠ è½½åˆ†è¯ç»“æœ
    if 'segmentation_loaded_from_db' not in st.session_state:
        st.session_state.segmentation_loaded_from_db = False

    # ========== è‡ªåŠ¨åŠ è½½å·²æœ‰åˆ†è¯ç»“æœ ==========
    # å¦‚æœè¿˜æ²¡æœ‰åˆ†è¯ç»“æœï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½
    if (st.session_state.word_counter is None and
        not st.session_state.segmentation_loaded_from_db):

        try:
            with WordSegmentRepository() as ws_repo:
                # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦æœ‰åˆ†è¯ç»“æœ
                stats = ws_repo.get_statistics()

                if stats['total_words'] > 0:
                    # åŠ è½½åˆ†è¯ç»“æœï¼ˆä½¿ç”¨é…ç½®çš„æœ€å°é¢‘æ¬¡ï¼‰
                    seg_prefs = st.session_state.preferences.get('segmentation', {})
                    min_freq = seg_prefs.get('min_frequency', 2)
                    min_ngram_freq = seg_prefs.get('min_ngram_frequency', 3)

                    (
                        word_counter,
                        ngram_counter,
                        pos_tags,
                        translations,
                        ngram_translations,
                        latest_batch
                    ) = ws_repo.load_segmentation_results(
                        min_word_frequency=min_freq,
                        min_ngram_frequency=min_ngram_freq
                    )

                    # æ›´æ–°Session State
                    st.session_state.word_counter = word_counter
                    st.session_state.ngram_counter = ngram_counter
                    st.session_state.pos_tags = pos_tags
                    st.session_state.translations = translations
                    st.session_state.ngram_translations = ngram_translations
                    st.session_state.segmentation_loaded_from_db = True

                    # æ˜¾ç¤ºåŠ è½½ä¿¡æ¯
                    st.success(
                        f"âœ… å·²è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡åˆ†è¯ç»“æœï¼š{len(word_counter):,} ä¸ªå•è¯ + "
                        f"{len(ngram_counter):,} ä¸ªçŸ­è¯­"
                    )

                    if latest_batch:
                        batch_date = latest_batch.batch_date.strftime('%Y-%m-%d %H:%M:%S')
                        st.info(f"ğŸ“… ä¸Šæ¬¡åˆ†è¯æ—¶é—´: {batch_date}")

                        # å­˜å‚¨æ‰¹æ¬¡ä¿¡æ¯ç”¨äºåç»­æ¯”è¾ƒ
                        st.session_state.last_batch_phrase_count = latest_batch.phrase_count
                else:
                    # æ ‡è®°å·²æ£€æŸ¥è¿‡ï¼Œé¿å…é‡å¤æ£€æŸ¥
                    st.session_state.segmentation_loaded_from_db = True
        except Exception as e:
            st.warning(f"âš ï¸ åŠ è½½åˆ†è¯ç»“æœå¤±è´¥: {str(e)}")
            st.session_state.segmentation_loaded_from_db = True

    # ========== é…ç½®ç®¡ç†åŒºåŸŸ ==========
    with st.expander("âš™ï¸ é…ç½®ç®¡ç†", expanded=False):
        st.markdown("""
        **é…ç½®è¯´æ˜**ï¼š
        - æ‰€æœ‰å‚æ•°ï¼ˆåˆ†è¯ã€ç­›é€‰ã€ç¿»è¯‘ã€å¯¼å‡ºï¼‰ä¼šè‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°
        - å…³é—­æµè§ˆå™¨åé…ç½®ä»ä¼šä¿ç•™
        - å¯ä»¥æ‰‹åŠ¨é‡ç½®ä¸ºé»˜è®¤å€¼
        """)

        col1, col2 = st.columns([3, 1])
        with col1:
            prefs_manager = get_preferences_manager()
            last_updated = st.session_state.preferences.get('last_updated', 'æœªè®¾ç½®')
            if last_updated != 'æœªè®¾ç½®':
                from datetime import datetime
                try:
                    dt = datetime.fromisoformat(last_updated)
                    last_updated = dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    pass
            st.info(f"ğŸ“… é…ç½®æœ€åæ›´æ–°æ—¶é—´: {last_updated}")

        with col2:
            if st.button("ğŸ”„ é‡ç½®ä¸ºé»˜è®¤é…ç½®"):
                prefs_manager.reset_to_defaults()
                st.session_state.preferences = load_phase0_preferences()
                st.success("âœ“ å·²é‡ç½®ä¸ºé»˜è®¤é…ç½®")
                st.rerun()

    # ========== 1. ä»æ•°æ®åº“åŠ è½½å…³é”®è¯ ==========
    st.header("1ï¸âƒ£ åŠ è½½æ•°æ®åº“å…³é”®è¯")

    # ä½¿ç”¨ç¼“å­˜é¿å…æ¯æ¬¡rerunéƒ½é‡æ–°åŠ è½½
    if st.session_state.keywords_cache is None:
        load_data = True
    else:
        col1, col2 = st.columns([3, 1])
        with col1:
            st.success(f"âœ“ å·²åŠ è½½ {len(st.session_state.keywords_cache)} æ¡å…³é”®è¯ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰")
        with col2:
            if st.button("ğŸ”„ é‡æ–°åŠ è½½", help="ä»æ•°æ®åº“é‡æ–°åŠ è½½æ•°æ®"):
                st.session_state.keywords_cache = None
                st.session_state.word_counter = None
                st.session_state.translations = {}
                st.session_state.pos_tags = {}
                st.session_state.selected_words = set()
                st.session_state.segmentation_loaded_from_db = False  # å…è®¸é‡æ–°åŠ è½½
                st.rerun()
        load_data = False

    if load_data:
        try:
            with PhraseRepository() as repo:
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = repo.get_statistics()
                total_count = stats.get('total_count', 0)

                if total_count == 0:
                    st.warning("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰å…³é”®è¯æ•°æ®ï¼Œè¯·å…ˆåœ¨ Phase 1 ä¸­å¯¼å…¥æ•°æ®")
                    return

                by_source = stats.get('by_source', {})

                # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                col1.metric("æ•°æ®åº“æ€»è¯æ•°", f"{total_count:,}")
                col2.metric("æ•°æ®æ¥æºæ•°", len(by_source))
                col3.metric("çŠ¶æ€", "å·²å°±ç»ª")

                # æ•°æ®æºç­›é€‰
                if by_source:
                    st.subheader("ğŸ“Š æ•°æ®æºç­›é€‰")

                    # æ˜¾ç¤ºå„æ¥æºç»Ÿè®¡
                    source_df = pd.DataFrame([
                        {"æ¥æº": source or "æœªçŸ¥", "æ•°é‡": count}
                        for source, count in by_source.items()
                    ])
                    st.dataframe(source_df, width='stretch')

                    # é€‰æ‹©æ•°æ®æº
                    available_sources = ["å…¨éƒ¨æ•°æ®"] + [s or "æœªçŸ¥" for s in by_source.keys()]
                    selected_source = st.selectbox(
                        "é€‰æ‹©è¦åˆ†è¯çš„æ•°æ®æº",
                        options=available_sources,
                        help="é€‰æ‹©ç‰¹å®šæ¥æºçš„æ•°æ®è¿›è¡Œåˆ†è¯ï¼Œæˆ–é€‰æ‹©'å…¨éƒ¨æ•°æ®'"
                    )

                    # æ ¹æ®é€‰æ‹©åŠ è½½æ•°æ®
                    if selected_source == "å…¨éƒ¨æ•°æ®":
                        # åŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆåˆ†é¡µåŠ è½½ä»¥é¿å…å†…å­˜é—®é¢˜ï¼‰
                        all_phrases = []
                        page = 1
                        page_size = 10000

                        with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                            while True:
                                phrases, total = repo.get_phrases_paginated(page=page, page_size=page_size)
                                if not phrases:
                                    break
                                all_phrases.extend(phrases)
                                page += 1
                                if len(all_phrases) >= total:
                                    break

                        keywords = [p.phrase for p in all_phrases]
                        # åŒæ—¶ç¼“å­˜å®Œæ•´çš„phraseså¯¹è±¡ï¼ˆç”¨äºè·å–seed_wordï¼‰
                        st.session_state.phrases_cache = all_phrases
                        st.info(f"âœ“ å·²é€‰æ‹©å…¨éƒ¨æ•°æ®ï¼Œå…± {len(keywords):,} æ¡å…³é”®è¯")
                    else:
                        # åŠ è½½æŒ‡å®šæ¥æºçš„æ•°æ®
                        source_type = None if selected_source == "æœªçŸ¥" else selected_source
                        all_phrases = []
                        page = 1
                        page_size = 10000

                        with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                            while True:
                                phrases, total = repo.get_phrases_paginated(
                                    page=page,
                                    page_size=page_size,
                                    filters={'source_type': source_type}
                                )
                                if not phrases:
                                    break
                                all_phrases.extend(phrases)
                                page += 1
                                if len(all_phrases) >= total:
                                    break

                        keywords = [p.phrase for p in all_phrases]
                        # åŒæ—¶ç¼“å­˜å®Œæ•´çš„phraseså¯¹è±¡ï¼ˆç”¨äºè·å–seed_wordï¼‰
                        st.session_state.phrases_cache = all_phrases
                        st.info(f"âœ“ å·²é€‰æ‹©æ¥æº '{selected_source}'ï¼Œå…± {len(keywords):,} æ¡å…³é”®è¯")
                else:
                    # æ²¡æœ‰æ¥æºä¿¡æ¯ï¼ŒåŠ è½½æ‰€æœ‰æ•°æ®
                    all_phrases = []
                    page = 1
                    page_size = 10000

                    with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                        while True:
                            phrases, total = repo.get_phrases_paginated(page=page, page_size=page_size)
                            if not phrases:
                                break
                            all_phrases.extend(phrases)
                            page += 1
                            if len(all_phrases) >= total:
                                break

                    keywords = [p.phrase for p in all_phrases]
                    # åŒæ—¶ç¼“å­˜å®Œæ•´çš„phraseså¯¹è±¡ï¼ˆç”¨äºè·å–seed_wordï¼‰
                    st.session_state.phrases_cache = all_phrases
                    st.info(f"ğŸ“Š å…±åŠ è½½ {len(keywords):,} æ¡å…³é”®è¯")

                # ç¼“å­˜åŠ è½½çš„æ•°æ®
                st.session_state.keywords_cache = keywords

        except Exception as e:
            st.error(f"âŒ åŠ è½½æ•°æ®åº“æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            st.error(traceback.format_exc())
            return
    else:
        # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®
        keywords = st.session_state.keywords_cache

    # ========== æ–°æ•°æ®æ£€æµ‹ ==========
    # å¦‚æœå·²æœ‰åˆ†è¯ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®å¯¼å…¥
    if (st.session_state.segmentation_loaded_from_db and
        hasattr(st.session_state, 'last_batch_phrase_count') and
        st.session_state.last_batch_phrase_count is not None):

        current_phrase_count = len(keywords)
        last_phrase_count = st.session_state.last_batch_phrase_count

        if current_phrase_count > last_phrase_count:
            new_count = current_phrase_count - last_phrase_count
            st.warning(f"âš ï¸ æ£€æµ‹åˆ° {new_count:,} æ¡æ–°å…³é”®è¯ï¼Œå»ºè®®é‡æ–°åˆ†è¯ä»¥è·å–æœ€æ–°ç»“æœ")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ å…¨é‡é‡æ–°åˆ†è¯", help="é‡æ–°åˆ†è¯æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬æ—§æ•°æ®å’Œæ–°æ•°æ®ï¼‰"):
                    # æ¸…ç©ºåˆ†è¯ç»“æœï¼Œè§¦å‘é‡æ–°åˆ†è¯
                    st.session_state.word_counter = None
                    st.session_state.ngram_counter = None
                    st.session_state.pos_tags = {}
                    st.session_state.translations = {}
                    st.session_state.ngram_translations = {}
                    st.session_state.segmentation_loaded_from_db = False
                    st.info("âœ“ å·²æ¸…ç©ºåˆ†è¯ç»“æœï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹çš„'å¼€å§‹åˆ†è¯'æŒ‰é’®")
                    st.rerun()

            with col2:
                st.info("ğŸ’¡ **æç¤º**ï¼šå¢é‡åˆ†è¯åŠŸèƒ½å¼€å‘ä¸­ï¼Œç›®å‰è¯·ä½¿ç”¨å…¨é‡é‡æ–°åˆ†è¯")

    # ========== 2. åœç”¨è¯ç®¡ç† ==========
    st.header("2ï¸âƒ£ åœç”¨è¯ç®¡ç†")

    tab1, tab2 = st.tabs(["æŸ¥çœ‹åœç”¨è¯", "ç®¡ç†åœç”¨è¯"])

    with tab1:
        stopwords_info = get_stopwords_info(st.session_state.stopwords)

        col1, col2, col3 = st.columns(3)
        col1.metric("åœç”¨è¯æ•°é‡", stopwords_info['total'])
        col2.metric("æ˜¯å¦ä¸ºé»˜è®¤", "æ˜¯" if stopwords_info['is_default'] else "å¦")

        st.write("**å½“å‰åœç”¨è¯ï¼ˆå‰50ä¸ªï¼‰**:")
        st.code(', '.join(sorted(list(st.session_state.stopwords))[:50]))

    with tab2:
        st.subheader("æ·»åŠ åœç”¨è¯")
        new_word = st.text_input("è¾“å…¥è¦æ·»åŠ çš„åœç”¨è¯ï¼ˆå°å†™ï¼‰")
        if st.button("â• æ·»åŠ "):
            if new_word:
                st.session_state.stopwords = add_stopword(
                    st.session_state.stopwords,
                    new_word
                )
                save_stopwords(st.session_state.stopwords, STOPWORDS_FILE)
                st.success(f"âœ“ å·²æ·»åŠ åœç”¨è¯: {new_word}")
                st.rerun()

        st.subheader("åˆ é™¤åœç”¨è¯")
        words_to_remove = st.multiselect(
            "é€‰æ‹©è¦åˆ é™¤çš„åœç”¨è¯",
            options=sorted(list(st.session_state.stopwords))
        )
        if st.button("â– åˆ é™¤é€‰ä¸­"):
            if words_to_remove:
                for word in words_to_remove:
                    st.session_state.stopwords = remove_stopword(
                        st.session_state.stopwords,
                        word
                    )
                save_stopwords(st.session_state.stopwords, STOPWORDS_FILE)
                st.success(f"âœ“ å·²åˆ é™¤ {len(words_to_remove)} ä¸ªåœç”¨è¯")
                st.rerun()

        st.subheader("é‡ç½®ä¸ºé»˜è®¤")
        if st.button("ğŸ”„ é‡ç½®åœç”¨è¯"):
            st.session_state.stopwords = reset_to_default(STOPWORDS_FILE)
            st.success("âœ“ å·²é‡ç½®ä¸ºé»˜è®¤åœç”¨è¯")
            st.rerun()

    # ========== 3. æ‰§è¡Œåˆ†è¯ ==========
    st.header("3ï¸âƒ£ æ‰§è¡Œåˆ†è¯")

    # æ¸…ç†å…³é”®è¯
    keywords_cleaned = clean_keywords(keywords)
    st.info(f"âœ“ æ¸…ç†åå‰©ä½™ {len(keywords_cleaned)} æ¡å…³é”®è¯")

    # åˆ†è¯é…ç½®ï¼ˆä½¿ç”¨ä¿å­˜çš„åå¥½è®¾ç½®ä½œä¸ºé»˜è®¤å€¼ï¼‰
    seg_prefs = st.session_state.preferences.get('segmentation', {})

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        min_frequency = st.number_input(
            "æœ€å°é¢‘æ¬¡",
            min_value=1,
            value=seg_prefs.get('min_frequency', 2),
            help="åªæ˜¾ç¤ºå‡ºç°æ¬¡æ•° >= æ­¤å€¼çš„è¯",
            key="min_frequency_input"
        )
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
        if min_frequency != seg_prefs.get('min_frequency', 2):
            update_phase0_preference('segmentation', 'min_frequency', min_frequency)
            st.session_state.preferences = load_phase0_preferences()

    with col2:
        sort_by_index = ['frequency', 'alphabetical', 'length'].index(
            seg_prefs.get('sort_by', 'frequency')
        )
        sort_by = st.selectbox(
            "æ’åºæ–¹å¼",
            options=['frequency', 'alphabetical', 'length'],
            index=sort_by_index,
            format_func=lambda x: {
                'frequency': 'æŒ‰é¢‘æ¬¡é™åº',
                'alphabetical': 'æŒ‰å­—æ¯å‡åº',
                'length': 'æŒ‰è¯é•¿åº¦é™åº'
            }[x],
            key="sort_by_select"
        )
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
        if sort_by != seg_prefs.get('sort_by', 'frequency'):
            update_phase0_preference('segmentation', 'sort_by', sort_by)
            st.session_state.preferences = load_phase0_preferences()

    with col3:
        enable_pos_tagging = st.checkbox(
            "å¯ç”¨è¯æ€§æ ‡æ³¨",
            value=seg_prefs.get('enable_pos_tagging', True),
            disabled=not POS_TAGGING_AVAILABLE,
            help="ä½¿ç”¨NLTKè¿›è¡Œè‹±æ–‡è¯æ€§æ ‡æ³¨",
            key="enable_pos_tagging_checkbox"
        )
        if not POS_TAGGING_AVAILABLE:
            st.info("â„¹ï¸ è¯æ€§æ ‡æ³¨éœ€è¦å®‰è£… nltk åº“")
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
        if enable_pos_tagging != seg_prefs.get('enable_pos_tagging', True):
            update_phase0_preference('segmentation', 'enable_pos_tagging', enable_pos_tagging)
            st.session_state.preferences = load_phase0_preferences()

    with col4:
        extract_ngrams = st.checkbox(
            "æå–çŸ­è¯­",
            value=seg_prefs.get('extract_ngrams', True),
            help="æå–é«˜é¢‘çŸ­è¯­ç»„åˆï¼ˆå¦‚ 'best free', 'how to' ç­‰ï¼‰",
            key="extract_ngrams_checkbox"
        )
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
        if extract_ngrams != seg_prefs.get('extract_ngrams', True):
            update_phase0_preference('segmentation', 'extract_ngrams', extract_ngrams)
            st.session_state.preferences = load_phase0_preferences()

    # n-gramé…ç½®ï¼ˆå½“å¯ç”¨çŸ­è¯­æå–æ—¶æ˜¾ç¤ºï¼‰
    if extract_ngrams:
        st.markdown("**çŸ­è¯­æå–é…ç½®**")
        st.info("ğŸ’¡ æç¤ºï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æå–æ‰€æœ‰2-6è¯çš„çŸ­è¯­ç»„åˆï¼ˆæ•°æ®é©±åŠ¨ï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨ç»“æœé¡µé¢ç­›é€‰æ„Ÿå…´è¶£çš„é•¿åº¦")
        min_ngram_frequency = st.number_input(
            "çŸ­è¯­æœ€å°é¢‘æ¬¡",
            min_value=2,
            value=seg_prefs.get('min_ngram_frequency', 3),
            help="åªä¿ç•™å‡ºç°æ¬¡æ•° >= æ­¤å€¼çš„çŸ­è¯­ï¼ˆè¿‡æ»¤å™ªå£°ï¼‰",
            key="min_ngram_frequency_input"
        )
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
        if min_ngram_frequency != seg_prefs.get('min_ngram_frequency', 3):
            update_phase0_preference('segmentation', 'min_ngram_frequency', min_ngram_frequency)
            st.session_state.preferences = load_phase0_preferences()

    if st.button("ğŸš€ å¼€å§‹åˆ†è¯", type="primary"):
        import time
        start_time = time.time()

        with st.spinner("æ­£åœ¨åˆ†è¯..."):
            # æ‰§è¡Œåˆ†è¯ï¼ˆä½¿ç”¨å¸¦seedè¿½è¸ªçš„ç‰ˆæœ¬ï¼‰
            if st.session_state.phrases_cache:
                # ä½¿ç”¨å¸¦seedè¿½è¸ªçš„åˆ†è¯
                word_counter, word_to_seeds, ngram_counter, ngram_to_seeds = segment_keywords_with_seed_tracking(
                    st.session_state.phrases_cache,
                    st.session_state.stopwords,
                    extract_ngrams=extract_ngrams,
                    min_ngram_frequency=min_ngram_frequency if extract_ngrams else 2
                )
                st.session_state.word_to_seeds = word_to_seeds
                st.session_state.ngram_counter = ngram_counter
                st.session_state.ngram_to_seeds = ngram_to_seeds
            else:
                # é™çº§åˆ°æ™®é€šåˆ†è¯ï¼ˆå¦‚æœæ²¡æœ‰phrases_cacheï¼‰
                word_counter = segment_keywords(
                    keywords_cleaned,
                    st.session_state.stopwords
                )
                st.session_state.word_to_seeds = {}
                st.session_state.ngram_counter = Counter()
                st.session_state.ngram_to_seeds = {}

            st.session_state.word_counter = word_counter

            # è¯æ€§æ ‡æ³¨
            if enable_pos_tagging and POS_TAGGING_AVAILABLE:
                with st.spinner("æ­£åœ¨è¿›è¡Œè¯æ€§æ ‡æ³¨..."):
                    words_list = list(word_counter.keys())
                    st.session_state.pos_tags = tag_words_batch(words_list)

            # è·å–ç»Ÿè®¡
            stats = get_statistics(word_counter)

            # æ˜¾ç¤ºç»Ÿè®¡
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("å”¯ä¸€è¯æ•°", stats['total_unique_words'])
            col2.metric("æ€»å‡ºç°æ¬¡æ•°", stats['total_occurrences'])
            col3.metric("å¹³å‡é¢‘æ¬¡", stats['avg_frequency'])

            # è¯æ€§ç»Ÿè®¡
            if st.session_state.pos_tags:
                pos_stats = get_pos_statistics(word_counter, st.session_state.pos_tags)
                noun_count = pos_stats.get('by_category', {}).get('Noun', 0)
                col4.metric("åè¯æ•°é‡", noun_count)

            # çŸ­è¯­ç»Ÿè®¡
            if extract_ngrams and st.session_state.ngram_counter:
                st.markdown("**çŸ­è¯­æå–ç»“æœ**")
                col1, col2 = st.columns(2)
                col1.metric("å”¯ä¸€çŸ­è¯­æ•°", len(st.session_state.ngram_counter))
                col2.metric("çŸ­è¯­æ€»å‡ºç°æ¬¡æ•°", sum(st.session_state.ngram_counter.values()))

        # ========== ä¿å­˜åˆ°æ•°æ®åº“ ==========
        try:
            with st.spinner("æ­£åœ¨ä¿å­˜åˆ†è¯ç»“æœåˆ°æ•°æ®åº“..."):
                with WordSegmentRepository() as ws_repo:
                    # åˆ›å»ºæ‰¹æ¬¡è®°å½•
                    batch_id = ws_repo.create_batch(
                        phrase_count=len(keywords_cleaned),
                        notes=f"Phase0åˆ†è¯ - {len(word_counter)}è¯ + {len(st.session_state.ngram_counter)}çŸ­è¯­"
                    )

                    # ä¿å­˜åˆ†è¯ç»“æœ
                    new_words, new_ngrams = ws_repo.save_word_segments(
                        word_counter=word_counter,
                        pos_tags=st.session_state.pos_tags if enable_pos_tagging else None,
                        translations=st.session_state.translations,
                        batch_id=batch_id,
                        ngram_counter=st.session_state.ngram_counter if extract_ngrams else None,
                        ngram_translations=st.session_state.ngram_translations
                    )

                    # æ›´æ–°æ‰¹æ¬¡è®°å½•
                    duration = int(time.time() - start_time)
                    ws_repo.complete_batch(
                        batch_id=batch_id,
                        word_count=len(word_counter) + len(st.session_state.ngram_counter),
                        new_word_count=new_words + new_ngrams,
                        duration_seconds=duration
                    )

                    # æ ‡è®°å·²ä¿å­˜
                    st.session_state.segmentation_loaded_from_db = True
                    st.session_state.last_batch_phrase_count = len(keywords_cleaned)

            st.success(f"âœ“ åˆ†è¯å®Œæˆå¹¶å·²ä¿å­˜ï¼æ–°å¢ {new_words} ä¸ªå•è¯ + {new_ngrams} ä¸ªçŸ­è¯­")

        except Exception as e:
            st.error(f"âŒ ä¿å­˜åˆ†è¯ç»“æœå¤±è´¥: {str(e)}")
            import traceback
            st.error(traceback.format_exc())
            st.warning("âš ï¸ åˆ†è¯ç»“æœæœªä¿å­˜ï¼Œä½†æ‚¨ä»å¯ä»¥åœ¨æœ¬æ¬¡ä¼šè¯ä¸­ä½¿ç”¨")

    # ========== 4. æ˜¾ç¤ºç»“æœ ==========
    if st.session_state.word_counter is not None:
        st.header("4ï¸âƒ£ åˆ†è¯ç»“æœ")

        st.subheader("ğŸ“Š Tokenåˆ†æï¼ˆå•è¯+çŸ­è¯­ï¼‰")

        # ========== åˆå¹¶å•è¯å’ŒçŸ­è¯­æ•°æ® ==========
        # 1. å‡†å¤‡å•è¯æ•°æ®
        sorted_words = get_sorted_words(
            st.session_state.word_counter,
            sort_by=sort_by,
            min_frequency=min_frequency
        )

        # åˆ›å»ºå•è¯DataFrame
        df_words = pd.DataFrame(sorted_words, columns=['Token', 'é¢‘æ¬¡'])
        df_words['è¯æ•°'] = 1  # å•è¯çš„è¯æ•°ä¸º1

        # 2. å‡†å¤‡çŸ­è¯­æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        df_ngrams = pd.DataFrame()
        if st.session_state.ngram_counter and len(st.session_state.ngram_counter) > 0:
            ngram_items = [(ngram, count) for ngram, count in st.session_state.ngram_counter.items()]
            df_ngrams = pd.DataFrame(ngram_items, columns=['Token', 'é¢‘æ¬¡'])
            df_ngrams['è¯æ•°'] = df_ngrams['Token'].map(lambda x: len(x.split()))

        # 3. åˆå¹¶å•è¯å’ŒçŸ­è¯­
        if not df_ngrams.empty:
            df_all = pd.concat([df_words, df_ngrams], ignore_index=True)
            st.info(f"âœ“ å·²åˆå¹¶ {len(df_words)} ä¸ªå•è¯ + {len(df_ngrams)} ä¸ªçŸ­è¯­ = {len(df_all)} ä¸ªtoken")
        else:
            df_all = df_words
            st.info(f"âœ“ å…± {len(df_all)} ä¸ªå•è¯ï¼ˆæœªæå–çŸ­è¯­ï¼‰")

        # 4. æŒ‰é¢‘æ¬¡é‡æ–°æ’åº
        df_all = df_all.sort_values('é¢‘æ¬¡', ascending=False).reset_index(drop=True)

        # ========== 5. æ·»åŠ è¯æ€§åˆ—ï¼ˆåªå¯¹å•è¯ï¼ŒçŸ­è¯­ç•™ç©ºï¼‰ ==========
        if st.session_state.pos_tags:
            df_all['è¯æ€§'] = df_all['Token'].map(
                lambda w: st.session_state.pos_tags.get(w, ('UNKNOWN', 'Other', 'æœªçŸ¥'))[2] if len(w.split()) == 1 else ''
            )
            df_all['è¯æ€§åˆ†ç±»'] = df_all['Token'].map(
                lambda w: st.session_state.pos_tags.get(w, ('UNKNOWN', 'Other', 'æœªçŸ¥'))[1] if len(w.split()) == 1 else ''
            )

        # ========== 6. æ·»åŠ è¯æ ¹çŠ¶æ€åˆ— ==========
        with st.spinner("æ­£åœ¨æŸ¥è¯¢è¯æ ¹çŠ¶æ€..."):
            with PhraseRepository() as repo:
                tokens_list = df_all['Token'].tolist()
                seed_status = repo.get_words_seed_status(tokens_list)

        df_all['æ˜¯å¦ä¸ºè¯æ ¹'] = df_all['Token'].map(
            lambda w: 'æ˜¯' if seed_status.get(w, 0) > 0 else 'å¦'
        )
        df_all['ä½œä¸ºè¯æ ¹çš„æ‰©å±•æ•°'] = df_all['Token'].map(
            lambda w: seed_status.get(w, 0)
        )

        # ========== 7. æ·»åŠ æ¥æºè¯æ ¹åˆ— ==========
        # åˆå¹¶å•è¯å’ŒçŸ­è¯­çš„seedsä¿¡æ¯
        all_seeds = {**st.session_state.word_to_seeds, **st.session_state.ngram_to_seeds}

        def format_seeds(token, max_show=5):
            """æ ¼å¼åŒ–è¯æ ¹æ˜¾ç¤ºï¼Œåªæ˜¾ç¤ºå‰å‡ ä¸ª"""
            seeds = sorted(all_seeds.get(token, ['unknown']))
            if len(seeds) <= max_show:
                return ', '.join(seeds)
            else:
                shown = ', '.join(seeds[:max_show])
                return f"{shown}... (+{len(seeds)-max_show}ä¸ª)"

        df_all['æ¥æºè¯æ ¹'] = df_all['Token'].map(
            lambda w: format_seeds(w, max_show=5)
        )
        df_all['æ¥æºè¯æ ¹æ•°'] = df_all['Token'].map(
            lambda w: len(all_seeds.get(w, []))
        )

        # ========== 8. é«˜çº§ç­›é€‰åŒºåŸŸ ==========
        st.subheader("ğŸ” é«˜çº§ç­›é€‰")

        # åŠ è½½ç­›é€‰åå¥½è®¾ç½®
        filter_prefs = st.session_state.preferences.get('filtering', {})

        # å¦‚æœDataFrameä¸ºç©ºï¼Œæå‰è¿”å›
        if len(df_all) == 0:
            st.warning("âš ï¸ æ²¡æœ‰å¯ç­›é€‰çš„æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œåˆ†è¯")
            return

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            # é¢‘æ¬¡ç­›é€‰
            st.markdown("**é¢‘æ¬¡èŒƒå›´**")
            freq_min = int(df_all['é¢‘æ¬¡'].min())
            freq_max = int(df_all['é¢‘æ¬¡'].max())

            freq_range = st.slider(
                "é€‰æ‹©é¢‘æ¬¡èŒƒå›´",
                min_value=freq_min,
                max_value=freq_max,
                value=(freq_min, freq_max),
                help="æ‹–åŠ¨æ»‘å—ç­›é€‰é¢‘æ¬¡èŒƒå›´",
                key="freq_range_slider"
            )

            # åº”ç”¨é¢‘æ¬¡ç­›é€‰
            df_all = df_all[
                (df_all['é¢‘æ¬¡'] >= freq_range[0]) &
                (df_all['é¢‘æ¬¡'] <= freq_range[1])
            ]

        with col2:
            # è¯æ•°ç­›é€‰ï¼ˆå•è¯=1ï¼ŒçŸ­è¯­=2-6ï¼‰
            st.markdown("**Tokené•¿åº¦ç­›é€‰**")
            available_word_counts = sorted(df_all['è¯æ•°'].unique().tolist())

            selected_word_counts = st.multiselect(
                "é€‰æ‹©è¯æ•°",
                options=available_word_counts,
                default=available_word_counts,
                help="1=å•è¯ï¼Œ2-6=çŸ­è¯­",
                key="word_count_filter"
            )

            # åº”ç”¨è¯æ•°ç­›é€‰
            if selected_word_counts:
                df_all = df_all[df_all['è¯æ•°'].isin(selected_word_counts)]

        with col3:
            # è¯æ€§ç­›é€‰ï¼ˆåªå¯¹å•è¯æœ‰æ•ˆï¼‰
            if st.session_state.pos_tags:
                st.markdown("**è¯æ€§ç­›é€‰**")
                # è·å–å¯ç”¨çš„è¯æ€§åˆ†ç±»
                available_categories = get_available_categories()
                category_names = [cn for _, cn in available_categories]

                selected_pos = st.multiselect(
                    "é€‰æ‹©è¯æ€§",
                    options=category_names,
                    help="ä»…å¯¹å•è¯æœ‰æ•ˆï¼ˆçŸ­è¯­æ— è¯æ€§ï¼‰",
                    key="pos_filter_multiselect"
                )

                # åº”ç”¨è¯æ€§ç­›é€‰ï¼ˆåªç­›é€‰å•è¯ï¼ŒçŸ­è¯­å§‹ç»ˆä¿ç•™ï¼‰
                if selected_pos:
                    # å»ºç«‹ä¸­æ–‡åˆ°è‹±æ–‡çš„æ˜ å°„
                    cn_to_en = {cn: en for en, cn in available_categories}
                    selected_en = [cn_to_en[cn] for cn in selected_pos]
                    # ç­›é€‰æ¡ä»¶ï¼šè¯æ€§åŒ¹é… OR æ˜¯çŸ­è¯­ï¼ˆè¯æ•°>1ï¼‰
                    df_all = df_all[
                        df_all['è¯æ€§åˆ†ç±»'].isin(selected_en) | (df_all['è¯æ•°'] > 1)
                    ]

        with col4:
            # è¯æ ¹ç­›é€‰
            st.markdown("**è¯æ ¹ç­›é€‰**")
            exclude_seeds = st.checkbox(
                "ä»…æ˜¾ç¤ºéè¯æ ¹",
                value=filter_prefs.get('exclude_seeds', False),
                help="éšè—ä½œä¸ºseed_wordçš„token",
                key="exclude_seeds_checkbox"
            )
            # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
            if exclude_seeds != filter_prefs.get('exclude_seeds', False):
                update_phase0_preference('filtering', 'exclude_seeds', exclude_seeds)
                st.session_state.preferences = load_phase0_preferences()

            if exclude_seeds:
                df_all = df_all[df_all['æ˜¯å¦ä¸ºè¯æ ¹'] == 'å¦']

        # æ˜¾ç¤ºç­›é€‰ç»“æœ
        st.info(f"âœ“ ç­›é€‰åå‰©ä½™ {len(df_all)} ä¸ªtoken")

        # å¦‚æœç­›é€‰åæ²¡æœ‰æ•°æ®ï¼Œæå‰è¿”å›
        if len(df_all) == 0:
            st.warning("ç­›é€‰åæ²¡æœ‰tokenï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶")
            return

        # ========== 9. ç¿»è¯‘é€‰é¡¹ ==========
        st.subheader("ğŸŒ ç¿»è¯‘")

        # åŠ è½½ç¿»è¯‘åå¥½è®¾ç½®
        trans_prefs = st.session_state.preferences.get('translation', {})

        col1, col2 = st.columns([3, 1])
        with col1:
            translate_enabled = st.checkbox(
                "æ˜¾ç¤ºä¸­æ–‡ç¿»è¯‘",
                value=trans_prefs.get('translate_enabled', False),
                disabled=not TRANSLATION_AVAILABLE,
                help="ä½¿ç”¨Google Translateè¿›è¡Œè‹±è¯‘ä¸­ï¼ˆåŸºäºdeep-translatoråº“ï¼‰",
                key="translate_enabled_checkbox"
            )
            if not TRANSLATION_AVAILABLE:
                st.info("â„¹ï¸ ç¿»è¯‘åŠŸèƒ½ä¸å¯ç”¨ï¼šè¯·è¿è¡Œ `pip install deep-translator` å®‰è£…ç¿»è¯‘åº“")
            # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
            if translate_enabled != trans_prefs.get('translate_enabled', False):
                update_phase0_preference('translation', 'translate_enabled', translate_enabled)
                st.session_state.preferences = load_phase0_preferences()

        with col2:
            if translate_enabled and TRANSLATION_AVAILABLE:
                # å…ˆä»æ•°æ®åº“åŠ è½½å·²æœ‰ç¿»è¯‘
                if st.button("ğŸŒ æ‰§è¡Œç¿»è¯‘"):
                    with st.spinner("æ­£åœ¨ä»æ•°æ®åº“åŠ è½½å·²æœ‰ç¿»è¯‘..."):
                        tokens_to_translate = df_all['Token'].tolist()

                        # ä»word_segmentsè¡¨åŠ è½½å·²æœ‰ç¿»è¯‘
                        with WordSegmentRepository() as ws_repo:
                            existing_translations = {}
                            for token in tokens_to_translate:
                                ws = ws_repo.get_word_segment(token)
                                if ws and ws.translation:
                                    existing_translations[token] = ws.translation

                        # æ‰¾å‡ºéœ€è¦ç¿»è¯‘çš„æ–°token
                        tokens_need_translation = [
                            t for t in tokens_to_translate
                            if t not in existing_translations
                        ]

                        if existing_translations:
                            st.info(f"âœ“ ä»æ•°æ®åº“åŠ è½½äº† {len(existing_translations)} ä¸ªå·²æœ‰ç¿»è¯‘")

                        # ç¿»è¯‘æ–°token
                        new_translations = {}
                        if tokens_need_translation:
                            with st.spinner(f"æ­£åœ¨ç¿»è¯‘ {len(tokens_need_translation)} ä¸ªæ–°token..."):
                                new_translations = translate_words_batch(
                                    tokens_need_translation,
                                    batch_size=100
                                )

                            # ä¿å­˜æ–°ç¿»è¯‘åˆ°æ•°æ®åº“
                            if new_translations:
                                with st.spinner("æ­£åœ¨ä¿å­˜ç¿»è¯‘åˆ°æ•°æ®åº“..."):
                                    with WordSegmentRepository() as ws_repo:
                                        for token, trans in new_translations.items():
                                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                                            existing = ws_repo.get_word_segment(token)
                                            if existing:
                                                # æ›´æ–°ç¿»è¯‘
                                                existing.translation = trans
                                            else:
                                                # åˆ›å»ºæ–°è®°å½•ï¼ˆåªä¿å­˜tokenå’Œtranslationï¼‰
                                                from storage.models import WordSegment
                                                from datetime import datetime
                                                new_ws = WordSegment(
                                                    word=token,
                                                    frequency=0,  # ç¿»è¯‘åŠŸèƒ½ä¸è®°å½•é¢‘æ¬¡
                                                    translation=trans,
                                                    created_at=datetime.utcnow()
                                                )
                                                ws_repo.session.add(new_ws)
                                        ws_repo.session.commit()
                                st.success(f"âœ“ ç¿»è¯‘äº† {len(new_translations)} ä¸ªæ–°tokenå¹¶å·²ä¿å­˜")
                        else:
                            st.success("âœ“ æ‰€æœ‰tokenéƒ½å·²æœ‰ç¿»è¯‘ï¼")

                        # åˆå¹¶ç¿»è¯‘ç»“æœåˆ°session stateï¼ˆåˆå¹¶å•è¯å’ŒçŸ­è¯­çš„ç¿»è¯‘ï¼‰
                        all_translations = {**st.session_state.translations, **st.session_state.ngram_translations}
                        all_translations.update(existing_translations)
                        all_translations.update(new_translations)

                        # åˆ†åˆ«ä¿å­˜åˆ°å¯¹åº”çš„session state
                        for token in df_all['Token'].tolist():
                            if token in all_translations:
                                if len(token.split()) == 1:
                                    st.session_state.translations[token] = all_translations[token]
                                else:
                                    st.session_state.ngram_translations[token] = all_translations[token]

        # æ·»åŠ ç¿»è¯‘åˆ—
        if translate_enabled and (st.session_state.translations or st.session_state.ngram_translations):
            all_translations = {**st.session_state.translations, **st.session_state.ngram_translations}
            df_all['ä¸­æ–‡'] = df_all['Token'].map(all_translations)

        # ========== 10. Tokené€‰æ‹© ==========
        st.subheader("ğŸ¯ Tokené€‰æ‹©")

        # è°ƒè¯•æ¨¡å¼å¼€å…³
        debug_mode = st.checkbox("ğŸ› å¼€å¯è°ƒè¯•æ¨¡å¼", value=True, help="æ˜¾ç¤ºçŠ¶æ€å˜åŒ–çš„è¯¦ç»†ä¿¡æ¯")

        # æ·»åŠ è¯¦ç»†çš„æ‰§è¡Œè¿½è¸ª
        if 'debug_log' not in st.session_state:
            st.session_state.debug_log = []

        def log_debug(message):
            """è®°å½•è°ƒè¯•ä¿¡æ¯ï¼ˆåŒæ—¶è¾“å‡ºåˆ°é¡µé¢å’Œæµè§ˆå™¨æ§åˆ¶å°ï¼‰"""
            import datetime
            timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
            log_entry = f"[{timestamp}] {message}"
            st.session_state.debug_log.append(log_entry)
            # åªä¿ç•™æœ€è¿‘50æ¡
            if len(st.session_state.debug_log) > 50:
                st.session_state.debug_log = st.session_state.debug_log[-50:]

            # åŒæ—¶è¾“å‡ºåˆ°æµè§ˆå™¨æ§åˆ¶å°
            import json
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ä»¥é¿å…JavaScriptæ³¨å…¥é—®é¢˜
            safe_message = json.dumps(log_entry)
            st.components.v1.html(
                f"<script>console.log({safe_message});</script>",
                height=0
            )

        # æ‰¹é‡é€‰æ‹©æ“ä½œ
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            if st.button("âœ… å…¨é€‰", key="select_all_btn"):
                log_debug("ğŸ”µ ç”¨æˆ·ç‚¹å‡»ã€å…¨é€‰ã€‘æŒ‰é’®")
                st.session_state.selected_words = set(df_all['Token'].tolist())
                log_debug(f"   - selected_wordsæ›´æ–°ä¸º: {len(st.session_state.selected_words)} ä¸ª")
                # è®¾ç½®å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æ ‡å¿—
                st.session_state.force_reinit_editor = True
                st.rerun()
        with col2:
            if st.button("âŒ å…¨ä¸é€‰", key="deselect_all_btn"):
                log_debug("ğŸ”µ ç”¨æˆ·ç‚¹å‡»ã€å…¨ä¸é€‰ã€‘æŒ‰é’®")
                st.session_state.selected_words = set()
                log_debug(f"   - selected_wordsæ¸…ç©º")
                # è®¾ç½®å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æ ‡å¿—
                st.session_state.force_reinit_editor = True
                st.rerun()
        with col3:
            if st.button("ğŸ”„ åé€‰", key="inverse_select_btn"):
                log_debug("ğŸ”µ ç”¨æˆ·ç‚¹å‡»ã€åé€‰ã€‘æŒ‰é’®")
                all_tokens = set(df_all['Token'].tolist())
                st.session_state.selected_words = all_tokens - st.session_state.selected_words
                log_debug(f"   - selected_wordsåé€‰ä¸º: {len(st.session_state.selected_words)} ä¸ª")
                # è®¾ç½®å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æ ‡å¿—
                st.session_state.force_reinit_editor = True
                st.rerun()
        with col4:
            st.metric("å·²é€‰æ‹©", len(st.session_state.selected_words))

        # å‡†å¤‡æ˜¾ç¤ºç”¨çš„DataFrame
        df_display = df_all.copy()

        # ========== æ£€æµ‹ç­›é€‰æ¡ä»¶å˜åŒ–ï¼ˆè§¦å‘editor_dfé‡æ–°åˆå§‹åŒ–ï¼‰==========
        # ä½¿ç”¨Tokenåˆ—è¡¨çš„hashæ¥æ£€æµ‹æ•°æ®æ˜¯å¦å˜åŒ–
        current_tokens_hash = hash(tuple(sorted(df_display['Token'].tolist())))
        if 'last_tokens_hash' not in st.session_state:
            st.session_state.last_tokens_hash = None

        # å¦‚æœTokenåˆ—è¡¨å‘ç”Ÿå˜åŒ–ï¼ˆç­›é€‰æ¡ä»¶æ”¹å˜ï¼‰ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        if st.session_state.last_tokens_hash != current_tokens_hash:
            log_debug(f"ğŸ”„ æ£€æµ‹åˆ°ç­›é€‰æ¡ä»¶å˜åŒ–ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–editor_df")
            st.session_state.force_reinit_editor = True
            st.session_state.last_tokens_hash = current_tokens_hash

        # ========== 11. æ˜¾ç¤ºè¡¨æ ¼ ==========
        st.subheader("ğŸ“‹ Tokenåˆ—è¡¨")

        # ========== åˆ—é¡ºåºé…ç½® ==========
        with st.expander("âš™ï¸ åˆ—æ˜¾ç¤ºè®¾ç½®", expanded=False):
            st.markdown("**è‡ªå®šä¹‰åˆ—çš„æ˜¾ç¤ºé¡ºåº**ï¼ˆé€šè¿‡æ‹–åŠ¨è°ƒæ•´é¡ºåºï¼‰")
            st.info("ğŸ’¡ è°ƒæ•´å®Œé¡ºåºåï¼Œ**å¿…é¡»ç‚¹å‡»'ä¿å­˜åˆ—é¡ºåº'æŒ‰é’®**æ‰ä¼šç”Ÿæ•ˆ")

            # è·å–æ‰€æœ‰å¯ç”¨çš„åˆ—ï¼ˆé™¤äº†"é€‰æ‹©"åˆ—ï¼‰
            available_columns = [col for col in df_display.columns if col != 'é€‰æ‹©']

            # åŠ è½½ä¿å­˜çš„åˆ—é¡ºåº
            display_prefs = st.session_state.preferences.get('display', {})
            saved_column_order = display_prefs.get('column_order', [])

            # å¦‚æœä¿å­˜çš„åˆ—é¡ºåºä¸å®Œæ•´ï¼Œä½¿ç”¨é»˜è®¤é¡ºåº
            if set(saved_column_order) != set(available_columns):
                # é»˜è®¤åˆ—é¡ºåºï¼šä¸­æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰ã€Tokenã€é¢‘æ¬¡ã€è¯æ•°ã€å…¶ä»–
                default_order = []
                if 'ä¸­æ–‡' in available_columns:
                    default_order.append('ä¸­æ–‡')
                if 'Token' in available_columns:
                    default_order.append('Token')
                if 'é¢‘æ¬¡' in available_columns:
                    default_order.append('é¢‘æ¬¡')
                if 'è¯æ•°' in available_columns:
                    default_order.append('è¯æ•°')
                # æ·»åŠ å‰©ä½™çš„åˆ—
                for col in available_columns:
                    if col not in default_order:
                        default_order.append(col)
                saved_column_order = default_order
                # è‡ªåŠ¨ä¿å­˜é»˜è®¤é¡ºåº
                update_phase0_preference('display', 'column_order', saved_column_order)
                st.session_state.preferences = load_phase0_preferences()

            # æ˜¾ç¤ºå½“å‰ç”Ÿæ•ˆçš„åˆ—é¡ºåº
            st.write(f"**å½“å‰åˆ—é¡ºåº**ï¼š{' â†’ '.join(saved_column_order[:5])}{'...' if len(saved_column_order) > 5 else ''}")

            # ä½¿ç”¨multiselectè®©ç”¨æˆ·è°ƒæ•´é¡ºåºï¼ˆä½†ä¸ä¼šç«‹å³åº”ç”¨ï¼‰
            col1, col2 = st.columns([4, 1])
            with col1:
                selected_column_order = st.multiselect(
                    "è°ƒæ•´åˆ—é¡ºåºï¼ˆæ‹–åŠ¨åç‚¹å‡»ä¿å­˜ï¼‰",
                    options=available_columns,
                    default=saved_column_order,
                    key="column_order_multiselect",
                    help="âš ï¸ è°ƒæ•´åå¿…é¡»ç‚¹å‡»'ä¿å­˜åˆ—é¡ºåº'æŒ‰é’®"
                )

            with col2:
                # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
                has_changes = (selected_column_order != saved_column_order)
                if st.button("ğŸ’¾ ä¿å­˜åˆ—é¡ºåº", disabled=not has_changes, help="ä¿å­˜å½“å‰çš„åˆ—é¡ºåºè®¾ç½®", key="save_column_order_btn"):
                    update_phase0_preference('display', 'column_order', selected_column_order)
                    st.session_state.preferences = load_phase0_preferences()
                    st.success("âœ“ å·²ä¿å­˜")
                    st.rerun()

            if has_changes:
                st.caption("âš ï¸ åˆ—é¡ºåºæœ‰æ”¹åŠ¨ä½†æœªä¿å­˜ï¼Œç‚¹å‡»'ä¿å­˜åˆ—é¡ºåº'æŒ‰é’®ä½¿å…¶ç”Ÿæ•ˆ")

        # åº”ç”¨åˆ—é¡ºåºï¼ˆå§‹ç»ˆä½¿ç”¨å·²ä¿å­˜çš„é¡ºåºï¼Œä¸ä½¿ç”¨multiselectçš„ä¸´æ—¶å€¼ï¼‰
        final_column_order = [col for col in saved_column_order if col in df_display.columns]
        df_display = df_display[final_column_order]

        # æ„å»ºç¦ç”¨åˆ—åˆ—è¡¨ï¼ˆæ‰€æœ‰åˆ—é™¤äº†é€‰æ‹©åˆ—éƒ½ç¦ç”¨ç¼–è¾‘ï¼‰
        disabled_cols = ['Token', 'é¢‘æ¬¡', 'è¯æ•°']
        if 'è¯æ€§' in df_display.columns:
            disabled_cols.extend(['è¯æ€§', 'è¯æ€§åˆ†ç±»'])
        if 'ä¸­æ–‡' in df_display.columns:
            disabled_cols.append('ä¸­æ–‡')
        if 'æ˜¯å¦ä¸ºè¯æ ¹' in df_display.columns:
            disabled_cols.extend(['æ˜¯å¦ä¸ºè¯æ ¹', 'ä½œä¸ºè¯æ ¹çš„æ‰©å±•æ•°'])
        if 'æ¥æºè¯æ ¹' in df_display.columns:
            disabled_cols.extend(['æ¥æºè¯æ ¹', 'æ¥æºè¯æ ¹æ•°'])

        # ========== å…³é”®æ”¹è¿›ï¼šDataFrameç¼“å­˜æœºåˆ¶ï¼ˆé¿å…æ¯æ¬¡reruné‡æ–°åˆå§‹åŒ–ï¼‰==========
        # åˆå§‹åŒ–å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æ ‡å¿—
        if 'force_reinit_editor' not in st.session_state:
            st.session_state.force_reinit_editor = False

        # åªåœ¨é¦–æ¬¡æˆ–å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æ—¶åˆ›å»ºè¾“å…¥DataFrame
        if 'editor_df' not in st.session_state or st.session_state.force_reinit_editor:
            log_debug("ğŸ”„ é‡æ–°åˆå§‹åŒ–editor_dfï¼ˆé¦–æ¬¡æˆ–æ‰¹é‡æ“ä½œåï¼‰")
            df_with_selection = df_display.copy()
            df_with_selection.insert(0, 'é€‰æ‹©', df_with_selection['Token'].apply(
                lambda t: t in st.session_state.selected_words
            ))
            st.session_state.editor_df = df_with_selection
            st.session_state.force_reinit_editor = False  # é‡ç½®æ ‡å¿—
            log_debug(f"   - åˆå§‹é€‰ä¸­æ•°é‡: {df_with_selection['é€‰æ‹©'].sum()}")
        else:
            log_debug("â™»ï¸ ä½¿ç”¨ç¼“å­˜çš„editor_dfï¼ˆé¿å…é‡æ–°åˆå§‹åŒ–ï¼‰")

        st.caption("ğŸ’¡ é€‰æ‹©å®Œæˆåï¼Œç‚¹å‡»ä¸‹æ–¹çš„'å¯¼å‡º'æˆ–'æ·»åŠ åˆ°è¯æ ¹'æŒ‰é’®è¿›è¡Œæ“ä½œ")

        # æ¸²æŸ“data_editor - ä½¿ç”¨ç¼“å­˜çš„DataFrameä½œä¸ºè¾“å…¥
        edited_df = st.data_editor(
            st.session_state.editor_df,  # âœ… å…³é”®ï¼šä½¿ç”¨ç¼“å­˜çš„DataFrameï¼Œè€Œä¸æ˜¯æ¯æ¬¡é‡æ–°åˆ›å»º
            width='stretch',
            height=400,
            disabled=disabled_cols,
            hide_index=True,
            key="tokens_editor",
            column_config={
                "é€‰æ‹©": st.column_config.CheckboxColumn(
                    "é€‰æ‹©",
                    help="å‹¾é€‰è¦å¯¼å‡ºçš„token",
                    default=False,
                )
            }
        )

        log_debug(f"ğŸ“Š data_editorè¿”å› - è¿”å›é€‰ä¸­æ•°é‡: {edited_df['é€‰æ‹©'].sum() if 'é€‰æ‹©' in edited_df.columns else 0}")

        # æ›´æ–°ç¼“å­˜å’Œsession_state
        st.session_state.editor_df = edited_df  # æ›´æ–°ç¼“å­˜
        if 'Token' in edited_df.columns and 'é€‰æ‹©' in edited_df.columns:
            new_selected = set(edited_df[edited_df['é€‰æ‹©']]['Token'].tolist())
            old_count = len(st.session_state.selected_words)
            new_count = len(new_selected)
            st.session_state.selected_words = new_selected
            log_debug(f"ğŸ’¾ æ›´æ–°selected_words: {old_count} â†’ {new_count}")

            if debug_mode:
                st.info(f"âœ“ å·²æ›´æ–°é€‰æ‹©çŠ¶æ€: {new_count} ä¸ªtokenå·²é€‰ä¸­")

        # ========== 12. å¯¼å‡ºåŠŸèƒ½ ==========
        st.header("5ï¸âƒ£ å¯¼å‡ºç»“æœ")

        # åŠ è½½å¯¼å‡ºåå¥½è®¾ç½®
        export_prefs = st.session_state.preferences.get('export', {})

        # å¯¼å‡ºé€‰é¡¹
        export_selected_only = st.checkbox(
            "ä»…å¯¼å‡ºé€‰ä¸­çš„token",
            value=export_prefs.get('export_selected_only', False),
            help=f"å½“å‰å·²é€‰æ‹© {len(st.session_state.selected_words)} ä¸ªtoken",
            key="export_selected_only_checkbox"
        )
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœå€¼æ”¹å˜ï¼‰
        if export_selected_only != export_prefs.get('export_selected_only', False):
            update_phase0_preference('export', 'export_selected_only', export_selected_only)
            st.session_state.preferences = load_phase0_preferences()

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        if export_selected_only and st.session_state.selected_words:
            df_export = edited_df[edited_df['é€‰æ‹©']].copy()
            df_export = df_export.drop(columns=['é€‰æ‹©'])
            st.info(f"âœ“ å°†å¯¼å‡º {len(df_export)} ä¸ªé€‰ä¸­çš„token")
        else:
            df_export = edited_df.drop(columns=['é€‰æ‹©']).copy()

        col1, col2, col3 = st.columns(3)

        with col1:
            # å¯¼å‡ºCSV
            csv = df_export.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“„ ä¸‹è½½CSV",
                data=csv,
                file_name="tokens_segmented.csv",
                mime="text/csv"
            )

        with col2:
            # å¯¼å‡ºHTMLï¼ˆå¸¦ç­›é€‰åŠŸèƒ½ï¼‰
            # âœ… å®‰å…¨ä¿®å¤ï¼šå¯ç”¨HTMLè½¬ä¹‰ï¼Œé˜²æ­¢XSSæ”»å‡»
            import html as html_module
            html = df_export.to_html(index=False, escape=True, table_id='dataTable')

            # æ„å»ºè¯æ€§ç­›é€‰é€‰é¡¹ï¼ˆå¦‚æœæœ‰è¯æ€§åˆ—ï¼‰
            pos_filter_html = ""
            pos_filter_js = ""
            if 'è¯æ€§' in df_export.columns:
                # è·å–æ‰€æœ‰å”¯ä¸€çš„è¯æ€§
                unique_pos = df_export['è¯æ€§'].unique().tolist()
                # âœ… å®‰å…¨ä¿®å¤ï¼šå¯¹è¯æ€§å€¼è¿›è¡ŒHTMLè½¬ä¹‰ï¼Œé˜²æ­¢XSSæ³¨å…¥
                pos_options = ''.join([f'<option value="{html_module.escape(str(pos))}">{html_module.escape(str(pos))}</option>' for pos in sorted(unique_pos) if pos])

                pos_filter_html = f"""
                        <div class="filter-row">
                            <span class="filter-label">è¯æ€§:</span>
                            <select id="posFilter" class="filter-input" onchange="filterTable()" style="max-width: 200px;">
                                <option value="">å…¨éƒ¨è¯æ€§</option>
                                {pos_options}
                            </select>
                        </div>
                """

                # è¯æ€§åˆ—çš„ç´¢å¼•
                pos_col_index = df_export.columns.tolist().index('è¯æ€§')

                pos_filter_js = f"""
                            // æ£€æŸ¥è¯æ€§ç­›é€‰
                            var posFilter = document.getElementById('posFilter').value;
                            var posMatch = true;
                            if (posFilter && cells.length > {pos_col_index}) {{
                                var pos = cells[{pos_col_index}].textContent;
                                if (pos !== posFilter) {{
                                    posMatch = false;
                                }}
                            }}
                """

                # ä¿®æ”¹æ˜¾ç¤ºé€»è¾‘
                pos_filter_js += """
                            // æ˜¾ç¤ºæˆ–éšè—è¡Œ
                            if (wordMatch && freqMatch && posMatch) {
                                rows[i].style.display = '';
                                visibleCount++;
                            } else {
                                rows[i].style.display = 'none';
                            }
                """
            else:
                pos_filter_js = """
                            // æ˜¾ç¤ºæˆ–éšè—è¡Œ
                            if (wordMatch && freqMatch) {
                                rows[i].style.display = '';
                                visibleCount++;
                            } else {
                                rows[i].style.display = 'none';
                            }
                """

            html_full = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="utf-8">
                <title>Tokenåˆ†è¯ç»“æœ</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                    .container {{ max-width: 1400px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                    h1 {{ color: #333; text-align: center; margin-bottom: 30px; }}
                    .filter-container {{ margin: 20px 0; padding: 20px; background-color: #f9f9f9; border-radius: 5px; border: 1px solid #ddd; }}
                    .filter-row {{ display: flex; gap: 15px; align-items: center; flex-wrap: wrap; margin-bottom: 10px; }}
                    .filter-input {{ padding: 10px; font-size: 14px; border: 1px solid #ccc; border-radius: 4px; flex: 1; min-width: 200px; }}
                    .filter-input:focus {{ outline: none; border-color: #4CAF50; }}
                    .filter-label {{ font-weight: bold; color: #555; min-width: 80px; }}
                    #resultCount {{ color: #4CAF50; font-weight: bold; font-size: 14px; }}
                    .reset-btn {{ padding: 10px 20px; background-color: #f44336; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 14px; }}
                    .reset-btn:hover {{ background-color: #da190b; }}
                    table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
                    th, td {{ border: 1px solid #ddd; padding: 12px 8px; text-align: left; }}
                    th {{ background-color: #4CAF50; color: white; position: sticky; top: 0; z-index: 10; }}
                    tr:nth-child(even) {{ background-color: #f2f2f2; }}
                    tr:hover {{ background-color: #e8f5e9; }}
                    .table-wrapper {{ max-height: 600px; overflow-y: auto; border: 1px solid #ddd; border-radius: 5px; }}
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>ğŸ“Š Tokenåˆ†è¯ç»“æœ</h1>

                    <div class="filter-container">
                        <h3 style="margin-top: 0; color: #333;">ğŸ” ç­›é€‰å·¥å…·</h3>
                        <div class="filter-row">
                            <span class="filter-label">å…³é”®è¯:</span>
                            <input type="text" id="wordFilter" class="filter-input"
                                   placeholder="è¾“å…¥å…³é”®è¯è¿›è¡Œç­›é€‰..." onkeyup="filterTable()">
                        </div>
                        <div class="filter-row">
                            <span class="filter-label">æœ€å°é¢‘æ¬¡:</span>
                            <input type="number" id="minFreq" class="filter-input"
                                   placeholder="æœ€å°é¢‘æ¬¡" onkeyup="filterTable()" style="max-width: 150px;">
                            <span class="filter-label">æœ€å¤§é¢‘æ¬¡:</span>
                            <input type="number" id="maxFreq" class="filter-input"
                                   placeholder="æœ€å¤§é¢‘æ¬¡" onkeyup="filterTable()" style="max-width: 150px;">
                            <button class="reset-btn" onclick="resetFilters()">ğŸ”„ é‡ç½®ç­›é€‰</button>
                        </div>
                        {pos_filter_html}
                        <div style="margin-top: 15px;">
                            <span id="resultCount"></span>
                        </div>
                    </div>

                    <div class="table-wrapper">
                        {html}
                    </div>
                </div>

                <script>
                    // ç­›é€‰è¡¨æ ¼å‡½æ•°
                    function filterTable() {{
                        var wordFilter = document.getElementById('wordFilter').value.toLowerCase();
                        var minFreq = document.getElementById('minFreq').value;
                        var maxFreq = document.getElementById('maxFreq').value;

                        var table = document.getElementById('dataTable');
                        var rows = table.getElementsByTagName('tr');
                        var visibleCount = 0;

                        // ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
                        for (var i = 1; i < rows.length; i++) {{
                            var cells = rows[i].getElementsByTagName('td');
                            if (cells.length === 0) continue;

                            var word = cells[0].textContent.toLowerCase();
                            var frequency = parseInt(cells[1].textContent);

                            // æ£€æŸ¥å…³é”®è¯ç­›é€‰
                            var wordMatch = true;
                            if (wordFilter) {{
                                // æ£€æŸ¥æ‰€æœ‰åˆ—ï¼ˆåŒ…æ‹¬å¯èƒ½çš„ä¸­æ–‡ç¿»è¯‘åˆ—ï¼‰
                                wordMatch = false;
                                for (var j = 0; j < cells.length; j++) {{
                                    if (cells[j].textContent.toLowerCase().indexOf(wordFilter) > -1) {{
                                        wordMatch = true;
                                        break;
                                    }}
                                }}
                            }}

                            // æ£€æŸ¥é¢‘æ¬¡ç­›é€‰
                            var freqMatch = true;
                            if (minFreq && frequency < parseInt(minFreq)) {{
                                freqMatch = false;
                            }}
                            if (maxFreq && frequency > parseInt(maxFreq)) {{
                                freqMatch = false;
                            }}

                            {pos_filter_js}
                        }}

                        // æ›´æ–°ç»“æœè®¡æ•°
                        var totalRows = rows.length - 1;
                        document.getElementById('resultCount').textContent =
                            'âœ“ æ˜¾ç¤º ' + visibleCount + ' / ' + totalRows + ' æ¡ç»“æœ';
                    }}

                    // é‡ç½®ç­›é€‰
                    function resetFilters() {{
                        document.getElementById('wordFilter').value = '';
                        document.getElementById('minFreq').value = '';
                        document.getElementById('maxFreq').value = '';
                        {'document.getElementById("posFilter").value = "";' if pos_filter_html else ''}
                        filterTable();
                    }}

                    // é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–
                    window.onload = function() {{
                        var table = document.getElementById('dataTable');
                        if (table) {{
                            var totalRows = table.getElementsByTagName('tr').length - 1;
                            document.getElementById('resultCount').textContent =
                                'âœ“ æ˜¾ç¤º ' + totalRows + ' / ' + totalRows + ' æ¡ç»“æœ';
                        }}
                    }};
                </script>
            </body>
            </html>
            """
            st.download_button(
                label="ğŸŒ ä¸‹è½½HTML",
                data=html_full,
                file_name="tokens_segmented.html",
                mime="text/html"
            )

        with col3:
            # å¤åˆ¶åˆ°å‰ªè´´æ¿
            text_for_copy = df_export.to_string(index=False)
            st.text_area(
                "å¤åˆ¶å†…å®¹",
                value=text_for_copy,
                height=100,
                help="å…¨é€‰å¹¶å¤åˆ¶æ–‡æœ¬"
            )

        # æ·»åŠ åˆ°è¯æ ¹ç®¡ç†
        st.markdown("---")
        st.subheader("ğŸŒ± æ·»åŠ åˆ°è¯æ ¹ç®¡ç†")

        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("""
            **å°†é€‰ä¸­çš„è¯æ±‡æ·»åŠ ä¸ºè¯æ ¹ï¼ˆseed_wordï¼‰**

            é€‰ä¸­çš„è¯æ±‡å°†è¢«æ·»åŠ åˆ°è¯æ ¹ç®¡ç†ç³»ç»Ÿä¸­ï¼Œåç»­å¯ä»¥ï¼š
            - ä½¿ç”¨ ğŸ¤– æ‰¹é‡è‡ªåŠ¨åˆ†ç±» åŠŸèƒ½è‡ªåŠ¨åˆ†é…Tokenç±»åˆ«
            - æ‰‹åŠ¨ç¼–è¾‘è¯æ ¹çš„å®šä¹‰ã€å•†ä¸šä»·å€¼ç­‰ä¿¡æ¯
            - å…³è”è¯æ ¹åˆ°éœ€æ±‚å¡ç‰‡
            """)

        with col2:
            add_to_seeds_btn = st.button(
                "â• æ·»åŠ é€‰ä¸­è¯æ±‡åˆ°è¯æ ¹",
                type="primary",
                disabled=len(st.session_state.selected_words) == 0,
                help=f"å°† {len(st.session_state.selected_words)} ä¸ªé€‰ä¸­çš„è¯æ±‡æ·»åŠ ä¸ºè¯æ ¹"
            )

        if add_to_seeds_btn:
            add_selected_words_to_seeds()


def add_selected_words_to_seeds():
    """å°†é€‰ä¸­çš„è¯æ±‡æ·»åŠ åˆ°seed_wordsè¡¨"""
    try:
        selected_words = st.session_state.selected_words

        if not selected_words:
            st.warning("âš ï¸ æ²¡æœ‰é€‰ä¸­ä»»ä½•è¯æ±‡")
            return

        with st.spinner(f"æ­£åœ¨æ·»åŠ  {len(selected_words)} ä¸ªè¯æ±‡åˆ°è¯æ ¹ç®¡ç†..."):
            imported_count = 0
            updated_count = 0
            skipped_count = 0

            with SeedWordRepository() as seed_repo:
                for word in selected_words:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = seed_repo.get_seed_word(word)

                    if existing:
                        # å·²å­˜åœ¨ï¼Œåªæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        seed_repo.update_expansion_stats(word)
                        updated_count += 1
                    else:
                        # åˆ›å»ºæ–°è¯æ ¹ï¼ˆæœªåˆ†ç±»çŠ¶æ€ï¼Œæ¥æºæ ‡è®°ä¸ºphase0_selectionï¼‰
                        seed_repo.create_or_update_seed_word(
                            seed_word=word,
                            token_types=None,  # å¾…åˆ†ç±»
                            primary_token_type=None,  # å¾…åˆ†ç±»
                            source='phase0_selection',
                            status='active'
                        )

                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        seed_repo.update_expansion_stats(word)
                        imported_count += 1

            # æ˜¾ç¤ºç»“æœ
            st.success(f"âœ“ æ·»åŠ å®Œæˆï¼")

            col1, col2, col3 = st.columns(3)
            col1.metric("æ–°å¢è¯æ ¹", imported_count)
            col2.metric("æ›´æ–°ç»Ÿè®¡", updated_count)
            col3.metric("æ€»å¤„ç†", imported_count + updated_count)

            # æç¤ºåç»­æ“ä½œ
            if imported_count > 0:
                st.info("""
                ğŸ’¡ **åç»­æ“ä½œå»ºè®®**ï¼š
                1. è¿›å…¥ **ğŸŒ± è¯æ ¹ç®¡ç†** é¡µé¢
                2. ç‚¹å‡» **ğŸ¤– æ‰¹é‡è‡ªåŠ¨åˆ†ç±»** æŒ‰é’®ï¼Œä¸ºæ–°è¯æ ¹è‡ªåŠ¨åˆ†é…Tokenç±»åˆ«
                3. äººå·¥å¤æŸ¥å¹¶è°ƒæ•´åˆ†ç±»ç»“æœ
                4. ä¸ºé‡è¦è¯æ ¹æ·»åŠ å®šä¹‰å’Œå•†ä¸šä»·å€¼
                """)

                # æ¸…ç©ºé€‰æ‹©
                if st.button("ğŸ”„ æ¸…ç©ºé€‰æ‹©å¹¶ç»§ç»­"):
                    st.session_state.selected_words = set()
                    st.rerun()

    except Exception as e:
        st.error(f"âŒ æ·»åŠ å¤±è´¥: {str(e)}")
        import traceback
        st.error(traceback.format_exc())


if __name__ == "__main__":
    main()
