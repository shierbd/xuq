# èšç±»ä¼˜åŒ–è¡ŒåŠ¨è®¡åˆ’

**åˆ›å»ºæ—¥æœŸ**: 2026-02-02
**åŸºäº**: GPT æŠ€æœ¯è¯„å®¡åé¦ˆ
**ç›®æ ‡**: ä»"è¦†ç›–ç‡ä¼˜å…ˆ"è½¬å‘"éœ€æ±‚ä¸»é¢˜è´¨é‡ä¼˜å…ˆ"

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

### å½“å‰çŠ¶æ€
- âœ… å·¥ç¨‹ä¸Šåˆç†ã€èƒ½ç”¨
- âœ… è¦†ç›–ç‡ 70.74%ï¼ˆ11,172/15,792ï¼‰
- âŒ ç”Ÿæˆ 1,412 ä¸ªç°‡ï¼ˆå…¶ä¸­ 725 ä¸ªå¾®ç°‡ï¼‰
- âŒ æ›´åƒ"å•†å“ç›¸ä¼¼å½’ç»„"è€Œé"éœ€æ±‚ä¸»é¢˜èšç±»"

### æ ¸å¿ƒé—®é¢˜
> **ç³»ç»Ÿä¼˜åŒ–æ–¹å‘èµ°åäº†**ï¼šä¸ºäº†å‡å°‘å™ªéŸ³ç‚¹è€Œåˆ¶é€ å¤§é‡ä¸å¯å‘½åçš„å¾®ç°‡

### ç›®æ ‡è½¬å˜
```
ä»: æœ€å¤§åŒ–è¦†ç›–ç‡ + æœ€å°åŒ–å™ªéŸ³ç‚¹
åˆ°: äº§å‡ºé«˜è´¨é‡ã€å¯å‘½åã€å¯ç”¨äºéœ€æ±‚åˆ¤æ–­çš„ä¸»é¢˜ç°‡
```

---

## ğŸ¯ å››ä¸ªå…³é”®é—®é¢˜ï¼ˆP0 ä¼˜å…ˆçº§ï¼‰

### é—®é¢˜ 1: metric='euclidean' ä¸é€‚åˆæ–‡æœ¬å‘é‡

**å½“å‰ä»£ç ** (`backend/services/clustering_service.py:354`):
```python
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=min_cluster_size,
    min_samples=min_samples,
    metric='euclidean',  # âŒ é—®é¢˜æ‰€åœ¨
    cluster_selection_method='leaf',
    cluster_selection_epsilon=0.3,
    core_dist_n_jobs=-1
)
```

**é—®é¢˜åˆ†æ**:
- Sentence Transformers çš„ 768 ç»´å‘é‡åº”è¯¥ç”¨ **cosine è·ç¦»**
- ç›´æ¥ç”¨ euclidean ä¼šå—å‘é‡èŒƒæ•°å½±å“
- å¯¼è‡´èšç±»è¾¹ç•Œä¸ç¨³å®šï¼Œå®¹æ˜“ç¢è£‚æˆå°ç°‡

**ä¿®å¤æ–¹æ¡ˆ**ï¼ˆäºŒé€‰ä¸€ï¼‰:

**æ–¹æ¡ˆ A: æ”¹ç”¨ cosine**
```python
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=min_cluster_size,
    min_samples=min_samples,
    metric='cosine',  # âœ… ä¿®æ”¹è¿™é‡Œ
    cluster_selection_method='leaf',
    cluster_selection_epsilon=0.3,
    core_dist_n_jobs=-1
)
```

**æ–¹æ¡ˆ B: å½’ä¸€åŒ– + euclidean**
```python
from sklearn.preprocessing import normalize

def perform_clustering(
    self,
    embeddings: np.ndarray,
    min_cluster_size: int = 8,
    min_samples: int = 3,
    metric: str = 'euclidean'
) -> np.ndarray:
    # å…ˆå½’ä¸€åŒ–
    embeddings = normalize(embeddings, norm='l2')  # âœ… æ·»åŠ è¿™è¡Œ

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric='euclidean',  # ä¿æŒ euclidean
        cluster_selection_method='leaf',
        cluster_selection_epsilon=0.3,
        core_dist_n_jobs=-1
    )

    cluster_labels = clusterer.fit_predict(embeddings)
    return cluster_labels
```

**é¢„æœŸæ•ˆæœ**:
- ç°‡æ›´ç¨³å®š
- å¾®ç°‡æ•°é‡å‡å°‘
- èšç±»è´¨é‡æå‡

**ä¼˜å…ˆçº§**: ğŸ”´ P0ï¼ˆæœ€åˆ’ç®—çš„æ”¹åŠ¨ï¼‰

---

### é—®é¢˜ 2: cluster_selection_method='leaf' å¯¼è‡´å¾®ç°‡çˆ†ç‚¸

**å½“å‰ä»£ç ** (`backend/services/clustering_service.py:354`):
```python
cluster_selection_method='leaf',  # âŒ å¤ªæ¿€è¿›
```

**é—®é¢˜åˆ†æ**:
- `leaf` æ–¹æ³•æ›´æ¿€è¿› â†’ ç°‡æ›´å¤šã€å™ªéŸ³æ›´å°‘
- ä¸‰é˜¶æ®µå åŠ ï¼ˆmin_size 10â†’5â†’3ï¼‰â†’ å¾®ç°‡çˆ†ç‚¸
- ç»“æœ: 725 ä¸ªå¾®ç°‡ã€575 ä¸ª 3-4 è§„æ¨¡ç°‡
- **è¿™äº›å¾®ç°‡åœ¨éœ€æ±‚æŒ–æ˜é‡Œé€šå¸¸ä¸å¯å‘½åã€ä¸å¯è¡ŒåŠ¨**

**ä¿®å¤æ–¹æ¡ˆ**:

```python
def perform_clustering(
    self,
    embeddings: np.ndarray,
    min_cluster_size: int = 8,
    min_samples: int = 3,
    metric: str = 'euclidean',
    use_eom: bool = False  # âœ… æ–°å¢å‚æ•°
) -> np.ndarray:
    """æ‰§è¡Œ HDBSCAN èšç±»"""

    # æ ¹æ®é˜¶æ®µé€‰æ‹©æ–¹æ³•
    method = 'eom' if use_eom else 'leaf'  # âœ… åŠ¨æ€é€‰æ‹©

    print(f"Performing HDBSCAN clustering...")
    print(f"  min_cluster_size: {min_cluster_size}")
    print(f"  min_samples: {min_samples}")
    print(f"  metric: {metric}")
    print(f"  cluster_selection_method: {method}")  # âœ… ä¿®æ”¹

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric=metric,
        cluster_selection_method=method,  # âœ… ä½¿ç”¨å˜é‡
        cluster_selection_epsilon=0.3,
        core_dist_n_jobs=-1
    )

    cluster_labels = clusterer.fit_predict(embeddings)
    return cluster_labels
```

**åœ¨ä¸‰é˜¶æ®µèšç±»ä¸­ä½¿ç”¨**:
```python
def perform_three_stage_clustering(
    self,
    embeddings: np.ndarray,
    product_ids: List[int],
    stage1_min_size: int = 10,
    stage2_min_size: int = 5,
    stage3_min_size: int = 3
) -> Tuple[np.ndarray, Dict]:
    """æ‰§è¡Œä¸‰é˜¶æ®µèšç±»"""

    # ============ ç¬¬ä¸€é˜¶æ®µ: ä¸»é¢˜ç°‡ï¼ˆé«˜ç²¾åº¦ï¼‰============
    print("\n========== Stage 1: Primary Clustering ==========")
    stage1_labels = self.perform_clustering(
        embeddings,
        min_cluster_size=stage1_min_size,
        min_samples=max(3, stage1_min_size // 2),
        use_eom=True  # âœ… ç¬¬ä¸€é˜¶æ®µç”¨ eomï¼ˆä¿å®ˆã€ç¨³å®šï¼‰
    )

    # ============ ç¬¬äºŒé˜¶æ®µ ============
    # ... (åç»­é˜¶æ®µå¯ä»¥ä¿ç•™ leaf)
```

**é¢„æœŸæ•ˆæœ**:
- Stage 1 ç”Ÿæˆæ›´ç¨³å®šçš„ä¸»é¢˜ç°‡
- ä¸»é¢˜ç°‡æ•°é‡å‡å°‘ï¼Œä½†è´¨é‡æå‡
- æ¯ä¸ªç°‡æ›´å®¹æ˜“å‘½åå’Œç†è§£

**ä¼˜å…ˆçº§**: ğŸ”´ P0

---

### é—®é¢˜ 3: cluster_selection_epsilon=0.3 ä¸å¯é 

**å½“å‰ä»£ç ** (`backend/services/clustering_service.py:354`):
```python
cluster_selection_epsilon=0.3,  # âŒ "ç»éªŒå€¼"æ— ä¾æ®
```

**é—®é¢˜åˆ†æ**:
- epsilon çš„æ„ä¹‰ä¸ distance metric å¼ºç›¸å…³
- åœ¨ 768 ç»´ + euclidean ä¸‹ï¼Œ"0.3 åˆç†"æ²¡æœ‰ä¾æ®
- æ¢æˆ cosine åï¼Œepsilon çš„æ„ä¹‰ä¼šå˜

**ä¿®å¤æ–¹æ¡ˆ**:

```python
def perform_clustering(
    self,
    embeddings: np.ndarray,
    min_cluster_size: int = 8,
    min_samples: int = 3,
    metric: str = 'euclidean',
    use_eom: bool = False,
    epsilon: float = 0.0  # âœ… æ–°å¢å‚æ•°ï¼Œé»˜è®¤å…³é—­
) -> np.ndarray:
    """æ‰§è¡Œ HDBSCAN èšç±»"""

    method = 'eom' if use_eom else 'leaf'

    print(f"Performing HDBSCAN clustering...")
    print(f"  min_cluster_size: {min_cluster_size}")
    print(f"  min_samples: {min_samples}")
    print(f"  metric: {metric}")
    print(f"  cluster_selection_method: {method}")
    print(f"  cluster_selection_epsilon: {epsilon}")  # âœ… æ˜¾ç¤º epsilon

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric=metric,
        cluster_selection_method=method,
        cluster_selection_epsilon=epsilon,  # âœ… ä½¿ç”¨å‚æ•°
        core_dist_n_jobs=-1
    )

    cluster_labels = clusterer.fit_predict(embeddings)
    return cluster_labels
```

**å®æ–½æ­¥éª¤**:
1. å…ˆè®¾ç½® `epsilon=0.0`ï¼ˆå…³é—­ï¼‰
2. ç­‰ metric é€‰å®šåï¼Œå†å°èŒƒå›´å®éªŒï¼ˆ0.0-0.2ï¼‰
3. æ ¹æ®å®é™…æ•ˆæœè°ƒæ•´

**é¢„æœŸæ•ˆæœ**:
- å‡å°‘ä¸ç¡®å®šæ€§
- èšç±»ç»“æœæ›´å¯æ§
- æ›´å®¹æ˜“è°ƒè¯•å’Œä¼˜åŒ–

**ä¼˜å…ˆçº§**: ğŸ”´ P0

---

### é—®é¢˜ 4: é¢„å¤„ç†ç­–ç•¥ä¸é€‚åˆéœ€æ±‚æŒ–æ˜

**å½“å‰ä»£ç ** (`backend/services/clustering_service.py:91-127`):
```python
def preprocess_text(self, text: str) -> str:
    """é¢„å¤„ç†å•†å“åç§°æ–‡æœ¬"""
    if not text:
        return ""

    # 1. è½¬å°å†™
    text = text.lower()

    # 2. å»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[|/\\()\[\]{}<>]', ' ', text)

    # 3. å»é™¤å°ºå¯¸ä¿¡æ¯
    text = re.sub(r'\d+x\d+', '', text)
    text = re.sub(r'\d+\s*(mm|cm|inch|in|ft|px)', '', text)

    # 4. å»é™¤åœç”¨è¯ï¼ˆåªæœ‰ 4 ä¸ªï¼‰
    stop_words = ['instant', 'download', 'file', 'files']
    for word in stop_words:
        text = re.sub(rf'\b{word}\b', '', text, flags=re.IGNORECASE)

    # 5. æ¸…ç†å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())

    return text.strip()
```

**é—®é¢˜åˆ†æ**:
- âŒ ä¿ç•™äº†é¢œè‰²/æè´¨/é£æ ¼è¯
- âŒ å¯¼è‡´èšç±»æŒ‰**å±æ€§**åˆ†ç»„ï¼Œè€Œé**éœ€æ±‚ä¸»é¢˜**åˆ†ç»„
- âŒ ä¾‹å¦‚:
  - "Pink Retro Shopify Theme" â†’ èšæˆ "Pink theme" / "Blue theme"
  - "Leather Wallet Template" â†’ èšæˆ "Leather wallet" / "Wood wallet"

**ä½ çœŸæ­£æƒ³è¦çš„**:
- âœ… "Shopify åº—é“ºä¸»é¢˜éœ€æ±‚"ï¼ˆä¸ç®¡ä»€ä¹ˆé¢œè‰²ï¼‰
- âœ… "é’±åŒ…åˆ¶ä½œ/é’±åŒ…æ¨¡æ¿éœ€æ±‚"ï¼ˆä¸ç®¡ä»€ä¹ˆæè´¨ï¼‰

**ä¿®å¤æ–¹æ¡ˆ - åŒæ–‡æœ¬ç­–ç•¥**:

```python
def preprocess_text(self, text: str) -> Tuple[str, str]:
    """
    é¢„å¤„ç†å•†å“åç§°æ–‡æœ¬

    è¿”å›:
        (topic_text, full_text)
        - topic_text: ç”¨äºèšç±»ï¼ˆå»é™¤å±æ€§è¯ï¼Œå¼ºè°ƒéœ€æ±‚ä¸»é¢˜ï¼‰
        - full_text: ç”¨äºå±•ç¤ºï¼ˆä¿ç•™å®Œæ•´ä¿¡æ¯ï¼Œç”¨ä½œç°‡å†…åˆ†é¢ï¼‰
    """
    if not text:
        return "", ""

    # ========== å®Œæ•´æ–‡æœ¬ï¼ˆç”¨äºå±•ç¤ºï¼‰==========
    full_text = text.lower()
    full_text = re.sub(r'[|/\\()\[\]{}<>]', ' ', full_text)
    full_text = re.sub(r'\d+x\d+', '', full_text)
    full_text = re.sub(r'\d+\s*(mm|cm|inch|in|ft|px)', '', full_text)
    full_text = ' '.join(full_text.split()).strip()

    # ========== ä¸»é¢˜æ–‡æœ¬ï¼ˆç”¨äºèšç±»ï¼‰==========
    topic_text = full_text

    # å»é™¤é¢œè‰²è¯
    color_words = [
        'red', 'blue', 'green', 'yellow', 'pink', 'purple',
        'black', 'white', 'gray', 'grey', 'brown', 'orange',
        'navy', 'teal', 'coral', 'mint', 'gold', 'silver',
        'beige', 'cream', 'ivory', 'maroon', 'olive'
    ]

    # å»é™¤æè´¨è¯
    material_words = [
        'leather', 'wood', 'wooden', 'metal', 'plastic', 'fabric',
        'cotton', 'silk', 'wool', 'paper', 'glass', 'ceramic',
        'stone', 'marble', 'granite', 'bamboo', 'canvas'
    ]

    # å»é™¤é£æ ¼è¯
    style_words = [
        'modern', 'vintage', 'retro', 'minimalist', 'elegant',
        'rustic', 'bohemian', 'boho', 'classic', 'contemporary',
        'traditional', 'industrial', 'scandinavian', 'farmhouse',
        'luxury', 'premium', 'deluxe', 'basic', 'simple'
    ]

    # å»é™¤æ—¶æ•ˆæ€§è¯æ±‡
    temporal_words = [
        'instant', 'download', 'digital', 'printable',
        'file', 'files', 'pdf', 'editable'
    ]

    # åˆå¹¶æ‰€æœ‰å±æ€§è¯
    attribute_words = color_words + material_words + style_words + temporal_words

    # å»é™¤å±æ€§è¯ï¼ˆä¿ç•™å•è¯è¾¹ç•Œï¼‰
    for word in attribute_words:
        topic_text = re.sub(rf'\b{word}\b', '', topic_text, flags=re.IGNORECASE)

    # æ¸…ç†å¤šä½™ç©ºæ ¼
    topic_text = ' '.join(topic_text.split()).strip()

    return topic_text, full_text
```

**ä¿®æ”¹å‘é‡åŒ–å‡½æ•°**:
```python
def vectorize_products(
    self,
    products: List[Tuple[int, str]]
) -> Tuple[np.ndarray, List[int], List[str], List[str]]:
    """
    å‘é‡åŒ–å•†å“åç§°

    è¿”å›:
        (embeddings, product_ids, topic_texts, full_texts)
    """
    product_ids = []
    topic_texts = []
    full_texts = []
    embeddings_list = []

    for product_id, product_name in products:
        # é¢„å¤„ç†å¾—åˆ°ä¸¤ä¸ªç‰ˆæœ¬
        topic_text, full_text = self.preprocess_text(product_name)

        if not topic_text:
            continue

        product_ids.append(product_id)
        topic_texts.append(topic_text)
        full_texts.append(full_text)

        # ä½¿ç”¨ topic_text ç”Ÿæˆå‘é‡ï¼ˆç”¨äºèšç±»ï¼‰
        text_hash = hashlib.md5(topic_text.encode()).hexdigest()
        cache_file = self.cache_dir / f"{text_hash}.pkl"

        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                embedding = pickle.load(f)
        else:
            embedding = self.model.encode(topic_text)
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)

        embeddings_list.append(embedding)

    embeddings = np.array(embeddings_list)
    return embeddings, product_ids, topic_texts, full_texts
```

**é¢„æœŸæ•ˆæœ**:
- èšç±»æŒ‰éœ€æ±‚ä¸»é¢˜åˆ†ç»„ï¼ˆä¸æŒ‰é¢œè‰²/æè´¨/é£æ ¼ï¼‰
- ç°‡æ›´å®¹æ˜“å‘½åå’Œç†è§£
- full_text å¯ç”¨äºç°‡å†…åˆ†é¢å’Œç­›é€‰

**ä¼˜å…ˆçº§**: ğŸ”´ P0

---

## ğŸ”¬ ä¸‰ä¸ªå¿«é€Ÿå®éªŒï¼ˆ10 åˆ†é’ŸéªŒè¯æ–¹å‘ï¼‰

### å®éªŒ 1: cosine vs euclidean

**ç›®æ ‡**: éªŒè¯ metric å¯¹èšç±»è´¨é‡çš„å½±å“

**æ­¥éª¤**:
1. å¤‡ä»½å½“å‰æ•°æ®åº“
2. ä¿®æ”¹ `metric='cosine'`
3. æ‰§è¡Œèšç±»
4. å¯¹æ¯”ç»“æœ:
   - ç°‡æ•°é‡å˜åŒ–
   - å¾®ç°‡æ•°é‡å˜åŒ–
   - ç°‡çš„ç¨³å®šæ€§

**é¢„æœŸ**:
- ç°‡æ›´ç¨³å®š
- å¾®ç°‡å‡å°‘
- èšç±»è´¨é‡æå‡

---

### å®éªŒ 2: stage1: eom vs leaf

**ç›®æ ‡**: éªŒè¯ cluster_selection_method å¯¹ä¸»é¢˜ç°‡è´¨é‡çš„å½±å“

**æ­¥éª¤**:
1. åªä¿®æ”¹ Stage 1 çš„ method
2. æ‰§è¡Œèšç±»
3. å¯¹æ¯” Stage 1 çš„ç°‡:
   - ç°‡æ•°é‡
   - ç°‡å¤§å°åˆ†å¸ƒ
   - ç°‡çš„å¯å‘½åæ€§

**é¢„æœŸ**:
- Stage 1 ç°‡æ•°é‡å‡å°‘
- æ¯ä¸ªç°‡æ›´ç¨³å®šã€æ›´å®¹æ˜“å‘½å
- ä¸»é¢˜ç°‡è´¨é‡æå‡

---

### å®éªŒ 3: åŒæ–‡æœ¬ç­–ç•¥

**ç›®æ ‡**: éªŒè¯å»é™¤å±æ€§è¯å¯¹èšç±»çš„å½±å“

**æ­¥éª¤**:
1. å®ç°åŒæ–‡æœ¬ç­–ç•¥
2. æ‰§è¡Œèšç±»
3. å¯¹æ¯”ç»“æœ:
   - ç°‡çš„ä¸»é¢˜æ˜¯å¦æ›´æ¸…æ™°
   - æ˜¯å¦æŒ‰éœ€æ±‚ä¸»é¢˜åˆ†ç»„ï¼ˆè€Œéå±æ€§ï¼‰
   - ç°‡çš„å¯å‘½åæ€§

**é¢„æœŸ**:
- ç°‡æŒ‰éœ€æ±‚ä¸»é¢˜åˆ†ç»„
- ä¸å†æŒ‰é¢œè‰²/æè´¨/é£æ ¼åˆ†ç»„
- ç°‡æ›´å®¹æ˜“å‘½åå’Œç†è§£

---

## ğŸ“Š æœ€ä¼˜ä¸‰é˜¶æ®µç­–ç•¥ï¼ˆé•¿æœŸç›®æ ‡ï¼‰

### å½“å‰ä¸‰é˜¶æ®µ vs æœ€ä¼˜ä¸‰é˜¶æ®µ

| é˜¶æ®µ | å½“å‰åšæ³• | æœ€ä¼˜åšæ³• | ç›®æ ‡å˜åŒ– |
|------|---------|---------|---------|
| **Stage 1** | min_size=10, leaf, euclidean | min_size=10+, **eom**, **cosine** | é«˜ç²¾åº¦ä¸»é¢˜ç°‡ |
| **Stage 2** | å¯¹å™ªéŸ³ç‚¹å†èšç±» (min_size=5) | **å½’å¹¶åˆ°æœ€è¿‘ä¸»é¢˜ç°‡** (cosine > 0.75) | ä¸åˆ¶é€ æ–°ç°‡ |
| **Stage 3** | å¯¹å™ªéŸ³ç‚¹å†èšç±» (min_size=3) | å°ç°‡èšç±» + **è´¨é‡é—¨æ§** | åªä¿ç•™å¯å‘½åç°‡ |

### Stage 1: ä¸»é¢˜ç°‡ï¼ˆé«˜ç²¾åº¦ï¼‰

**ç›®æ ‡**: ç”Ÿæˆç¨³å®šã€å¯å‘½åçš„ä¸»é¢˜ç°‡

**å‚æ•°**:
```python
stage1_labels = self.perform_clustering(
    embeddings,
    min_cluster_size=10,  # æˆ–æ›´é«˜
    min_samples=5,
    metric='cosine',
    use_eom=True,  # ä¿å®ˆã€ç¨³å®š
    epsilon=0.0
)
```

**è¾“å‡º**: è¿™æ‰¹ç°‡æ˜¯"æ­£å¼ä¸»é¢˜ç°‡"

---

### Stage 2: å½’å¹¶åˆ°æœ€è¿‘ä¸»é¢˜ç°‡ï¼ˆä¸å†èšç±»ï¼‰

**ç›®æ ‡**: å°†å™ªéŸ³ç‚¹å½’å¹¶åˆ°æœ€è¿‘çš„ä¸»é¢˜ç°‡ï¼Œè€Œä¸æ˜¯åˆ¶é€ æ–°ç°‡

**å®ç°**:
```python
def merge_noise_to_clusters(
    self,
    embeddings: np.ndarray,
    labels: np.ndarray,
    threshold: float = 0.75
) -> np.ndarray:
    """
    å°†å™ªéŸ³ç‚¹å½’å¹¶åˆ°æœ€è¿‘çš„ä¸»é¢˜ç°‡

    Args:
        embeddings: æ‰€æœ‰å•†å“çš„å‘é‡
        labels: å½“å‰èšç±»æ ‡ç­¾ï¼ˆ-1 è¡¨ç¤ºå™ªéŸ³ï¼‰
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆcosine similarityï¼‰

    Returns:
        æ›´æ–°åçš„æ ‡ç­¾
    """
    from sklearn.metrics.pairwise import cosine_similarity

    # æ‰¾å‡ºå™ªéŸ³ç‚¹å’Œç°‡ç‚¹
    noise_mask = labels == -1
    cluster_mask = labels != -1

    if not np.any(noise_mask) or not np.any(cluster_mask):
        return labels

    # è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒï¼ˆcentroidï¼‰
    unique_labels = np.unique(labels[cluster_mask])
    cluster_centers = {}

    for label in unique_labels:
        cluster_points = embeddings[labels == label]
        cluster_centers[label] = np.mean(cluster_points, axis=0)

    # å¯¹æ¯ä¸ªå™ªéŸ³ç‚¹ï¼Œæ‰¾æœ€è¿‘çš„ç°‡
    noise_embeddings = embeddings[noise_mask]
    noise_indices = np.where(noise_mask)[0]

    updated_labels = labels.copy()

    for i, noise_idx in enumerate(noise_indices):
        noise_vec = noise_embeddings[i].reshape(1, -1)

        # è®¡ç®—ä¸æ‰€æœ‰ç°‡ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
        max_similarity = -1
        best_cluster = -1

        for label, center in cluster_centers.items():
            center_vec = center.reshape(1, -1)
            similarity = cosine_similarity(noise_vec, center_vec)[0][0]

            if similarity > max_similarity:
                max_similarity = similarity
                best_cluster = label

        # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œå½’å¹¶åˆ°è¯¥ç°‡
        if max_similarity >= threshold:
            updated_labels[noise_idx] = best_cluster

    return updated_labels
```

**åœ¨ä¸‰é˜¶æ®µä¸­ä½¿ç”¨**:
```python
# Stage 1: ä¸»é¢˜ç°‡
stage1_labels = self.perform_clustering(...)

# Stage 2: å½’å¹¶å™ªéŸ³ç‚¹ï¼ˆä¸å†èšç±»ï¼‰
stage2_labels = self.merge_noise_to_clusters(
    embeddings,
    stage1_labels,
    threshold=0.75
)
```

---

### Stage 3: å°ç°‡èšç±» + è´¨é‡é—¨æ§

**ç›®æ ‡**: å¯¹å‰©ä½™å™ªéŸ³ç‚¹èšç±»ï¼Œä½†åªä¿ç•™é«˜è´¨é‡ã€å¯å‘½åçš„ç°‡

**å®ç°**:
```python
def filter_low_quality_clusters(
    self,
    embeddings: np.ndarray,
    labels: np.ndarray,
    product_texts: List[str],
    min_internal_similarity: float = 0.6,
    min_keyword_distinctiveness: float = 0.3
) -> np.ndarray:
    """
    è¿‡æ»¤ä½è´¨é‡ç°‡

    è´¨é‡æ ‡å‡†:
    1. å†…éƒ¨ç›¸ä¼¼åº¦ > é˜ˆå€¼
    2. å…³é”®è¯åŒºåˆ†åº¦ > é˜ˆå€¼
    3. å¯å‘½åæ€§ï¼ˆé€šè¿‡å…³é”®è¯è¦†ç›–ç‡åˆ¤æ–­ï¼‰

    Args:
        embeddings: å‘é‡
        labels: èšç±»æ ‡ç­¾
        product_texts: å•†å“æ–‡æœ¬
        min_internal_similarity: æœ€å°å†…éƒ¨ç›¸ä¼¼åº¦
        min_keyword_distinctiveness: æœ€å°å…³é”®è¯åŒºåˆ†åº¦

    Returns:
        è¿‡æ»¤åçš„æ ‡ç­¾ï¼ˆä½è´¨é‡ç°‡æ ‡è®°ä¸º -1ï¼‰
    """
    from sklearn.metrics.pairwise import cosine_similarity
    from collections import Counter

    unique_labels = np.unique(labels[labels != -1])
    updated_labels = labels.copy()

    for label in unique_labels:
        cluster_mask = labels == label
        cluster_embeddings = embeddings[cluster_mask]
        cluster_texts = [product_texts[i] for i in np.where(cluster_mask)[0]]

        # è´¨é‡æ£€æŸ¥ 1: å†…éƒ¨ç›¸ä¼¼åº¦
        if len(cluster_embeddings) > 1:
            similarities = cosine_similarity(cluster_embeddings)
            avg_similarity = (similarities.sum() - len(similarities)) / (len(similarities) * (len(similarities) - 1))

            if avg_similarity < min_internal_similarity:
                # å†…éƒ¨ç›¸ä¼¼åº¦å¤ªä½ï¼Œæ ‡è®°ä¸ºå™ªéŸ³
                updated_labels[cluster_mask] = -1
                continue

        # è´¨é‡æ£€æŸ¥ 2: å…³é”®è¯åŒºåˆ†åº¦
        # æå–ç°‡å†…é«˜é¢‘è¯
        all_words = []
        for text in cluster_texts:
            all_words.extend(text.split())

        word_counts = Counter(all_words)
        top_words = [word for word, count in word_counts.most_common(3)]

        # æ£€æŸ¥è¿™äº›è¯åœ¨å…¶ä»–ç°‡ä¸­çš„å‡ºç°é¢‘ç‡
        # ï¼ˆç®€åŒ–ç‰ˆï¼šè¿™é‡Œå¯ä»¥æ›´å¤æ‚ï¼‰
        if len(top_words) == 0:
            # æ— æ³•æå–å…³é”®è¯ï¼Œæ ‡è®°ä¸ºå™ªéŸ³
            updated_labels[cluster_mask] = -1
            continue

    return updated_labels
```

**åœ¨ä¸‰é˜¶æ®µä¸­ä½¿ç”¨**:
```python
# Stage 3: å°ç°‡èšç±»
stage3_labels = self.perform_clustering(
    remaining_noise_embeddings,
    min_cluster_size=3,
    min_samples=2,
    metric='cosine',
    use_eom=False,  # å¯ä»¥ç”¨ leaf
    epsilon=0.0
)

# è´¨é‡é—¨æ§
stage3_labels = self.filter_low_quality_clusters(
    remaining_noise_embeddings,
    stage3_labels,
    remaining_noise_texts,
    min_internal_similarity=0.6,
    min_keyword_distinctiveness=0.3
)
```

---

## ğŸ“… å®æ–½è®¡åˆ’

### Phase 1: å¿«é€Ÿå®éªŒï¼ˆ1-2 å¤©ï¼‰

**ç›®æ ‡**: éªŒè¯æ–¹å‘æ˜¯å¦æ­£ç¡®

- [ ] å®éªŒ 1: cosine vs euclidean
- [ ] å®éªŒ 2: stage1: eom vs leaf
- [ ] å®éªŒ 3: åŒæ–‡æœ¬ç­–ç•¥
- [ ] è®°å½•å®éªŒç»“æœ
- [ ] å†³å®šæ˜¯å¦ç»§ç»­

---

### Phase 2: P0 é—®é¢˜ä¿®å¤ï¼ˆ2-3 å¤©ï¼‰

**ç›®æ ‡**: ä¿®å¤å››ä¸ªå…³é”®é—®é¢˜

- [ ] ä¿®æ”¹ metric ä¸º cosine
- [ ] Stage 1 ä½¿ç”¨ eom
- [ ] å…³é—­ epsilonï¼ˆè®¾ä¸º 0.0ï¼‰
- [ ] å®ç°åŒæ–‡æœ¬ç­–ç•¥
- [ ] æµ‹è¯•éªŒè¯
- [ ] æ›´æ–°æ–‡æ¡£

---

### Phase 3: é‡æ–°è®¾è®¡ Stage 2/3ï¼ˆ3-5 å¤©ï¼‰

**ç›®æ ‡**: å®ç°æœ€ä¼˜ä¸‰é˜¶æ®µç­–ç•¥

- [ ] å®ç° Stage 2 å½’å¹¶é€»è¾‘
- [ ] å®ç° Stage 3 è´¨é‡é—¨æ§
- [ ] æµ‹è¯•éªŒè¯
- [ ] å¯¹æ¯”æ–°æ—§æ–¹æ¡ˆ
- [ ] æ›´æ–°æ–‡æ¡£

---

### Phase 4: è´¨é‡è¯„ä¼°ï¼ˆ1-2 å¤©ï¼‰

**ç›®æ ‡**: è¯„ä¼°ä¼˜åŒ–æ•ˆæœ

- [ ] ç°‡æ•°é‡å¯¹æ¯”
- [ ] ç°‡è´¨é‡å¯¹æ¯”ï¼ˆå¯å‘½åæ€§ï¼‰
- [ ] å™ªéŸ³æ¯”ä¾‹å¯¹æ¯”
- [ ] ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
- [ ] å†³å®šæ˜¯å¦ä¸Šçº¿

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### å®šé‡æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | è¯´æ˜ |
|------|------|------|------|
| æ€»ç°‡æ•° | 1,412 | 300-500 | å‡å°‘å¾®ç°‡ |
| ä¸»è¦ç°‡ (â‰¥10) | 335 | 200-300 | æ›´ç¨³å®š |
| å¾®å‹ç°‡ (3-4) | 575 | 50-100 | å¤§å¹…å‡å°‘ |
| å™ªéŸ³æ¯”ä¾‹ | 29.26% | 30-40% | å¯æ¥å— |
| å¯å‘½åç°‡æ¯”ä¾‹ | ~50% | >80% | å¤§å¹…æå‡ |

### å®šæ€§æŒ‡æ ‡

- âœ… ç°‡æŒ‰éœ€æ±‚ä¸»é¢˜åˆ†ç»„ï¼ˆä¸æŒ‰å±æ€§ï¼‰
- âœ… æ¯ä¸ªç°‡å®¹æ˜“å‘½åå’Œç†è§£
- âœ… ç°‡å†…å•†å“è¯­ä¹‰ä¸€è‡´æ€§é«˜
- âœ… å¯ç”¨äºéœ€æ±‚åˆ¤æ–­å’Œå†³ç­–
- âœ… ç³»ç»Ÿæ›´å¯æ§ã€æ›´æ˜“è°ƒè¯•

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### å¿…é¡»è¾¾æˆï¼ˆP0ï¼‰

1. **ç°‡è´¨é‡æå‡**: å¯å‘½åç°‡æ¯”ä¾‹ > 80%
2. **å¾®ç°‡å‡å°‘**: å¾®å‹ç°‡ (3-4) < 100 ä¸ª
3. **ä¸»é¢˜æ¸…æ™°**: ç°‡æŒ‰éœ€æ±‚ä¸»é¢˜åˆ†ç»„ï¼Œä¸æŒ‰å±æ€§åˆ†ç»„

### æœŸæœ›è¾¾æˆï¼ˆP1ï¼‰

4. **æ€»ç°‡æ•°å‡å°‘**: 300-500 ä¸ªï¼ˆè€Œé 1,412 ä¸ªï¼‰
5. **ç³»ç»Ÿå¯æ§**: å‚æ•°è°ƒæ•´åæ•ˆæœå¯é¢„æµ‹
6. **æ–‡æ¡£å®Œå–„**: æ‰€æœ‰æ”¹åŠ¨æœ‰æ–‡æ¡£è®°å½•

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **GPT æŠ€æœ¯è¯„å®¡**: `docs/gptå›å¤.md`
2. **å½“å‰å®ç°**: `backend/services/clustering_service.py`
3. **èšç±»æŠ€æœ¯æ–‡æ¡£**: `docs/èšç±»æŠ€æœ¯æ–‡æ¡£/`
4. **æµ‹è¯•æŠ¥å‘Š**: `docs/å•†å“ç®¡ç†æ¨¡å—/èšç±»åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š.md`

---

## ğŸ”„ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¼€å§‹**:
1. é˜…è¯»æœ¬æ–‡æ¡£
2. ç¡®è®¤å®æ–½è®¡åˆ’
3. å¼€å§‹ Phase 1: å¿«é€Ÿå®éªŒ

**éœ€è¦å†³ç­–**:
- é€‰æ‹© metric ä¿®å¤æ–¹æ¡ˆï¼ˆcosine æˆ– normalize+euclideanï¼‰
- ç¡®å®šå®éªŒé¡ºåº
- ç¡®å®šæ—¶é—´å®‰æ’

---

**åˆ›å»ºè€…**: Claude Sonnet 4.5
**æœ€åæ›´æ–°**: 2026-02-02
