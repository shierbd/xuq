# è‹±æ–‡å…³é”®è¯éœ€æ±‚æŒ–æ˜ç³»ç»Ÿ - ä¼˜åŒ–å®æ–½è®¡åˆ’

> **è®¡åˆ’ç›®æ ‡**ï¼šåœ¨ä¿æŒè‹±æ–‡SEOåœºæ™¯æ ¸å¿ƒéœ€æ±‚çš„å‰æä¸‹ï¼Œå¸æ”¶å›è¨€å…³é”®è¯å¤„ç†æ–¹æ³•è®ºçš„æ ¸å¿ƒä¼˜åŠ¿
>
> **æ ¸å¿ƒåŸåˆ™**ï¼šéœ€æ±‚ç»Ÿä¸€ï¼Œå…³é”®è¯ä¸åˆå¹¶ï¼›ä¸‰å±‚æ•°æ®æ¨¡å‹ï¼›è¯çº§å¤„ç†ï¼ˆéå­—ç¬¦çº§ï¼‰
>
> **åˆ›å»ºæ—¥æœŸ**ï¼š2025-12-23

---

## ğŸ“‘ ç›®å½•

1. [å½“å‰ç³»ç»ŸåŠŸèƒ½è¯„ä¼°](#å½“å‰ç³»ç»ŸåŠŸèƒ½è¯„ä¼°)
2. [æ ¸å¿ƒé—®é¢˜è¯Šæ–­](#æ ¸å¿ƒé—®é¢˜è¯Šæ–­)
3. [ä¼˜åŒ–æˆ˜ç•¥ç›®æ ‡](#ä¼˜åŒ–æˆ˜ç•¥ç›®æ ‡)
4. [æ¶æ„é‡æ„æ–¹æ¡ˆ](#æ¶æ„é‡æ„æ–¹æ¡ˆ)
5. [åˆ†é˜¶æ®µå®æ–½è®¡åˆ’](#åˆ†é˜¶æ®µå®æ–½è®¡åˆ’)
6. [ä¸å›è¨€ç³»ç»Ÿå¯¹ç…§](#ä¸å›è¨€ç³»ç»Ÿå¯¹ç…§)
7. [é¢„æœŸæˆæœ](#é¢„æœŸæˆæœ)

---

## å½“å‰ç³»ç»ŸåŠŸèƒ½è¯„ä¼°

### ç³»ç»Ÿç°çŠ¶ï¼ˆMVPç‰ˆæœ¬ï¼‰

```yaml
æŠ€æœ¯æ ˆ:
  - èšç±»ç®—æ³•: HDBSCAN (æˆç†Ÿã€å¯é )
  - Embedding: Sentence-BERT (all-MiniLM-L6-v2, 384-dim)
  - è‡ªåŠ¨åŒ–: LLM (DeepSeek/OpenAI)
  - UI: Streamlit Web UI
  - æ•°æ®åº“: MySQL/SQLite + SQLAlchemy ORM

æ•°æ®è§„æ¨¡:
  - å½“å‰: 55,275 phrases
  - è®¾è®¡ç›®æ ‡: 5-10ä¸‡è‹±æ–‡çŸ­è¯­
  - æœªæ¥æ‰©å±•: æ”¯æŒåˆ°50ä¸‡+

å·¥ä½œæµç¨‹:
  Phase 1: æ•°æ®å¯¼å…¥ (CSV/Excel)
  Phase 2: å¤§ç»„èšç±» (HDBSCAN, 60-100ç°‡)
  Phase 3: äººå·¥ç­›é€‰ (é€‰æ‹©10-15ä¸ªå¤§ç»„)
  Phase 4: å°ç»„èšç±» + LLMç”Ÿæˆéœ€æ±‚å¡ç‰‡
  Phase 5: Tokenæå– + LLMåˆ†ç±» (4ç±»: intent/action/object/other)

å½“å‰æˆæœ:
  - 307ä¸ªå¤§ç»„èšç±»
  - 26ä¸ªn-gram tokens (æµ‹è¯•æ•°æ®)
  - åŸºç¡€éœ€æ±‚å¡ç‰‡ç”Ÿæˆ
```

### âœ… å½“å‰ç³»ç»Ÿä¼˜åŠ¿

| ä¼˜åŠ¿é¡¹ | å…·ä½“è¡¨ç° | ä¸å›è¨€å¯¹æ¯” |
|--------|----------|-----------|
| **æˆç†ŸæŠ€æœ¯æ ˆ** | HDBSCANç¨³å®šå¯é  | å›è¨€ç”¨è‡ªç ”ç®—æ³• |
| **é«˜åº¦è‡ªåŠ¨åŒ–** | LLMè‡ªåŠ¨ç”Ÿæˆéœ€æ±‚å¡ç‰‡å’Œåˆ†ç±» | å›è¨€éœ€è¦å¤§é‡äººå·¥å®¡æ ¸ |
| **ç”¨æˆ·å‹å¥½** | Streamlit Web UIï¼Œå®æ—¶æ—¥å¿— | å›è¨€æ— UIå±•ç¤º |
| **é€‚é…åœºæ™¯** | ä¸“ä¸ºè‹±æ–‡å…³é”®è¯è®¾è®¡ | å›è¨€ä¸ºä¸­æ–‡è®¾è®¡ |
| **æ•°æ®ä¿ç•™** | å®Œæ•´ä¿ç•™æ‰€æœ‰åŸå§‹çŸ­è¯­ | âœ… è¿™ç‚¹ä¸€è‡´ |

### âŒ å½“å‰ç³»ç»Ÿä¸è¶³

| ä¸è¶³é¡¹ | å½“å‰çŠ¶æ€ | ä¸å›è¨€å·®è· | å½±å“ |
|--------|---------|-----------|------|
| **æ•°æ®æ¸…æ´—æ·±åº¦** | ä»…åŸºç¡€å»é‡ | å›è¨€æœ‰æ–‡å­—æ’åºå»é‡ï¼ˆ20%é¢å¤–å»é‡ï¼‰ | æ•°æ®å†—ä½™ï¼Œæµªè´¹è®¡ç®— |
| **æ¶æ„è®¾è®¡** | å•å±‚æ•°æ®æ¨¡å‹ | å›è¨€è™½æ— æ˜ç¡®ä¸‰å±‚ï¼Œä½†ç†å¿µç±»ä¼¼ | SEOæ‰§è¡Œä¸åˆ†ææ··æ·† |
| **ç‰¹å¾æå–** | æ— ç‰‡æ®µæ˜ å°„æœºåˆ¶ | å›è¨€æ ¸å¿ƒåˆ›æ–°ï¼š180ä¸‡â†’2ä¸‡æ ·æœ¬ | å¤§è§„æ¨¡æ•°æ®ä¸å¯å®¡æ ¸ |
| **è¯åº“æ„å»º** | ä»…26ä¸ªtoken | å›è¨€ï¼š8000+æ¸ é“è¯ï¼Œæ•°åƒåŠŸèƒ½/å¯¹è±¡è¯ | è¯åº“ä¸å®Œæ•´ |
| **è¿­ä»£æœºåˆ¶** | æ— æ¨¡æ¿-å˜é‡è¿­ä»£ | å›è¨€ï¼š3è½®è¿­ä»£è‡ªåŠ¨æ‰©å±• | è¯åº“æ— æ³•è‡ªåŠ¨ç”Ÿé•¿ |
| **éœ€æ±‚åˆ†ç±»** | æ— åˆ†ç±»ä½“ç³» | å›è¨€ï¼š6å¤§ç±»åˆ«+å æ¯”åˆ†æï¼ˆ95%å¯»æ‰¾ç±»ï¼‰ | ç¼ºå°‘ä¸šåŠ¡æ´å¯Ÿ |
| **ä¸­è‹±å·®å¼‚** | æœªå……åˆ†è€ƒè™‘è‹±æ–‡ç‰¹æ€§ | - | ç›´æ¥ç…§æ¬ä¸­æ–‡ç®—æ³•ä¼šå¤±è´¥ |

---

## æ ¸å¿ƒé—®é¢˜è¯Šæ–­

### é—®é¢˜1ï¼šæ¶æ„å±‚é¢ - å•å±‚æ•°æ®æ¨¡å‹çš„å›°å¢ƒ

**ç°çŠ¶**ï¼š
```
æ‰€æœ‰çŸ­è¯­å­˜å‚¨åœ¨ phrases è¡¨
  â†“
ç›´æ¥è¿›è¡Œèšç±»åˆ†æ
  â†“
ç”Ÿæˆéœ€æ±‚å¡ç‰‡
```

**é—®é¢˜**ï¼š
- âŒ SEOæ‰§è¡Œéœ€è¦ï¼šä¿ç•™æ‰€æœ‰å…³é”®è¯å˜ä½“ï¼ˆbest image compressor / image compressor best / top image compressorï¼‰
- âŒ åˆ†æéœ€è¦ï¼šç»Ÿä¸€ç›¸åŒéœ€æ±‚ï¼Œå‡å°‘é‡å¤è®¡ç®—
- âŒ ä¸¤è€…å†²çªï¼šåˆ è¯å½±å“SEOï¼Œä¸åˆ è¯å½±å“åˆ†æè´¨é‡

**å›è¨€ç³»ç»Ÿçš„éšå«æ€è·¯**ï¼ˆè™½æœªæ˜ç¡®åˆ†å±‚ï¼Œä½†ç†å¿µç±»ä¼¼ï¼‰ï¼š
```
åŸå§‹æ•°æ®ï¼ˆ1.6äº¿ï¼‰ â†’ æ¸…æ´—å»é‡ï¼ˆ4000ä¸‡ï¼‰ â†’ èšç±»ï¼ˆ180ä¸‡æ ‡è¯†ï¼‰ â†’ ç‰‡æ®µæ˜ å°„ï¼ˆ2ä¸‡æ ·æœ¬ï¼‰
```
- å›è¨€è™½ç„¶åˆ äº†æ•°æ®ï¼Œä½†ä»–ä»¬çš„åœºæ™¯æ˜¯"å‘ç°éœ€æ±‚"ï¼Œä¸æ˜¯"SEOæ‰§è¡Œ"
- æˆ‘ä»¬éœ€è¦çš„æ˜¯ï¼š**ä¿ç•™åŸå§‹æ•°æ® + å¢åŠ åˆ†æå±‚**

### é—®é¢˜2ï¼šå¤„ç†å•å…ƒ - å­—ç¬¦çº§ vs è¯çº§

**ç°çŠ¶åˆ†æ**ï¼š
- å½“å‰N-gramæå–ï¼š1-4 gramï¼ˆè¯çº§ï¼Œæ­£ç¡®âœ…ï¼‰
- å¦‚æœç›´æ¥ç…§æ¬å›è¨€çš„"æ–‡å­—æ’åºå»é‡"ï¼šä¼šå˜æˆå­—ç¬¦çº§ï¼ˆé”™è¯¯âŒï¼‰

**ä¸­è‹±æ–‡æ ¹æœ¬å·®å¼‚**ï¼š

| ç»´åº¦ | ä¸­æ–‡ï¼ˆå›è¨€ï¼‰ | è‹±æ–‡ï¼ˆæˆ‘ä»¬ï¼‰ |
|------|-------------|------------|
| **æœ€å°å•å…ƒ** | å­—ï¼ˆä¸€å­—ä¸€æ„ï¼‰ | è¯ï¼ˆwordï¼‰ |
| **æ’åºå¯¹è±¡** | å­—çš„æ‹¼éŸ³ | è¯æœ¬èº« |
| **N-gram** | 3-5å­—ç‰‡æ®µ | 2-4è¯ç‰‡æ®µ |
| **å»é‡ç¤ºä¾‹** | "å›¾ç‰‡å‹ç¼©"â†’"å›¾å‹ç‰‡ç¼©" | "best calculator"â†’"best calculator"ï¼ˆè¯çº§æ’åºï¼‰ |

**é”™è¯¯ç¤ºèŒƒ**ï¼ˆå­—ç¬¦çº§æ’åºï¼‰ï¼š
```python
# âŒ é”™è¯¯ï¼šç›´æ¥ç…§æ¬å›è¨€çš„å­—ç¬¦æ’åº
"best calculator for students"
  â†’ å»åœç”¨è¯: "best calculator students"
  â†’ å­—ç¬¦æ’åº: "abcbcelllorssttu"  # å®Œå…¨å¤±å»è¯­ä¹‰ï¼
```

**æ­£ç¡®åšæ³•**ï¼ˆè¯çº§æ’åºï¼‰ï¼š
```python
# âœ… æ­£ç¡®ï¼šè¯çº§è§„èŒƒåŒ–
"best calculator for students"
  â†’ åˆ†è¯: ["best", "calculator", "for", "students"]
  â†’ å»åœç”¨è¯: ["best", "calculator", "students"]
  â†’ è¯å½¢è¿˜åŸ: ["best", "calculator", "student"]
  â†’ è¯çº§æ’åº: ["best", "calculator", "student"]
  â†’ canonical form: "best calculator student"
```

### é—®é¢˜3ï¼šåŠŸèƒ½å±‚é¢ - ç¼ºå°‘æ ¸å¿ƒåˆ›æ–°æœºåˆ¶

**å›è¨€çš„ä¸¤å¤§æ ¸å¿ƒåˆ›æ–°**ï¼š

1. **ç‰¹å¾ç‰‡æ®µæ˜ å°„**ï¼ˆPhase 4çš„çªç ´ï¼‰
   ```
   é—®é¢˜ï¼š180ä¸‡èšç±»æ ‡è¯† â†’ äººå·¥æ— æ³•å®¡æ ¸
   å›è¨€æ–¹æ¡ˆï¼š
     Step 1: å…¨å±€N-gramç»Ÿè®¡ï¼ˆ3-5å­—ç‰‡æ®µï¼‰
     Step 2: é€‰æ‹©Top 1ä¸‡é«˜é¢‘ç‰‡æ®µ
     Step 3: æ¯ä¸ªç‰‡æ®µæå–2ä¸ªæ¯è¯
     Step 4: å¾—åˆ°2ä¸‡æ ·æœ¬è¯
     Step 5: å¯¹æ ·æœ¬èšç±»ï¼ˆå¿«é€Ÿï¼ï¼‰
     Step 6: äººå·¥å®¡æ ¸2ä¸‡æ ·æœ¬ï¼ˆ<2å°æ—¶ï¼‰
   ä»·å€¼ï¼šä»"ä¸å¯èƒ½"åˆ°"2å°æ—¶å®Œæˆ"
   ```

2. **æ¨¡æ¿-å˜é‡è¿­ä»£**ï¼ˆPhase 5çš„çªç ´ï¼‰
   ```
   é—®é¢˜ï¼šè¯åº“å¤ªå°ï¼Œæ— æ³•è¦†ç›–éœ€æ±‚
   å›è¨€æ–¹æ¡ˆï¼š
     Step 1: ç§å­å˜é‡ï¼ˆå¾®ä¿¡ã€æŠ–éŸ³ã€æ¸…ç†ã€å‹ç¼©...ï¼‰
     Step 2: ç”¨å˜é‡æå–æ¨¡æ¿ï¼ˆ"[X]è½¯ä»¶ä¸‹è½½"ï¼‰
     Step 3: ç”¨æ¨¡æ¿æå–æ›´å¤šå˜é‡
     Step 4: è´¨é‡è¿‡æ»¤ï¼ˆå˜é‡éœ€é€‚é…â‰¥3ä¸ªæ¨¡æ¿ï¼‰
     Step 5: è¿­ä»£3è½®æ”¶æ•›
   ä»·å€¼ï¼šè¯åº“ä»å‡ ä¸ªæ‰©å±•åˆ°8000+
   ```

**æˆ‘ä»¬å½“å‰çŠ¶æ€**ï¼š
- âœ… æœ‰N-gramæå–åŸºç¡€
- âŒ æ— ç‰‡æ®µæ˜ å°„æœºåˆ¶
- âŒ æ— æ¨¡æ¿-å˜é‡è¿­ä»£
- âŒ è¯åº“è§„æ¨¡å°ï¼ˆ26ä¸ª vs 8000+ï¼‰

---

## ä¼˜åŒ–æˆ˜ç•¥ç›®æ ‡

### ç›®æ ‡1ï¼šæ¶æ„å‡çº§ - å»ºç«‹ä¸‰å±‚æ•°æ®æ¨¡å‹

**æˆ˜ç•¥å®šä½**ï¼š
```
ç¬¬1å±‚ï¼šåŸå§‹å…³é”®è¯å±‚ï¼ˆSEOæ‰§è¡Œå±‚ï¼‰
  - ä¿ç•™æ‰€æœ‰å…³é”®è¯å˜ä½“
  - æ¯ä¸ªè¯ç‹¬ç«‹ä¿ç•™ volumeã€KDã€CPC
  - ç”¨äºå®é™…SEO/ç«å“æ‰§è¡Œ
  - åŸåˆ™ï¼šä¸€æ¡éƒ½ä¸åˆ é™¤

ç¬¬2å±‚ï¼šè§„èŒƒåŒ–è¡¨è¾¾å±‚ï¼ˆç»Ÿè®¡å»å™ªå±‚ï¼‰
  - è§„èŒƒåŒ–keyï¼ˆè¯çº§æ’åº+è¯å½¢è¿˜åŸï¼‰
  - ç”¨äºèšç±»ã€N-gramç»Ÿè®¡
  - å‡å°‘é‡å¤è®¡ç®—
  - åŸåˆ™ï¼šä¸åˆ é™¤æ•°æ®ï¼Œåªå¢åŠ æ˜ å°„å…³ç³»

ç¬¬3å±‚ï¼šéœ€æ±‚/æœºä¼šå±‚ï¼ˆè¯­ä¹‰ç»Ÿä¸€å±‚ï¼‰
  - èšåˆå¤šä¸ªè§„èŒƒåŒ–è¡¨è¾¾
  - éœ€æ±‚å¡ç‰‡ + åˆ†ç±»
  - æˆ˜ç•¥å†³ç­–å±‚
  - åŸåˆ™ï¼šç»Ÿä¸€éœ€æ±‚ï¼Œä½†ä¿ç•™æ‰€æœ‰å…³é”®è¯å¼•ç”¨
```

**ç›®æ ‡ä»·å€¼**ï¼š
- âœ… è§£å†³SEOæ‰§è¡Œ vs åˆ†æéœ€æ±‚çš„å†²çª
- âœ… ä¿ç•™å…³é”®è¯å¤šæ ·æ€§ï¼ˆvolumeã€KDã€CPCç‹¬ç«‹ï¼‰
- âœ… æå‡åˆ†æè´¨é‡ï¼ˆç»Ÿè®¡å»å™ªï¼‰
- âœ… æ”¯æŒæˆ˜ç•¥å†³ç­–ï¼ˆéœ€æ±‚åœ°å›¾ï¼‰

### ç›®æ ‡2ï¼šéœ€æ±‚æ¡†æ¶é‡æ„

**å½“å‰æ¡†æ¶**ï¼š
```
[æ„å›¾æ¡†æ¶è¯] + [æ ¸å¿ƒéœ€æ±‚è¯] + [é¢†åŸŸ/å¯¹è±¡è¯] + [ç»“æœ/å±æ€§è¯] + [æ¡ä»¶/é™åˆ¶è¯]
```

**æ–°æ¡†æ¶**ï¼ˆ4ç»´ç»“æ„ï¼‰ï¼š

```
ç»´åº¦Aï¼šéœ€æ±‚ç±»å‹ï¼ˆIntent Typeï¼‰- ç»Ÿé¢†å…¨å±€
  â”œâ”€ Search/Findï¼ˆå¯»æ‰¾ï¼‰- é¢„è®¡å æ¯”70-80%
  â”‚   â”œâ”€ Downloadï¼ˆä¸‹è½½ï¼‰
  â”‚   â”œâ”€ Recommendï¼ˆæ¨è/å¯¹æ¯”ï¼‰
  â”‚   â”œâ”€ Freeï¼ˆå…è´¹èµ„æºï¼‰
  â”‚   â””â”€ Alternativeï¼ˆæ›¿ä»£å“ï¼‰
  â”œâ”€ Operate/Useï¼ˆæ“ä½œ/ä½¿ç”¨ï¼‰
  â”œâ”€ Fix/Troubleshootï¼ˆé—®é¢˜/æ•…éšœï¼‰
  â”œâ”€ Price/Costï¼ˆè¯¢ä»·ï¼‰
  â”œâ”€ Learn/Tutorialï¼ˆå­¦ä¹ /æ•™ç¨‹ï¼‰
  â””â”€ Otherï¼ˆå…¶ä»–ï¼‰

ç»´åº¦Bï¼šç‰¹å¾å˜é‡ï¼ˆFeature Variablesï¼‰- æ ¸å¿ƒè¯åº“ï¼ˆå›è¨€4å¤§ç±»ï¼‰
  â”œâ”€ Channelï¼ˆæ¸ é“ï¼‰
  â”‚   ç¤ºä¾‹ï¼šgoogle, youtube, reddit, twitter, tiktok, shopify, wordpress...
  â”œâ”€ Objectï¼ˆå¯¹è±¡ï¼‰
  â”‚   ç¤ºä¾‹ï¼šimage, video, pdf, text, domain, email, password, invoice...
  â”œâ”€ Functionï¼ˆåŠŸèƒ½/åŠ¨ä½œï¼‰
  â”‚   ç¤ºä¾‹ï¼šcompress, resize, convert, extract, generate, analyze, monitor...
  â””â”€ Audienceï¼ˆç¾¤ä½“ï¼‰
      ç¤ºä¾‹ï¼šstudents, developers, marketers, designers, small business...

ç»´åº¦Cï¼šç»“æœ/å±æ€§ï¼ˆOutcome/Attributeï¼‰- ä¿®é¥°ç»´åº¦1
  â”œâ”€ è´¨é‡ï¼šbest, top, cheap, fast, reliable, secure, professional...
  â”œâ”€ ä»·æ ¼ï¼šfree, paid, lifetime, subscription, no credit card...
  â””â”€ æ•ˆæœï¼šhigh-converting, SEO-friendly, mobile-friendly...

ç»´åº¦Dï¼šæ¡ä»¶/é™åˆ¶ï¼ˆConstraintï¼‰- ä¿®é¥°ç»´åº¦2
  â”œâ”€ åœºæ™¯ï¼šfor students, for Mac, for Shopify, for beginners...
  â”œâ”€ èŒƒå›´ï¼šnear me, in US, online, offline, no download...
  â””â”€ æ—¶é—´ï¼š2025, this year, real-time, daily...
```

**æ˜ å°„å…³ç³»**ï¼š
```
åŸæ¡†æ¶ â†’ æ–°æ¡†æ¶
  [æ„å›¾æ¡†æ¶è¯] â†’ ç»´åº¦Aï¼ˆéœ€æ±‚ç±»å‹ï¼‰
  [æ ¸å¿ƒéœ€æ±‚è¯] â†’ ç»´åº¦Bï¼ˆåŠŸèƒ½ç±»ï¼‰
  [é¢†åŸŸ/å¯¹è±¡è¯] â†’ ç»´åº¦Bï¼ˆå¯¹è±¡ç±»+æ¸ é“ç±»ï¼‰
  [ç»“æœ/å±æ€§è¯] â†’ ç»´åº¦C
  [æ¡ä»¶/é™åˆ¶è¯] â†’ ç»´åº¦D
```

**ä»·å€¼**ï¼š
- âœ… ä¸å›è¨€4å¤§ç‰¹å¾å˜é‡å¯¹é½
- âœ… å¢åŠ æœç´¢æ„å›¾ç»´åº¦ï¼ˆSEOæ´å¯Ÿï¼‰
- âœ… ç»“æ„æ›´æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- âœ… æ”¯æŒåç»­æ¨¡æ¿-å˜é‡è¿­ä»£

### ç›®æ ‡3ï¼šå¸æ”¶å›è¨€æ ¸å¿ƒåˆ›æ–°ï¼ˆè‹±æ–‡åŒ–æ”¹é€ ï¼‰

**åˆ›æ–°1ï¼šè§„èŒƒåŒ–è¡¨è¾¾**ï¼ˆå›è¨€çš„"æ–‡å­—æ’åºå»é‡"è‹±æ–‡ç‰ˆï¼‰

```python
ç›®æ ‡ï¼šè¯†åˆ«åŒä¹‰è¡¨è¾¾ï¼Œä½†ä¸åˆ é™¤å…³é”®è¯

å›è¨€ç‰ˆæœ¬ï¼ˆä¸­æ–‡ï¼‰ï¼š
  "å›¾ç‰‡å‹ç¼©" + "å‹ç¼©å›¾ç‰‡" â†’ å”¯ä¸€æ ‡è¯†ï¼š"å›¾å‹ç‰‡ç¼©"

æˆ‘ä»¬çš„ç‰ˆæœ¬ï¼ˆè‹±æ–‡ï¼‰ï¼š
  "best calculator for students" â†’ canonical: "best calculator student"
  "calculator best for students" â†’ canonical: "best calculator student"
  "students best calculator" â†’ canonical: "best calculator student"

å…³é”®å·®å¼‚ï¼š
  - ä¸­æ–‡ï¼šå­—çº§æ’åºï¼ˆæ‹¼éŸ³ï¼‰
  - è‹±æ–‡ï¼šè¯çº§æ’åºï¼ˆwordï¼‰+ è¯å½¢è¿˜åŸï¼ˆstemmingï¼‰
```

**åˆ›æ–°2ï¼šç‰¹å¾ç‰‡æ®µæ˜ å°„**ï¼ˆå›è¨€Phase 4æ ¸å¿ƒï¼‰

```python
ç›®æ ‡ï¼šä»æµ·é‡æ•°æ®ä¸­æå–å¯å®¡æ ¸æ ·æœ¬

å›è¨€ç‰ˆæœ¬ï¼ˆä¸­æ–‡ï¼‰ï¼š
  4000ä¸‡æ•°æ® â†’ 3-5å­—ç‰‡æ®µç»Ÿè®¡ â†’ Top 1ä¸‡ç‰‡æ®µ â†’ 2ä¸‡æ ·æœ¬è¯

æˆ‘ä»¬çš„ç‰ˆæœ¬ï¼ˆè‹±æ–‡ï¼‰ï¼š
  50ä¸‡æ•°æ® â†’ 2-4è¯ç‰‡æ®µç»Ÿè®¡ â†’ Top 5åƒç‰‡æ®µ â†’ 1ä¸‡æ ·æœ¬è¯

å…³é”®å·®å¼‚ï¼š
  - ä¸­æ–‡ï¼šå­—çº§N-gramï¼ˆ3-5å­—ï¼‰
  - è‹±æ–‡ï¼šè¯çº§N-gramï¼ˆ2-4è¯ï¼‰
  - ç¤ºä¾‹ï¼š"best image compressor" â†’ ["best image", "image compressor", "best image compressor"]
```

**åˆ›æ–°3ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£**ï¼ˆå›è¨€Phase 5æ ¸å¿ƒï¼‰

```python
ç›®æ ‡ï¼šè‡ªåŠ¨æ„å»ºå®Œæ•´ç‰¹å¾è¯åº“

å›è¨€ç‰ˆæœ¬ï¼ˆä¸­æ–‡ï¼‰ï¼š
  ç§å­ï¼š["å¾®ä¿¡","æŠ–éŸ³","æ¸…ç†","å‹ç¼©"]
  â†’ æ¨¡æ¿ï¼š["[X]è½¯ä»¶ä¸‹è½½","[X]è½¯ä»¶å“ªä¸ªå¥½"]
  â†’ å˜é‡ï¼š["å¾®åš","å°çº¢ä¹¦","æ‹ç…§","å½•å±"]
  â†’ è¿­ä»£3è½® â†’ 8000+æ¸ é“è¯ï¼Œå‡ åƒåŠŸèƒ½/å¯¹è±¡è¯

æˆ‘ä»¬çš„ç‰ˆæœ¬ï¼ˆè‹±æ–‡ï¼‰ï¼š
  ç§å­ï¼š["google","youtube","compress","convert"]
  â†’ æ¨¡æ¿ï¼š["best [X] for","how to [X]","[X] online"]
  â†’ å˜é‡ï¼š["tiktok","reddit","resize","extract"]
  â†’ è¿­ä»£3è½® â†’ é¢„è®¡æ•°åƒè¯ï¼ˆChannel/Object/Function/Audienceï¼‰

å…³é”®å·®å¼‚ï¼š
  - ä¸­æ–‡ï¼šå›ºå®šå¥å¼ï¼ˆ"XXè½¯ä»¶ä¸‹è½½"ï¼‰
  - è‹±æ–‡ï¼šçµæ´»æ§½ä½ï¼ˆ"best [X] for [Y]"ï¼‰
  - è´¨é‡è¿‡æ»¤ï¼šå˜é‡éœ€é€‚é…â‰¥3ä¸ªæ¨¡æ¿
```

---

## æ¶æ„é‡æ„æ–¹æ¡ˆ

### æ–°æ•°æ®åº“Schemaè®¾è®¡

#### ç¬¬1å±‚ï¼šåŸå§‹å…³é”®è¯å±‚

```sql
-- 1. åŸå§‹çŸ­è¯­è¡¨ï¼ˆä¸åšä»»ä½•åˆ é™¤ï¼Œå®Œæ•´ä¿ç•™ï¼‰
CREATE TABLE phrases (
    phrase_id INT PRIMARY KEY AUTO_INCREMENT,
    phrase VARCHAR(255) UNIQUE NOT NULL,

    -- æ¥æºä¿¡æ¯
    source VARCHAR(50),  -- 'semrush', 'autocomplete', 'related', 'ads'
    import_round INT DEFAULT 1,

    -- SEOæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
    search_volume INT,
    keyword_difficulty DECIMAL(5,2),
    cpc DECIMAL(10,2),
    serp_features TEXT,  -- JSON: ["featured_snippet", "people_also_ask"]

    -- æ˜ å°„å…³ç³»ï¼ˆæŒ‡å‘ç¬¬2å±‚å’Œç¬¬3å±‚ï¼‰
    canonical_id INT,  -- è§„èŒƒåŒ–è¡¨è¾¾IDï¼ˆç¬¬2å±‚ï¼‰
    mapped_demand_id INT,  -- éœ€æ±‚IDï¼ˆç¬¬3å±‚ï¼‰

    -- çŠ¶æ€ç®¡ç†
    processed_status ENUM('unseen', 'reviewed', 'assigned', 'archived') DEFAULT 'unseen',

    -- èšç±»ä¿¡æ¯ï¼ˆä¿ç•™åŸæœ‰å­—æ®µï¼‰
    cluster_id_A INT,  -- å¤§ç»„èšç±»ID
    cluster_id_B INT,  -- å°ç»„èšç±»ID

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    INDEX idx_canonical (canonical_id),
    INDEX idx_demand (mapped_demand_id),
    INDEX idx_source (source),
    INDEX idx_volume (search_volume DESC)
);

-- è¯´æ˜ï¼š
-- âœ… ä¿ç•™æ‰€æœ‰å…³é”®è¯å˜ä½“
-- âœ… ä¿ç•™volumeã€KDã€CPCç­‰SEOæŒ‡æ ‡
-- âœ… é€šè¿‡canonical_idå’Œmapped_demand_idå»ºç«‹æ˜ å°„ï¼Œä¸åˆ é™¤æ•°æ®
```

#### ç¬¬2å±‚ï¼šè§„èŒƒåŒ–è¡¨è¾¾å±‚

```sql
-- 2. è§„èŒƒåŒ–è¡¨è¾¾è¡¨ï¼ˆç»Ÿè®¡å»å™ªå±‚ï¼‰
CREATE TABLE canonical_forms (
    canonical_id INT PRIMARY KEY AUTO_INCREMENT,
    canonical_text TEXT NOT NULL,  -- è§„èŒƒåŒ–åçš„æ–‡æœ¬

    -- ç»Ÿè®¡ä¿¡æ¯
    keyword_count INT DEFAULT 0,  -- è¯¥è¡¨è¾¾æ—ä¸‹æœ‰å¤šå°‘å…³é”®è¯
    total_volume INT DEFAULT 0,   -- æ€»æœç´¢é‡ï¼ˆèšåˆï¼‰
    avg_difficulty DECIMAL(5,2),  -- å¹³å‡éš¾åº¦

    -- ç”¨äºèšç±»çš„å‘é‡ï¼ˆç¼“å­˜ï¼‰
    embedding_round INT,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    UNIQUE KEY idx_canonical_text (canonical_text(255)),
    INDEX idx_keyword_count (keyword_count DESC)
);

-- è¯´æ˜ï¼š
-- âœ… ä¸€ä¸ªcanonical_formå¯¹åº”å¤šä¸ªåŸå§‹phrase
-- âœ… ç”¨äºèšç±»åˆ†ææ—¶å‡å°‘é‡å¤è®¡ç®—
-- âœ… èšåˆç»Ÿè®¡ï¼ˆæ€»æœç´¢é‡ã€å¹³å‡éš¾åº¦ï¼‰
```

#### ç¬¬3å±‚ï¼šéœ€æ±‚/æœºä¼šå±‚

```sql
-- 3. éœ€æ±‚è¡¨ï¼ˆè¯­ä¹‰ç»Ÿä¸€å±‚ï¼‰
CREATE TABLE demands (
    demand_id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(255) NOT NULL,
    description TEXT,

    -- éœ€æ±‚åˆ†ç±»ï¼ˆæ–°å¢4ç»´æ¡†æ¶ï¼‰
    intent_type VARCHAR(50),      -- ç»´åº¦A: 'search', 'operate', 'fix', 'price', 'learn', 'other'
    intent_subtype VARCHAR(50),   -- å­ç±»å‹: 'download', 'recommend', 'free', 'alternative'...

    demand_type ENUM('tool', 'content', 'service', 'education', 'other'),  -- ä¿ç•™åŸæœ‰äº§å“ç±»å‹åˆ†ç±»

    -- ç»Ÿè®¡ä¿¡æ¯
    canonical_count INT DEFAULT 0,  -- è¦†ç›–å¤šå°‘ä¸ªè§„èŒƒåŒ–è¡¨è¾¾
    keyword_count INT DEFAULT 0,     -- è¦†ç›–å¤šå°‘ä¸ªåŸå§‹å…³é”®è¯
    total_volume INT DEFAULT 0,      -- æ€»æœç´¢é‡

    -- æœºä¼šè¯„åˆ†ï¼ˆå¯é€‰ï¼‰
    opportunity_score DECIMAL(5,2),

    created_by VARCHAR(50) DEFAULT 'llm',  -- 'llm' or 'manual'
    reviewed BOOLEAN DEFAULT FALSE,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    INDEX idx_intent (intent_type, intent_subtype),
    INDEX idx_opportunity (opportunity_score DESC)
);

-- 4. éœ€æ±‚-è§„èŒƒåŒ–æ˜ å°„è¡¨ï¼ˆå¤šå¯¹å¤šå…³ç³»ï¼‰
CREATE TABLE demand_canonical_mapping (
    id INT PRIMARY KEY AUTO_INCREMENT,
    demand_id INT NOT NULL,
    canonical_id INT NOT NULL,

    FOREIGN KEY (demand_id) REFERENCES demands(demand_id) ON DELETE CASCADE,
    FOREIGN KEY (canonical_id) REFERENCES canonical_forms(canonical_id) ON DELETE CASCADE,

    UNIQUE KEY idx_mapping (demand_id, canonical_id)
);

-- è¯´æ˜ï¼š
-- âœ… éœ€æ±‚å±‚ï¼šç»Ÿä¸€è¯­ä¹‰ï¼Œèšåˆå¤šä¸ªè§„èŒƒåŒ–è¡¨è¾¾
-- âœ… å¢åŠ æœç´¢æ„å›¾åˆ†ç±»ï¼ˆintent_typeï¼‰
-- âœ… ä¿ç•™äº§å“ç±»å‹åˆ†ç±»ï¼ˆdemand_typeï¼‰
-- âœ… é€šè¿‡mappingè¡¨å»ºç«‹éœ€æ±‚ä¸å…³é”®è¯çš„å®Œæ•´å…³ç³»é“¾
```

#### ç‰¹å¾è¯åº“è¡¨ï¼ˆæ”¯æŒæ¨¡æ¿-å˜é‡è¿­ä»£ï¼‰

```sql
-- 5. ç‰¹å¾å˜é‡è¡¨ï¼ˆç»´åº¦Bï¼š4å¤§ç±»è¯åº“ï¼‰
CREATE TABLE feature_variables (
    id INT PRIMARY KEY AUTO_INCREMENT,
    variable_text VARCHAR(100) UNIQUE NOT NULL,

    -- å˜é‡åˆ†ç±»ï¼ˆå›è¨€4å¤§ç±»ï¼‰
    category ENUM('channel', 'object', 'function', 'audience') NOT NULL,

    -- ç»Ÿè®¡ä¿¡æ¯
    in_phrase_count INT DEFAULT 0,     -- å‡ºç°åœ¨å¤šå°‘ä¸ªçŸ­è¯­ä¸­
    template_count INT DEFAULT 0,       -- é€‚é…å¤šå°‘ä¸ªæ¨¡æ¿

    -- è´¨é‡æ§åˆ¶
    verified BOOLEAN DEFAULT FALSE,     -- æ˜¯å¦äººå·¥éªŒè¯
    confidence ENUM('high', 'medium', 'low') DEFAULT 'medium',

    -- è¯å½¢ä¿¡æ¯
    gram_size INT DEFAULT 1,  -- 1=å•è¯, 2=2-gram, etc.
    original_form TEXT,       -- åŸå§‹å½¢æ€ï¼ˆæœªstemmingï¼‰

    notes TEXT,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_category (category),
    INDEX idx_verified (verified),
    INDEX idx_template_count (template_count DESC)
);

-- 6. æœç´¢æ¨¡æ¿è¡¨ï¼ˆæ”¯æŒæ¨¡æ¿-å˜é‡è¿­ä»£ï¼‰
CREATE TABLE search_templates (
    id INT PRIMARY KEY AUTO_INCREMENT,
    template TEXT UNIQUE NOT NULL,  -- å¦‚ "best [X] for [Y]"

    -- ç»Ÿè®¡ä¿¡æ¯
    frequency INT NOT NULL,      -- åŒ¹é…çš„å…³é”®è¯æ•°
    variable_count INT DEFAULT 1,  -- æ¨¡æ¿ä¸­æœ‰å‡ ä¸ªå˜é‡æ§½ä½

    -- æ¨¡æ¿åˆ†ç±»
    intent_type VARCHAR(50),  -- å…³è”åˆ°éœ€æ±‚ç±»å‹

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_frequency (frequency DESC)
);

-- 7. æ¨¡æ¿-å˜é‡å…³è”è¡¨
CREATE TABLE template_variable_mapping (
    id INT PRIMARY KEY AUTO_INCREMENT,
    template_id INT NOT NULL,
    variable_id INT NOT NULL,

    frequency INT DEFAULT 0,  -- è¯¥å˜é‡åœ¨è¯¥æ¨¡æ¿ä¸­å‡ºç°æ¬¡æ•°

    FOREIGN KEY (template_id) REFERENCES search_templates(id) ON DELETE CASCADE,
    FOREIGN KEY (variable_id) REFERENCES feature_variables(id) ON DELETE CASCADE,

    UNIQUE KEY idx_mapping (template_id, variable_id)
);
```

#### N-gramç‰‡æ®µè¡¨ï¼ˆæ”¯æŒç‰¹å¾ç‰‡æ®µåˆ†æï¼‰

```sql
-- 8. N-gramç‰‡æ®µè¡¨ï¼ˆè¯çº§ï¼Œéå­—ç¬¦çº§ï¼‰
CREATE TABLE ngram_segments (
    id INT PRIMARY KEY AUTO_INCREMENT,
    segment TEXT UNIQUE NOT NULL,  -- å¦‚ "best image compressor"

    frequency INT NOT NULL,
    gram_size INT NOT NULL,  -- 2-gram, 3-gram, 4-gram

    -- ç”¨äºç‰‡æ®µæ˜ å°„
    is_high_frequency BOOLEAN DEFAULT FALSE,  -- æ˜¯å¦åœ¨Top-Kä¸­
    sample_keywords TEXT,  -- JSON: ["keyword1", "keyword2"]ï¼ˆæ¯è¯æ ·æœ¬ï¼‰

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_frequency (frequency DESC),
    INDEX idx_gram_size (gram_size),
    INDEX idx_high_freq (is_high_frequency)
);
```

### æ•°æ®æµè½¬å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ•°æ®æµè½¬å…¨æ™¯å›¾                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€è¾“å…¥ã€‘åŸå§‹CSV/Excel
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬1å±‚ï¼šåŸå§‹å…³é”®è¯å±‚ï¼ˆphrasesè¡¨ï¼‰                                 â”‚
â”‚   - å®Œæ•´ä¿ç•™æ‰€æœ‰å…³é”®è¯                                           â”‚
â”‚   - è®°å½•volumeã€KDã€CPC                                        â”‚
â”‚   - è®°å½•æ¥æºã€å¯¼å…¥è½®æ¬¡                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (è§„èŒƒåŒ–å¤„ç†)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬2å±‚ï¼šè§„èŒƒåŒ–è¡¨è¾¾å±‚ï¼ˆcanonical_formsè¡¨ï¼‰                         â”‚
â”‚   - è¯çº§è§„èŒƒåŒ–ï¼šå»åœç”¨è¯+è¯å½¢è¿˜åŸ+è¯æ’åº                          â”‚
â”‚   - phrasesè¡¨é€šè¿‡canonical_idæ˜ å°„                              â”‚
â”‚   - èšåˆç»Ÿè®¡ï¼škeyword_count, total_volume, avg_difficulty     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (èšç±»åˆ†æ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding + HDBSCANèšç±»ï¼ˆåŸºäºcanonical_formsï¼‰                  â”‚
â”‚   - å¯¹è§„èŒƒåŒ–è¡¨è¾¾è®¡ç®—embedding                                    â”‚
â”‚   - å‡å°‘é‡å¤è®¡ç®—                                                 â”‚
â”‚   - å¾—åˆ°cluster_id                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (ç‰¹å¾ç‰‡æ®µåˆ†æ - æ–°å¢)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4.5ï¼šç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼ˆngram_segmentsè¡¨ï¼‰                      â”‚
â”‚   - å…¨å±€è¯çº§N-gramç»Ÿè®¡ï¼ˆ2-4è¯ï¼‰                                  â”‚
â”‚   - é€‰æ‹©Top 5åƒé«˜é¢‘ç‰‡æ®µ                                          â”‚
â”‚   - æ¯ä¸ªç‰‡æ®µæå–2ä¸ªæ¯è¯æ ·æœ¬                                       â”‚
â”‚   - å¾—åˆ°1ä¸‡æ ·æœ¬è¯ â†’ å¿«é€Ÿèšç±» â†’ äººå·¥å®¡æ ¸                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (éœ€æ±‚è¯†åˆ«)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬3å±‚ï¼šéœ€æ±‚/æœºä¼šå±‚ï¼ˆdemandsè¡¨ï¼‰                                  â”‚
â”‚   - LLMç”Ÿæˆéœ€æ±‚å¡ç‰‡                                              â”‚
â”‚   - éœ€æ±‚åˆ†ç±»ï¼ˆintent_type + demand_typeï¼‰                       â”‚
â”‚   - é€šè¿‡demand_canonical_mappingå…³è”æ‰€æœ‰å…³é”®è¯                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (ç‰¹å¾æå–)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 5.5ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆfeature_variables + search_templatesï¼‰â”‚
â”‚   - ç§å­å˜é‡ â†’ æå–æ¨¡æ¿                                          â”‚
â”‚   - æ¨¡æ¿ â†’ æå–æ›´å¤šå˜é‡                                          â”‚
â”‚   - è¿­ä»£3è½®ï¼Œè´¨é‡è¿‡æ»¤ï¼ˆé€‚é…â‰¥3æ¨¡æ¿ï¼‰                              â”‚
â”‚   - æ„å»º4å¤§ç±»è¯åº“ï¼ˆChannel/Object/Function/Audienceï¼‰           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ã€è¾“å‡ºã€‘ä¸‰å±‚æ•°æ®è§†å›¾
    â”œâ”€ éœ€æ±‚åœ°å›¾è§†å›¾ï¼ˆå†³ç­–å±‚ï¼‰
    â”œâ”€ å…³é”®è¯&è¯åº“è§†å›¾ï¼ˆç­–ç•¥å±‚ï¼‰
    â””â”€ å…³é”®è¯-éœ€æ±‚-äº§å“æ˜ å°„è¡¨ï¼ˆæ‰§è¡Œå±‚ï¼‰
```

---

## åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### Phase 0ï¼šå‡†å¤‡é˜¶æ®µï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**ï¼šæ•°æ®åº“å‡çº§ï¼Œä»£ç é‡æ„å‡†å¤‡

#### ä»»åŠ¡æ¸…å•

**0.1 æ•°æ®åº“Schemaå‡çº§**
```sql
-- æ–°å¢è¡¨
CREATE TABLE canonical_forms (...);
CREATE TABLE demand_canonical_mapping (...);
CREATE TABLE feature_variables (...);
CREATE TABLE search_templates (...);
CREATE TABLE template_variable_mapping (...);
CREATE TABLE ngram_segments (...);

-- ä¿®æ”¹ç°æœ‰è¡¨
ALTER TABLE phrases ADD COLUMN canonical_id INT;
ALTER TABLE phrases ADD COLUMN search_volume INT;
ALTER TABLE phrases ADD COLUMN keyword_difficulty DECIMAL(5,2);
ALTER TABLE phrases ADD COLUMN cpc DECIMAL(10,2);
ALTER TABLE phrases ADD COLUMN source VARCHAR(50);

ALTER TABLE demands ADD COLUMN intent_type VARCHAR(50);
ALTER TABLE demands ADD COLUMN intent_subtype VARCHAR(50);
ALTER TABLE demands ADD COLUMN canonical_count INT DEFAULT 0;
ALTER TABLE demands ADD COLUMN keyword_count INT DEFAULT 0;
```

**0.2 ä»£ç æ¨¡å—é‡æ„**
```
æ–°å¢æ¨¡å—ï¼š
  - core/canonical.py         # è§„èŒƒåŒ–è¡¨è¾¾ç”Ÿæˆ
  - core/segment_analysis.py  # ç‰¹å¾ç‰‡æ®µåˆ†æï¼ˆPhase 4.5ï¼‰
  - core/template_extraction.py  # æ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆPhase 5.5ï¼‰
  - core/intent_classifier.py    # æœç´¢æ„å›¾åˆ†ç±»

ä¿®æ”¹æ¨¡å—ï¼š
  - core/data_integration.py  # å¢åŠ è§„èŒƒåŒ–é€»è¾‘
  - storage/models.py         # å¢åŠ æ–°è¡¨æ¨¡å‹
  - storage/repository.py     # å¢åŠ æ–°è¡¨çš„Repository
```

**0.3 åœç”¨è¯åº“æ‰©å……**
```python
# config/stopwords_en.pyï¼ˆæ–°å»ºï¼‰
STOP_WORDS_EN = {
    # åŸæœ‰åŸºç¡€åœç”¨è¯
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
    'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
    'to', 'was', 'will', 'with',

    # æ–°å¢ï¼šç–‘é—®è¯
    'what', 'when', 'where', 'who', 'which', 'why', 'how',

    # æ–°å¢ï¼šä»‹è¯
    'about', 'after', 'before', 'between', 'during', 'under', 'over',
    'through', 'into', 'onto', 'upon',

    # æ–°å¢ï¼šåŠ¨è¯
    'can', 'could', 'do', 'does', 'did', 'get', 'have', 'had',
    'make', 'may', 'might', 'must', 'should', 'would',

    # æ–°å¢ï¼šå…¶ä»–ï¼ˆSEOåˆ†ææ—¶å¯è¿‡æ»¤ï¼Œä½†ä¿ç•™åœ¨åŸå§‹æ•°æ®ï¼‰
    'top', 'best', 'good', 'better', 'new', 'old', 'free', 'online',
    'near', 'me', 'my', 'you', 'your'
}

# è¯´æ˜ï¼š
# - è¿™äº›è¯åœ¨è§„èŒƒåŒ–æ—¶å¯ä»¥å»é™¤ï¼ˆåˆ†æç”¨ï¼‰
# - ä½†åœ¨åŸå§‹å…³é”®è¯å±‚å¿…é¡»ä¿ç•™ï¼ˆSEOæ‰§è¡Œç”¨ï¼‰
```

**é¢„æœŸäº§å‡º**ï¼š
- âœ… æ•°æ®åº“Schemaå‡çº§å®Œæˆ
- âœ… ä»£ç æ¡†æ¶é‡æ„å®Œæˆ
- âœ… åœç”¨è¯åº“å‡†å¤‡å°±ç»ª

---

### Phase 1ï¼šè§„èŒƒåŒ–è¡¨è¾¾å±‚ï¼ˆ2å‘¨ï¼‰

**ç›®æ ‡**ï¼šå»ºç«‹ç¬¬2å±‚æ•°æ®æ¨¡å‹ï¼Œå®ç°è¯çº§è§„èŒƒåŒ–

#### 1.1 è§„èŒƒåŒ–ç®—æ³•å®ç°

**æ–‡ä»¶**ï¼š`core/canonical.py`

```python
# core/canonical.py
import re
from typing import List, Set
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

class CanonicalFormGenerator:
    """
    è§„èŒƒåŒ–è¡¨è¾¾ç”Ÿæˆå™¨ï¼ˆè¯çº§å¤„ç†ï¼Œéå­—ç¬¦çº§ï¼‰

    æ ¸å¿ƒåŸåˆ™ï¼š
    1. è¯çº§æ’åºï¼ˆä¸æ˜¯å­—ç¬¦çº§ï¼‰
    2. è¯å½¢è¿˜åŸï¼ˆstemming/lemmatizationï¼‰
    3. å»åœç”¨è¯
    4. ä¸åˆ é™¤åŸå§‹æ•°æ®ï¼Œåªç”Ÿæˆæ˜ å°„
    """

    def __init__(self, stop_words: Set[str]):
        self.stop_words = stop_words
        self.stemmer = PorterStemmer()

    def generate(self, phrase: str) -> str:
        """
        ç”Ÿæˆè§„èŒƒåŒ–è¡¨è¾¾

        ç¤ºä¾‹ï¼š
          "best image compressor for students"
          â†’ åˆ†è¯: ["best", "image", "compressor", "for", "students"]
          â†’ å»åœç”¨è¯: ["image", "compressor", "students"]
          â†’ è¯å½¢è¿˜åŸ: ["image", "compressor", "student"]
          â†’ è¯çº§æ’åº: ["compressor", "image", "student"]
          â†’ æ‹¼æ¥: "compressor image student"
        """
        # 1. è½¬å°å†™
        phrase = phrase.lower().strip()

        # 2. åˆ†è¯
        words = word_tokenize(phrase)

        # 3. å»é™¤åœç”¨è¯å’Œæ ‡ç‚¹
        words = [w for w in words if w.isalnum() and w not in self.stop_words]

        # 4. è¯å½¢è¿˜åŸ
        words = [self.stemmer.stem(w) for w in words]

        # 5. è¯çº§æ’åºï¼ˆå…³é”®ï¼šä¸æ˜¯å­—ç¬¦æ’åºï¼ï¼‰
        words = sorted(words)

        # 6. æ‹¼æ¥
        return ' '.join(words)

    def generate_with_metadata(self, phrase: str) -> dict:
        """
        ç”Ÿæˆè§„èŒƒåŒ–è¡¨è¾¾ + å…ƒæ•°æ®
        """
        canonical = self.generate(phrase)

        return {
            'canonical': canonical,
            'original': phrase,
            'word_count': len(canonical.split()),
            'has_stopwords': any(w in phrase.lower() for w in self.stop_words)
        }

# ä½¿ç”¨ç¤ºä¾‹
from config.stopwords_en import STOP_WORDS_EN

generator = CanonicalFormGenerator(STOP_WORDS_EN)

# æµ‹è¯•ç”¨ä¾‹
test_phrases = [
    "best image compressor for students",
    "image compressor best for students",
    "students best image compressor",
    "free online image compressor"
]

for phrase in test_phrases:
    canonical = generator.generate(phrase)
    print(f"{phrase:45s} â†’ {canonical}")

# é¢„æœŸè¾“å‡ºï¼š
# best image compressor for students           â†’ compressor image student
# image compressor best for students           â†’ compressor image student
# students best image compressor               â†’ compressor image student
# free online image compressor                 â†’ compressor image

# âœ… æˆåŠŸè¯†åˆ«åŒä¹‰è¡¨è¾¾
# âœ… ä¸åŒè¯åºæ˜ å°„åˆ°ç›¸åŒcanonical form
```

#### 1.2 æ•°æ®å¯¼å…¥æµç¨‹å‡çº§

**æ–‡ä»¶**ï¼š`core/data_integration.py`ï¼ˆä¿®æ”¹ï¼‰

```python
# core/data_integration.pyï¼ˆæ–°å¢æ–¹æ³•ï¼‰

from core.canonical import CanonicalFormGenerator
from storage.repository import CanonicalFormRepository

class DataIntegrationService:
    def __init__(self):
        # ... åŸæœ‰åˆå§‹åŒ–
        self.canonical_generator = CanonicalFormGenerator(STOP_WORDS_EN)
        self.canonical_repo = CanonicalFormRepository()

    def merge_and_clean(self, round_id: int = 1):
        """
        æ•°æ®åˆå¹¶ä¸æ¸…æ´—ï¼ˆå‡çº§ç‰ˆï¼‰

        æ–°å¢ï¼šç”Ÿæˆè§„èŒƒåŒ–è¡¨è¾¾ï¼Œå»ºç«‹æ˜ å°„å…³ç³»
        """
        # 1. åŠ è½½åŸå§‹æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        df = self._load_raw_data()

        # 2. åŸºç¡€æ¸…æ´—ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        df = self._basic_cleaning(df)

        # 3. æ–°å¢ï¼šç”Ÿæˆè§„èŒƒåŒ–è¡¨è¾¾
        print("\nç”Ÿæˆè§„èŒƒåŒ–è¡¨è¾¾...")
        canonical_mapping = {}  # {canonical_text: [phrase_ids]}

        for idx, row in tqdm(df.iterrows(), total=len(df)):
            phrase = row['phrase']
            canonical = self.canonical_generator.generate(phrase)

            if canonical not in canonical_mapping:
                canonical_mapping[canonical] = []
            canonical_mapping[canonical].append(idx)

        print(f"âœ“ åŸå§‹çŸ­è¯­æ•°: {len(df)}")
        print(f"âœ“ è§„èŒƒåŒ–è¡¨è¾¾æ•°: {len(canonical_mapping)}")
        print(f"âœ“ å»é‡ç‡: {(1 - len(canonical_mapping)/len(df)) * 100:.1f}%")

        # 4. ä¿å­˜åˆ°æ•°æ®åº“
        with PhraseRepository() as phrase_repo, \
             CanonicalFormRepository() as canon_repo:

            # 4.1 ä¿å­˜è§„èŒƒåŒ–è¡¨è¾¾
            for canonical_text, phrase_indices in tqdm(
                canonical_mapping.items(),
                desc="ä¿å­˜è§„èŒƒåŒ–è¡¨è¾¾"
            ):
                # åˆ›å»ºcanonical_formè®°å½•
                canonical_id = canon_repo.create_or_get(
                    canonical_text=canonical_text,
                    keyword_count=len(phrase_indices)
                )

                # æ›´æ–°phrasesè¡¨çš„canonical_id
                for idx in phrase_indices:
                    phrase_text = df.loc[idx, 'phrase']
                    phrase_repo.update_canonical_id(
                        phrase=phrase_text,
                        canonical_id=canonical_id
                    )

        print("\nâœ… è§„èŒƒåŒ–æ˜ å°„å»ºç«‹å®Œæˆ")

        # 5. åŸæœ‰çš„ä¿å­˜é€»è¾‘...
```

#### 1.3 èšç±»æµç¨‹å‡çº§

**æ–‡ä»¶**ï¼š`scripts/run_phase2_clustering.py`ï¼ˆä¿®æ”¹ï¼‰

```python
# scripts/run_phase2_clustering.pyï¼ˆä¿®æ”¹æ ¸å¿ƒé€»è¾‘ï¼‰

def run_phase2_clustering(round_id: int = 1, limit: int = 0):
    """
    Phase 2ï¼šå¤§ç»„èšç±»ï¼ˆå‡çº§ç‰ˆï¼‰

    æ ¸å¿ƒå˜åŒ–ï¼š
    1. åŸºäºè§„èŒƒåŒ–è¡¨è¾¾è¿›è¡Œèšç±»ï¼ˆè€ŒéåŸå§‹çŸ­è¯­ï¼‰
    2. å‡å°‘é‡å¤è®¡ç®—
    3. èšç±»ç»“æœæ˜ å°„å›æ‰€æœ‰åŸå§‹çŸ­è¯­
    """
    print("\nã€Phase 2ã€‘å¤§ç»„èšç±»ï¼ˆåŸºäºè§„èŒƒåŒ–è¡¨è¾¾ï¼‰")

    # 1. åŠ è½½è§„èŒƒåŒ–è¡¨è¾¾ï¼ˆè€ŒéåŸå§‹çŸ­è¯­ï¼‰
    with CanonicalFormRepository() as repo:
        canonical_forms = repo.get_all(limit=limit)

    print(f"âœ“ è§„èŒƒåŒ–è¡¨è¾¾æ•°: {len(canonical_forms)}")

    # 2. æ„å»ºç”¨äºembeddingçš„æ•°æ®
    canonical_data = [
        {
            'phrase': cf.canonical_text,  # ä½¿ç”¨è§„èŒƒåŒ–æ–‡æœ¬
            'phrase_id': cf.canonical_id,
            'keyword_count': cf.keyword_count
        }
        for cf in canonical_forms
    ]

    # 3. è®¡ç®—embeddingï¼ˆåŸºäºè§„èŒƒåŒ–è¡¨è¾¾ï¼‰
    print("\nè®¡ç®—Embedding...")
    embedding_service = EmbeddingService(use_cache=True)
    embeddings, canonical_ids = embedding_service.embed_phrases_from_db(
        canonical_data,
        round_id=round_id
    )

    print(f"âœ“ EmbeddingçŸ©é˜µ: {embeddings.shape}")
    print(f"âœ“ è§„èŒƒåŒ–è¡¨è¾¾æ•°: {len(canonical_ids)}")

    # 4. æ‰§è¡Œèšç±»
    print("\næ‰§è¡ŒHDBSCANèšç±»...")
    cluster_ids, cluster_info, clusterer = cluster_phrases_large(
        embeddings,
        canonical_data
    )

    print(f"âœ“ èšç±»ç°‡æ•°: {len(cluster_info)}")

    # 5. ä¿å­˜èšç±»ç»“æœ
    #    - canonical_formsè¡¨ä¿å­˜cluster_id
    #    - phrasesè¡¨é€šè¿‡canonical_idç»§æ‰¿cluster_id
    with CanonicalFormRepository() as canon_repo, \
         PhraseRepository() as phrase_repo:

        # 5.1 æ›´æ–°canonical_formsçš„cluster_id
        for canonical_id, cluster_id in zip(canonical_ids, cluster_ids):
            canon_repo.update_cluster_id(canonical_id, cluster_id)

        # 5.2 æ›´æ–°phrasesçš„cluster_idï¼ˆé€šè¿‡canonical_idï¼‰
        phrase_repo.update_cluster_from_canonical()

    print("\nâœ… Phase 2å®Œæˆ")
    print(f"   - è§„èŒƒåŒ–è¡¨è¾¾: {len(canonical_forms)}")
    print(f"   - èšç±»ç°‡æ•°: {len(cluster_info)}")
    print(f"   - èŠ‚çœè®¡ç®—: {(1 - len(canonical_forms)/limit)*100:.1f}%")
```

**é¢„æœŸäº§å‡º**ï¼š
- âœ… è§„èŒƒåŒ–è¡¨è¾¾ç”Ÿæˆç®—æ³•ï¼ˆè¯çº§ï¼‰
- âœ… æ•°æ®å¯¼å…¥æµç¨‹æ”¯æŒè§„èŒƒåŒ–æ˜ å°„
- âœ… èšç±»åŸºäºè§„èŒƒåŒ–è¡¨è¾¾ï¼ˆå‡å°‘é‡å¤ï¼‰
- âœ… åŸå§‹å…³é”®è¯å®Œæ•´ä¿ç•™

---

### Phase 2ï¼šç‰¹å¾ç‰‡æ®µåˆ†æï¼ˆ3å‘¨ï¼‰

**ç›®æ ‡**ï¼šå®ç°å›è¨€æ ¸å¿ƒåˆ›æ–°1 - ç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼ˆè¯çº§N-gramï¼‰

#### 2.1 è¯çº§N-gramæå–

**æ–‡ä»¶**ï¼š`core/segment_analysis.py`ï¼ˆæ–°å»ºï¼‰

```python
# core/segment_analysis.py
from collections import Counter
from typing import List, Tuple
import re

class WordNGramExtractor:
    """
    è¯çº§N-gramæå–å™¨ï¼ˆè‹±æ–‡ç‰ˆï¼‰

    å…³é”®å·®å¼‚ï¼š
    - ä¸­æ–‡ï¼šå­—ç¬¦çº§N-gramï¼ˆ3-5å­—ï¼‰
    - è‹±æ–‡ï¼šè¯çº§N-gramï¼ˆ2-4è¯ï¼‰
    """

    def __init__(self, min_n=2, max_n=4):
        self.min_n = min_n
        self.max_n = max_n

    def extract_from_phrase(self, phrase: str) -> List[str]:
        """
        ä»å•ä¸ªçŸ­è¯­æå–è¯çº§N-gram

        ç¤ºä¾‹ï¼š
          "best image compressor online"
          â†’ 2-gram: ["best image", "image compressor", "compressor online"]
          â†’ 3-gram: ["best image compressor", "image compressor online"]
          â†’ 4-gram: ["best image compressor online"]
        """
        words = phrase.lower().split()
        ngrams = []

        for n in range(self.min_n, min(self.max_n + 1, len(words) + 1)):
            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i+n])
                ngrams.append(ngram)

        return ngrams

    def extract_global_statistics(
        self,
        phrases: List[str],
        top_k: int = 10000
    ) -> Counter:
        """
        å…¨å±€N-gramç»Ÿè®¡

        Args:
            phrases: è§„èŒƒåŒ–è¡¨è¾¾åˆ—è¡¨ï¼ˆæ¥è‡ªcanonical_formsè¡¨ï¼‰
            top_k: è¿”å›Top-Ké«˜é¢‘ç‰‡æ®µ

        Returns:
            Counterå¯¹è±¡ï¼Œæ ¼å¼ï¼š{ngram: frequency}
        """
        print(f"\nå…¨å±€è¯çº§N-gramç»Ÿè®¡ï¼ˆ{self.min_n}-{self.max_n}è¯ï¼‰...")

        ngram_counter = Counter()

        for phrase in tqdm(phrases, desc="æå–N-gram"):
            ngrams = self.extract_from_phrase(phrase)
            ngram_counter.update(ngrams)

        print(f"âœ“ ä¸åŒç‰‡æ®µæ•°: {len(ngram_counter):,}")
        print(f"âœ“ æ€»å‡ºç°æ¬¡æ•°: {sum(ngram_counter.values()):,}")

        # è¿”å›Top-K
        top_segments = ngram_counter.most_common(top_k)

        print(f"\nTop 10é«˜é¢‘ç‰‡æ®µï¼š")
        for i, (segment, freq) in enumerate(top_segments[:10], 1):
            print(f"  {i:2d}. '{segment:30s}' - {freq:,}æ¬¡")

        return ngram_counter


def find_mother_keywords(
    segment: str,
    phrases: List[str],
    count: int = 2
) -> List[str]:
    """
    æ‰¾åˆ°åŒ…å«è¯¥ç‰‡æ®µçš„æ¯è¯ï¼ˆæ ·æœ¬è¯ï¼‰

    ç­–ç•¥ï¼š
    1. æ‰¾åˆ°æ‰€æœ‰åŒ…å«è¯¥ç‰‡æ®µçš„çŸ­è¯­
    2. æŒ‰é•¿åº¦æ’åºï¼ˆä¼˜å…ˆé€‰æ‹©è¾ƒçŸ­çš„ï¼‰
    3. è¿”å›å‰countä¸ª
    """
    mothers = []

    for phrase in phrases:
        if segment in phrase:
            mothers.append(phrase)
            if len(mothers) >= count * 10:  # é¢„å…ˆæ”¶é›†æ›´å¤šå€™é€‰
                break

    # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆé€‰æ‹©è¾ƒçŸ­çš„ï¼ˆæ›´å…¸å‹ï¼‰
    mothers.sort(key=len)

    return mothers[:count]
```

#### 2.2 å®ç°Phase 4.5è„šæœ¬

**æ–‡ä»¶**ï¼š`scripts/run_phase4_5_segments.py`ï¼ˆæ–°å»ºï¼‰

```python
# scripts/run_phase4_5_segments.py
"""
Phase 4.5ï¼šç‰¹å¾ç‰‡æ®µåˆ†æ

ç›®æ ‡ï¼šä»æµ·é‡æ•°æ®ä¸­æå–å¯å®¡æ ¸æ ·æœ¬
æµç¨‹ï¼šè§„èŒƒåŒ–è¡¨è¾¾ â†’ è¯çº§N-gramç»Ÿè®¡ â†’ Topç‰‡æ®µ â†’ æ ·æœ¬è¯ â†’ å¿«é€Ÿèšç±»
"""

from core.segment_analysis import WordNGramExtractor, find_mother_keywords
from storage.repository import CanonicalFormRepository, NGramSegmentRepository
from core.embedding import EmbeddingService
from core.clustering import cluster_phrases_large
import argparse

def run_phase4_5_segments(
    sample_size: int = 5000,
    min_frequency: int = 5,
    samples_per_segment: int = 2
):
    """
    Phase 4.5ï¼šç‰¹å¾ç‰‡æ®µåˆ†æ

    Args:
        sample_size: æå–Topå¤šå°‘ä¸ªé«˜é¢‘ç‰‡æ®µ
        min_frequency: æœ€å°é¢‘æ¬¡é˜ˆå€¼
        samples_per_segment: æ¯ä¸ªç‰‡æ®µæå–å¤šå°‘ä¸ªæ¯è¯æ ·æœ¬
    """
    print("\n" + "="*70)
    print("ã€Phase 4.5ã€‘ç‰¹å¾ç‰‡æ®µåˆ†æ - è¯çº§N-gram")
    print("="*70)

    # 1. åŠ è½½è§„èŒƒåŒ–è¡¨è¾¾
    print("\n1. åŠ è½½è§„èŒƒåŒ–è¡¨è¾¾...")
    with CanonicalFormRepository() as repo:
        canonical_forms = repo.get_all()
        phrases = [cf.canonical_text for cf in canonical_forms]

    print(f"âœ“ è§„èŒƒåŒ–è¡¨è¾¾æ•°: {len(phrases):,}")

    # 2. å…¨å±€è¯çº§N-gramç»Ÿè®¡
    print("\n2. æ‰§è¡Œå…¨å±€è¯çº§N-gramç»Ÿè®¡...")
    extractor = WordNGramExtractor(min_n=2, max_n=4)
    ngram_counter = extractor.extract_global_statistics(
        phrases=phrases,
        top_k=sample_size
    )

    # 3. é€‰æ‹©Topç‰‡æ®µ
    print(f"\n3. é€‰æ‹©Top {sample_size}é«˜é¢‘ç‰‡æ®µ...")
    top_segments = [
        (segment, freq)
        for segment, freq in ngram_counter.most_common(sample_size)
        if freq >= min_frequency
    ]

    print(f"âœ“ ç­›é€‰åç‰‡æ®µæ•°: {len(top_segments)}")

    # 4. ä¿å­˜åˆ°æ•°æ®åº“
    print("\n4. ä¿å­˜ç‰‡æ®µåˆ°æ•°æ®åº“...")
    with NGramSegmentRepository() as repo:
        for segment, freq in tqdm(top_segments, desc="ä¿å­˜ç‰‡æ®µ"):
            repo.create_or_update(
                segment=segment,
                frequency=freq,
                gram_size=len(segment.split()),
                is_high_frequency=True
            )

    # 5. æå–æ ·æœ¬è¯
    print(f"\n5. æå–æ ·æœ¬è¯ï¼ˆæ¯ä¸ªç‰‡æ®µ{samples_per_segment}ä¸ªæ¯è¯ï¼‰...")
    sample_keywords = []

    for segment, freq in tqdm(top_segments, desc="æå–æ¯è¯"):
        mothers = find_mother_keywords(
            segment=segment,
            phrases=phrases,
            count=samples_per_segment
        )
        sample_keywords.extend(mothers)

    # å»é‡
    sample_keywords = list(set(sample_keywords))
    print(f"âœ“ æ ·æœ¬è¯æ•°: {len(sample_keywords):,}")

    # 6. å¯¹æ ·æœ¬è¯èšç±»ï¼ˆå¿«é€Ÿï¼ï¼‰
    print("\n6. å¯¹æ ·æœ¬è¯è¿›è¡Œèšç±»...")
    sample_data = [
        {'phrase': kw, 'phrase_id': i}
        for i, kw in enumerate(sample_keywords)
    ]

    # Embedding
    embedding_service = EmbeddingService(use_cache=True)
    embeddings, phrase_ids = embedding_service.embed_phrases_from_db(
        sample_data,
        round_id=999  # ç‰¹æ®Šè½®æ¬¡IDè¡¨ç¤ºæ ·æœ¬èšç±»
    )

    # èšç±»
    cluster_ids, cluster_info, clusterer = cluster_phrases_large(
        embeddings,
        sample_data
    )

    print(f"âœ“ æ ·æœ¬èšç±»ç°‡æ•°: {len(cluster_info)}")

    # 7. ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆå¯äººå·¥å®¡æ ¸ï¼‰
    print("\n7. ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š...")
    report_file = OUTPUT_DIR / 'phase4_5_segment_clusters.html'
    generate_segment_report(cluster_info, sample_keywords, report_file)

    # 8. ç»Ÿè®¡è¾“å‡º
    print("\n" + "="*70)
    print("âœ… Phase 4.5å®Œæˆï¼")
    print("="*70)
    print(f"   - è§„èŒƒåŒ–è¡¨è¾¾æ•°: {len(phrases):,}")
    print(f"   - é«˜é¢‘ç‰‡æ®µæ•°: {len(top_segments)}")
    print(f"   - æ ·æœ¬è¯æ•°: {len(sample_keywords):,}")
    print(f"   - æ ·æœ¬èšç±»ç°‡æ•°: {len(cluster_info)}")
    print(f"   - å®¡æ ¸æŠ¥å‘Š: {report_file}")
    print(f"\nğŸ“Œ ä¸‹ä¸€æ­¥: äººå·¥å®¡æ ¸æŠ¥å‘Šï¼Œæ ‡è®°éœ€æ±‚ç±»åˆ«")
    print(f"   é¢„è®¡å®¡æ ¸æ—¶é—´: < 2å°æ—¶")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--sample-size", type=int, default=5000)
    parser.add_argument("--min-frequency", type=int, default=5)
    parser.add_argument("--samples-per-segment", type=int, default=2)

    args = parser.parse_args()

    run_phase4_5_segments(
        sample_size=args.sample_size,
        min_frequency=args.min_frequency,
        samples_per_segment=args.samples_per_segment
    )
```

**é¢„æœŸäº§å‡º**ï¼š
- âœ… è¯çº§N-gramæå–å™¨
- âœ… å…¨å±€ç‰‡æ®µç»Ÿè®¡åŠŸèƒ½
- âœ… æ ·æœ¬è¯æå–é€»è¾‘
- âœ… å¿«é€Ÿèšç±» + HTMLå®¡æ ¸æŠ¥å‘Š
- âœ… ä»"ä¸å¯èƒ½å®¡æ ¸"åˆ°"2å°æ—¶å®Œæˆ"

---

### Phase 3ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆ3å‘¨ï¼‰

**ç›®æ ‡**ï¼šå®ç°å›è¨€æ ¸å¿ƒåˆ›æ–°2 - æ„å»º4å¤§ç±»ç‰¹å¾è¯åº“

#### 3.1 æ¨¡æ¿-å˜é‡è¿­ä»£ç®—æ³•

**æ–‡ä»¶**ï¼š`core/template_extraction.py`ï¼ˆæ–°å»ºï¼‰

```python
# core/template_extraction.py
"""
æ¨¡æ¿-å˜é‡è¿­ä»£æå–ç®—æ³•ï¼ˆè‹±æ–‡ç‰ˆï¼‰

æ ¸å¿ƒæ€æƒ³ï¼š
- å˜é‡ â†” æ¨¡æ¿ åŒå‘è¿­ä»£
- è´¨é‡è¿‡æ»¤ï¼šå˜é‡å¿…é¡»é€‚é…â‰¥3ä¸ªæ¨¡æ¿
- 3è½®è¿­ä»£æ”¶æ•›
"""

from collections import Counter, defaultdict
from typing import List, Tuple, Set
import re

class TemplateVariableExtractor:
    """
    æ¨¡æ¿-å˜é‡è¿­ä»£æå–å™¨
    """

    def __init__(
        self,
        min_template_freq: int = 5,
        min_variable_freq: int = 5,
        min_template_match: int = 3
    ):
        self.min_template_freq = min_template_freq
        self.min_variable_freq = min_variable_freq
        self.min_template_match = min_template_match

    def extract_templates(
        self,
        keywords: List[str],
        variables: Set[str]
    ) -> List[Tuple[str, int]]:
        """
        Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿

        é€»è¾‘ï¼š
        1. åœ¨å…³é”®è¯ä¸­æŸ¥æ‰¾åŒ…å«å˜é‡çš„è¯
        2. å°†å˜é‡æ›¿æ¢ä¸ºå ä½ç¬¦[X]
        3. ç»Ÿè®¡æ¨¡æ¿é¢‘æ¬¡

        ç¤ºä¾‹ï¼š
          å˜é‡: "compress"
          å…³é”®è¯: "best image compressor online"
          â†’ ä¸åŒ…å«"compress"å®Œæ•´è¯ï¼Œè·³è¿‡

          å…³é”®è¯: "compress image online"
          â†’ æ›¿æ¢: "compress [X] online"
          âŒ é”™è¯¯ï¼šå˜é‡ä½ç½®ä¸å¯¹

          æ­£ç¡®åšæ³•ï¼š
          å…³é”®è¯: "image compressor best"
          å˜é‡: "compressor"ï¼ˆè¯å½¢è¿˜åŸåçš„compressï¼‰
          â†’ æ¨¡æ¿: "image [X] best"
        """
        template_counter = Counter()

        for keyword in keywords:
            for var in variables:
                # å®Œæ•´è¯åŒ¹é…ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
                if re.search(r'\b' + re.escape(var) + r'\b', keyword):
                    template = re.sub(
                        r'\b' + re.escape(var) + r'\b',
                        '[X]',
                        keyword
                    )
                    template_counter[template] += 1

        # è¿‡æ»¤ä½é¢‘æ¨¡æ¿
        filtered = [
            (t, freq) for t, freq in template_counter.items()
            if freq >= self.min_template_freq
        ]

        return sorted(filtered, key=lambda x: x[1], reverse=True)

    def extract_variables(
        self,
        keywords: List[str],
        templates: List[str]
    ) -> Tuple[List[Tuple[str, int]], dict]:
        """
        Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡

        é€»è¾‘ï¼š
        1. å¯¹æ¯ä¸ªæ¨¡æ¿,æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
        2. æå–å ä½ç¬¦[X]ä½ç½®çš„è¯æ±‡
        3. ç»Ÿè®¡å˜é‡é¢‘æ¬¡å’Œé€‚é…æ¨¡æ¿æ•°

        ç¤ºä¾‹ï¼š
          æ¨¡æ¿: "best [X] for"
          å…³é”®è¯: "best image compressor for students"
          â†’ åŒ¹é…: "best [image compressor] for"
          â†’ å˜é‡: "image compressor"ï¼ˆå¯èƒ½æ˜¯2-gramï¼‰
        """
        variable_freq = Counter()
        variable_templates = defaultdict(set)

        for template in templates:
            # å°†[X]æ›¿æ¢ä¸ºæ­£åˆ™æ•è·ç»„
            # æ³¨æ„ï¼šæ•è·1ä¸ªæˆ–å¤šä¸ªè¯
            pattern = template.replace('[X]', r'(\S+(?:\s+\S+)*?)')
            pattern = '^' + pattern + '$'

            for keyword in keywords:
                match = re.match(pattern, keyword)
                if match:
                    var = match.group(1).strip()

                    # è¿‡æ»¤ï¼šå˜é‡é•¿åº¦åˆç†ï¼ˆ1-5ä¸ªè¯ï¼‰
                    if 1 <= len(var.split()) <= 5:
                        variable_freq[var] += 1
                        variable_templates[var].add(template)

        # è´¨é‡è¿‡æ»¤ï¼šé€‚é…æ¨¡æ¿æ•° >= min_template_match
        quality_variables = [
            (var, freq) for var, freq in variable_freq.items()
            if freq >= self.min_variable_freq
            and len(variable_templates[var]) >= self.min_template_match
        ]

        return (
            sorted(quality_variables, key=lambda x: x[1], reverse=True),
            variable_templates
        )

    def iterative_extraction(
        self,
        keywords: List[str],
        seed_variables: List[str],
        max_iterations: int = 3
    ) -> Tuple[List[str], List[str]]:
        """
        å¾ªç¯æå–æ¨¡æ¿å’Œå˜é‡

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆè§„èŒƒåŒ–è¡¨è¾¾ï¼‰
            seed_variables: ç§å­å˜é‡
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

        Returns:
            (templates, variables)
        """
        print("\n" + "="*70)
        print("æ¨¡æ¿-å˜é‡è¿­ä»£æå–")
        print("="*70)
        print(f"ç§å­å˜é‡æ•°: {len(seed_variables)}")
        print(f"å…³é”®è¯æ•°: {len(keywords):,}")

        all_templates = set()
        all_variables = set(seed_variables)

        for iteration in range(max_iterations):
            print(f"\n{'='*70}")
            print(f"=== Iteration {iteration + 1}/{max_iterations} ===")
            print(f"{'='*70}")

            # Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿
            print("\n  Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿...")
            template_list = self.extract_templates(keywords, all_variables)
            new_templates = [t for t, freq in template_list]

            before_templates = len(all_templates)
            all_templates.update(new_templates)
            after_templates = len(all_templates)

            print(f"    å‘ç°æ–°æ¨¡æ¿: {after_templates - before_templates}")
            print(f"    ç´¯è®¡æ¨¡æ¿æ•°: {after_templates}")

            # æ˜¾ç¤ºTop 10æ–°æ¨¡æ¿
            print(f"\n    Top 10æ–°æ¨¡æ¿:")
            for i, (template, freq) in enumerate(template_list[:10], 1):
                print(f"      {i:2d}. '{template:40s}' - {freq}æ¬¡")

            # Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡
            print("\n  Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡...")
            variable_list, variable_templates = self.extract_variables(
                keywords,
                new_templates
            )
            new_variables = [v for v, freq in variable_list]

            before_variables = len(all_variables)
            all_variables.update(new_variables)
            after_variables = len(all_variables)

            print(f"    å‘ç°æ–°å˜é‡: {after_variables - before_variables}")
            print(f"    ç´¯è®¡å˜é‡æ•°: {after_variables}")

            # æ˜¾ç¤ºTop 10æ–°å˜é‡
            print(f"\n    Top 10æ–°å˜é‡:")
            for i, (var, freq) in enumerate(variable_list[:10], 1):
                template_count = len(variable_templates[var])
                print(f"      {i:2d}. '{var:30s}' - {freq}æ¬¡, é€‚é…{template_count}ä¸ªæ¨¡æ¿")

            # æ”¶æ•›åˆ¤æ–­
            if after_variables == before_variables:
                print("\n  âœ“ å·²æ”¶æ•›ï¼ˆæ— æ–°å˜é‡ï¼‰")
                break

        print(f"\n{'='*70}")
        print("âœ… è¿­ä»£å®Œæˆï¼")
        print(f"{'='*70}")
        print(f"   - æ€»æ¨¡æ¿æ•°: {len(all_templates)}")
        print(f"   - æ€»å˜é‡æ•°: {len(all_variables)}")

        return list(all_templates), list(all_variables)
```

#### 3.2 å®ç°Phase 5.5è„šæœ¬

**æ–‡ä»¶**ï¼š`scripts/run_phase5_5_templates.py`ï¼ˆæ–°å»ºï¼‰

```python
# scripts/run_phase5_5_templates.py
"""
Phase 5.5ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£æå–

ç›®æ ‡ï¼šæ„å»º4å¤§ç±»ç‰¹å¾è¯åº“ï¼ˆChannel/Object/Function/Audienceï¼‰
"""

from core.template_extraction import TemplateVariableExtractor
from storage.repository import (
    CanonicalFormRepository,
    FeatureVariableRepository,
    SearchTemplateRepository
)
from ai.client import get_llm_client
import argparse

def classify_variable_category(variable: str, llm_client) -> str:
    """
    ä½¿ç”¨LLMå¯¹å˜é‡è¿›è¡Œåˆ†ç±»

    åˆ†ç±»ï¼š
    - channel: å¹³å°/æ¸ é“ï¼ˆgoogle, youtube, shopify...ï¼‰
    - object: å¯¹è±¡ï¼ˆimage, pdf, video...ï¼‰
    - function: åŠŸèƒ½/åŠ¨ä½œï¼ˆcompress, convert, analyze...ï¼‰
    - audience: ç¾¤ä½“ï¼ˆstudents, developers, marketers...ï¼‰
    """
    prompt = f"""
Classify the following keyword into one of these categories:
- channel: platforms, websites, apps, or distribution channels (e.g., google, youtube, shopify, twitter)
- object: things being processed or worked with (e.g., image, pdf, video, text, email)
- function: actions or capabilities (e.g., compress, convert, analyze, track, generate)
- audience: target user groups (e.g., students, developers, marketers, designers, small business)

Keyword: "{variable}"

Return only the category name (channel/object/function/audience).
"""

    response = llm_client.chat([{"role": "user", "content": prompt}])
    category = response.strip().lower()

    if category not in ['channel', 'object', 'function', 'audience']:
        category = 'object'  # é»˜è®¤

    return category


def run_phase5_5_templates():
    """
    Phase 5.5ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£æå–
    """
    print("\n" + "="*70)
    print("ã€Phase 5.5ã€‘æ¨¡æ¿-å˜é‡è¿­ä»£æå–")
    print("="*70)

    # 1. åŠ è½½è§„èŒƒåŒ–è¡¨è¾¾
    print("\n1. åŠ è½½è§„èŒƒåŒ–è¡¨è¾¾...")
    with CanonicalFormRepository() as repo:
        canonical_forms = repo.get_all()
        keywords = [cf.canonical_text for cf in canonical_forms]

    print(f"âœ“ è§„èŒƒåŒ–è¡¨è¾¾æ•°: {len(keywords):,}")

    # 2. å‡†å¤‡ç§å­å˜é‡
    print("\n2. å‡†å¤‡ç§å­å˜é‡...")

    # ä»ç°æœ‰tokensè¡¨åŠ è½½ï¼ˆå¦‚æœæœ‰ï¼‰
    with FeatureVariableRepository() as repo:
        existing_vars = repo.get_all_verified()

    if existing_vars:
        seed_variables = [v.variable_text for v in existing_vars]
        print(f"âœ“ ä»æ•°æ®åº“åŠ è½½ç§å­å˜é‡: {len(seed_variables)}")
    else:
        # æ‰‹å·¥å®šä¹‰ä¸€äº›ç§å­å˜é‡
        seed_variables = [
            # Channel
            'google', 'youtube', 'facebook', 'twitter', 'instagram',
            'shopify', 'wordpress', 'gmail', 'outlook',

            # Object
            'image', 'video', 'photo', 'pdf', 'text', 'file',
            'domain', 'email', 'password', 'url',

            # Function
            'compress', 'resize', 'convert', 'download', 'upload',
            'analyze', 'track', 'monitor', 'generate', 'extract',

            # Audience
            'students', 'developers', 'marketers', 'designers',
            'business', 'beginners'
        ]
        print(f"âœ“ ä½¿ç”¨é¢„å®šä¹‰ç§å­å˜é‡: {len(seed_variables)}")

    # 3. æ‰§è¡Œè¿­ä»£æå–
    print("\n3. æ‰§è¡Œæ¨¡æ¿-å˜é‡è¿­ä»£...")
    extractor = TemplateVariableExtractor(
        min_template_freq=5,
        min_variable_freq=5,
        min_template_match=3
    )

    templates, variables = extractor.iterative_extraction(
        keywords=keywords,
        seed_variables=seed_variables,
        max_iterations=3
    )

    # 4. ä¿å­˜æ¨¡æ¿åˆ°æ•°æ®åº“
    print("\n4. ä¿å­˜æ¨¡æ¿åˆ°æ•°æ®åº“...")
    with SearchTemplateRepository() as repo:
        for template in tqdm(templates, desc="ä¿å­˜æ¨¡æ¿"):
            # è®¡ç®—æ¨¡æ¿é¢‘æ¬¡
            freq = sum(1 for kw in keywords if template.replace('[X]', '.+?') in kw)
            variable_count = template.count('[X]')

            repo.create_or_update(
                template=template,
                frequency=freq,
                variable_count=variable_count
            )

    # 5. å¯¹å˜é‡è¿›è¡Œåˆ†ç±»å¹¶ä¿å­˜
    print("\n5. å¯¹å˜é‡è¿›è¡Œåˆ†ç±»...")
    llm_client = get_llm_client()

    with FeatureVariableRepository() as repo:
        for variable in tqdm(variables, desc="åˆ†ç±»å¹¶ä¿å­˜å˜é‡"):
            # LLMåˆ†ç±»
            category = classify_variable_category(variable, llm_client)

            # è®¡ç®—ç»Ÿè®¡
            in_phrase_count = sum(1 for kw in keywords if variable in kw)
            template_count = sum(1 for t in templates if variable in keywords_matching_template(t, keywords))

            # ä¿å­˜
            repo.create_or_update(
                variable_text=variable,
                category=category,
                in_phrase_count=in_phrase_count,
                template_count=template_count,
                gram_size=len(variable.split()),
                verified=False,
                confidence='medium'
            )

    # 6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\n6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
    generate_template_variable_report(templates, variables)

    # 7. ç»Ÿè®¡è¾“å‡º
    print("\n" + "="*70)
    print("âœ… Phase 5.5å®Œæˆï¼")
    print("="*70)
    print(f"   - æ¨¡æ¿æ•°: {len(templates)}")
    print(f"   - å˜é‡æ•°: {len(variables)}")
    print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")

    with FeatureVariableRepository() as repo:
        for category in ['channel', 'object', 'function', 'audience']:
            count = repo.count_by_category(category)
            print(f"   - {category.capitalize():10s}: {count}")


if __name__ == "__main__":
    run_phase5_5_templates()
```

**é¢„æœŸäº§å‡º**ï¼š
- âœ… æ¨¡æ¿-å˜é‡è¿­ä»£ç®—æ³•
- âœ… 4å¤§ç±»è¯åº“æ„å»º
- âœ… LLMè‡ªåŠ¨åˆ†ç±»
- âœ… è´¨é‡è¿‡æ»¤ï¼ˆå˜é‡é€‚é…â‰¥3æ¨¡æ¿ï¼‰
- âœ… è¯åº“è§„æ¨¡ï¼šä»26ä¸ª â†’ é¢„è®¡æ•°åƒä¸ª

---

### Phase 4ï¼šéœ€æ±‚åˆ†ç±»ä½“ç³»ï¼ˆ2å‘¨ï¼‰

**ç›®æ ‡**ï¼šå»ºç«‹æœç´¢æ„å›¾åˆ†ç±»æ¡†æ¶

#### 4.1 æœç´¢æ„å›¾åˆ†ç±»å™¨

**æ–‡ä»¶**ï¼š`core/intent_classifier.py`ï¼ˆæ–°å»ºï¼‰

```python
# core/intent_classifier.py
"""
æœç´¢æ„å›¾åˆ†ç±»å™¨

åˆ†ç±»æ¡†æ¶ï¼ˆç»´åº¦Aï¼‰ï¼š
- Search/Findï¼ˆå¯»æ‰¾ï¼‰
  - Downloadï¼ˆä¸‹è½½ï¼‰
  - Recommendï¼ˆæ¨è/å¯¹æ¯”ï¼‰
  - Freeï¼ˆå…è´¹èµ„æºï¼‰
  - Alternativeï¼ˆæ›¿ä»£å“ï¼‰
- Operate/Useï¼ˆæ“ä½œ/ä½¿ç”¨ï¼‰
- Fix/Troubleshootï¼ˆé—®é¢˜/æ•…éšœï¼‰
- Price/Costï¼ˆè¯¢ä»·ï¼‰
- Learn/Tutorialï¼ˆå­¦ä¹ /æ•™ç¨‹ï¼‰
- Otherï¼ˆå…¶ä»–ï¼‰
"""

from typing import Dict
import re

class IntentClassifier:
    """
    åŸºäºè§„åˆ™ + LLMçš„æ„å›¾åˆ†ç±»å™¨
    """

    # æ„å›¾å…³é”®è¯åº“
    INTENT_KEYWORDS = {
        'search': {
            'download': ['download', 'get', 'install', 'setup', 'apk', 'installer'],
            'recommend': ['best', 'top', 'recommend', 'comparison', 'vs', 'versus', 'better', 'alternative'],
            'free': ['free', 'open source', 'no cost', 'without payment', 'no credit card'],
            'alternative': ['alternative', 'instead of', 'replace', 'similar to']
        },
        'operate': {
            'how_to': ['how to', 'how do i', 'how can i', 'guide', 'tutorial', 'steps'],
            'use': ['use', 'using', 'usage', 'operate', 'work with']
        },
        'fix': {
            'error': ['error', 'fix', 'not working', 'problem', 'issue', 'troubleshoot', 'broken'],
            'cant': ["can't", 'cannot', 'unable to', 'won\'t', 'failed to']
        },
        'price': {
            'cost': ['price', 'cost', 'how much', 'pricing', 'subscription', 'plan', 'fee']
        },
        'learn': {
            'tutorial': ['tutorial', 'learn', 'course', 'training', 'beginner guide'],
            'what_is': ['what is', 'what are', 'explain', 'definition']
        }
    }

    def classify(self, phrase: str, tokens: list = None) -> Dict:
        """
        å¯¹çŸ­è¯­è¿›è¡Œæ„å›¾åˆ†ç±»

        Args:
            phrase: çŸ­è¯­æ–‡æœ¬
            tokens: çŸ­è¯­åŒ…å«çš„ç‰¹å¾å˜é‡ï¼ˆå¯é€‰ï¼‰

        Returns:
            {
                'main_type': 'search',
                'sub_type': 'download',
                'confidence': 'high',
                'matched_keywords': ['download']
            }
        """
        phrase_lower = phrase.lower()

        # éå†åˆ†ç±»æ¡†æ¶
        for main_type, sub_types in self.INTENT_KEYWORDS.items():
            for sub_type, keywords in sub_types.items():
                for keyword in keywords:
                    if keyword in phrase_lower:
                        return {
                            'main_type': main_type,
                            'sub_type': sub_type,
                            'confidence': 'high',
                            'matched_keywords': [keyword]
                        }

        # å¦‚æœæœ‰tokensï¼Œä½¿ç”¨tokenç±»å‹è¾…åŠ©åˆ¤æ–­
        if tokens:
            token_types = [t.get('token_type') for t in tokens]
            if 'intent' in token_types:
                return {
                    'main_type': 'search',
                    'sub_type': 'recommend',
                    'confidence': 'medium',
                    'matched_keywords': []
                }

        # é»˜è®¤åˆ†ç±»
        return {
            'main_type': 'other',
            'sub_type': None,
            'confidence': 'low',
            'matched_keywords': []
        }
```

#### 4.2 æ‰¹é‡åˆ†ç±»ä¸ç»Ÿè®¡

**æ–‡ä»¶**ï¼š`scripts/analyze_demand_distribution.py`ï¼ˆæ–°å»ºï¼‰

```python
# scripts/analyze_demand_distribution.py
"""
éœ€æ±‚åˆ†ç±»åˆ†å¸ƒåˆ†æ

ç›®æ ‡ï¼š
1. å¯¹æ‰€æœ‰éœ€æ±‚è¿›è¡Œæ„å›¾åˆ†ç±»
2. ç»Ÿè®¡å„ç±»åˆ«å æ¯”
3. ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

from core.intent_classifier import IntentClassifier
from storage.repository import DemandRepository, PhraseRepository
from collections import Counter

def analyze_demand_distribution():
    """
    åˆ†æéœ€æ±‚åˆ†ç±»åˆ†å¸ƒ
    """
    print("\nã€éœ€æ±‚åˆ†ç±»åˆ†å¸ƒåˆ†æã€‘")

    # 1. åŠ è½½æ‰€æœ‰éœ€æ±‚
    with DemandRepository() as repo:
        demands = repo.get_all()

    print(f"âœ“ éœ€æ±‚æ€»æ•°: {len(demands)}")

    # 2. æ‰¹é‡åˆ†ç±»
    print("\nå¯¹éœ€æ±‚è¿›è¡Œæ„å›¾åˆ†ç±»...")
    classifier = IntentClassifier()
    category_counter = Counter()

    for demand in tqdm(demands, desc="åˆ†ç±»éœ€æ±‚"):
        # è·å–éœ€æ±‚å…³è”çš„çŸ­è¯­ï¼ˆé‡‡æ ·ï¼‰
        with PhraseRepository() as phrase_repo:
            phrases = phrase_repo.get_by_demand_id(demand.demand_id, limit=10)

        # å¯¹ç¬¬ä¸€ä¸ªçŸ­è¯­åˆ†ç±»ï¼ˆä»£è¡¨éœ€æ±‚ï¼‰
        if phrases:
            result = classifier.classify(phrases[0].phrase)
            category_key = f"{result['main_type']}-{result['sub_type']}"
            category_counter[category_key] += 1

            # æ›´æ–°éœ€æ±‚è¡¨
            with DemandRepository() as repo:
                repo.update_intent(
                    demand_id=demand.demand_id,
                    intent_type=result['main_type'],
                    intent_subtype=result['sub_type']
                )

    # 3. ç»Ÿè®¡åˆ†æ
    print("\nã€éœ€æ±‚åˆ†ç±»ç»Ÿè®¡ã€‘")
    total = sum(category_counter.values())

    print(f"\næ€»éœ€æ±‚æ•°: {total}")
    print(f"\nå„ç±»åˆ«åˆ†å¸ƒ:")

    for category, count in category_counter.most_common():
        percentage = count / total * 100
        print(f"  {category:30s}: {count:6d} ({percentage:5.1f}%)")

    # 4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    generate_category_chart(category_counter)

    print("\nâœ… åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    analyze_demand_distribution()
```

**é¢„æœŸäº§å‡º**ï¼š
- âœ… æœç´¢æ„å›¾åˆ†ç±»å™¨
- âœ… æ‰¹é‡åˆ†ç±»åŠŸèƒ½
- âœ… ç»Ÿè®¡æŠ¥å‘Šï¼ˆå æ¯”åˆ†æï¼‰
- âœ… å‘ç°ï¼šå“ªç±»æœç´¢å ä¸»å¯¼ï¼ˆé¢„è®¡Search/Findå 70-80%ï¼‰

---

### Phase 5ï¼šWeb UIé›†æˆï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**ï¼šå°†æ–°åŠŸèƒ½é›†æˆåˆ°Streamlit UI

#### 5.1 æ–°å¢é¡µé¢

```python
# ui/pages/phase4_5_segments.pyï¼ˆæ–°å¢ï¼‰
import streamlit as st
import subprocess
from pathlib import Path

st.title("ğŸ“Š Phase 4.5: ç‰¹å¾ç‰‡æ®µåˆ†æ")

st.markdown("""
### ğŸ¯ ç›®æ ‡
é€šè¿‡è¯çº§N-gramç‰‡æ®µç»Ÿè®¡ï¼Œä»å¤§é‡èšç±»ç»“æœä¸­æå–å¯å®¡æ ¸çš„æ ·æœ¬è¯ã€‚

### ğŸ“‹ æµç¨‹
1. å…¨å±€è¯çº§N-gramç»Ÿè®¡ï¼ˆ2-4è¯ï¼‰
2. é€‰æ‹©Topé«˜é¢‘ç‰‡æ®µ
3. æå–æ ·æœ¬è¯ï¼ˆæ¯ä¸ªç‰‡æ®µ2ä¸ªæ¯è¯ï¼‰
4. èšç±»æ ·æœ¬è¯
5. ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š

### â­ æ ¸å¿ƒä»·å€¼
- å°†æµ·é‡æ•°æ® â†’ å¯å®¡æ ¸æ ·æœ¬
- äººå·¥å®¡æ ¸æ—¶é—´ï¼š< 2å°æ—¶
- è¦†ç›–ç‡ï¼š> 95%
""")

sample_size = st.number_input("é«˜é¢‘ç‰‡æ®µæ•°", 1000, 50000, 5000, 1000)
min_frequency = st.number_input("æœ€å°é¢‘æ¬¡", 3, 100, 5, 1)

if st.button("å¼€å§‹åˆ†æ", type="primary"):
    cmd = [
        "python", "scripts/run_phase4_5_segments.py",
        f"--sample-size={sample_size}",
        f"--min-frequency={min_frequency}"
    ]

    with st.spinner("æ­£åœ¨æ‰§è¡Œç‰¹å¾ç‰‡æ®µåˆ†æ..."):
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode == 0:
            st.success("âœ… Phase 4.5å®Œæˆï¼")
            st.code(result.stdout)

            # æ˜¾ç¤ºå®¡æ ¸æŠ¥å‘Šé“¾æ¥
            report_file = Path("data/output/phase4_5_segment_clusters.html")
            if report_file.exists():
                st.markdown(f"[ğŸ“„ æŸ¥çœ‹å®¡æ ¸æŠ¥å‘Š]({report_file})")
        else:
            st.error("âŒ æ‰§è¡Œå¤±è´¥")
            st.code(result.stderr)


# ui/pages/phase5_5_templates.pyï¼ˆæ–°å¢ï¼‰
import streamlit as st
import subprocess

st.title("ğŸ”„ Phase 5.5: æ¨¡æ¿-å˜é‡è¿­ä»£")

st.markdown("""
### ğŸ¯ ç›®æ ‡
è‡ªåŠ¨æ„å»º4å¤§ç±»ç‰¹å¾è¯åº“ï¼ˆChannel/Object/Function/Audienceï¼‰

### ğŸ“‹ æµç¨‹
1. å‡†å¤‡ç§å­å˜é‡
2. ç”¨å˜é‡æå–æ¨¡æ¿
3. ç”¨æ¨¡æ¿æå–æ›´å¤šå˜é‡
4. è´¨é‡è¿‡æ»¤ï¼ˆå˜é‡éœ€é€‚é…â‰¥3ä¸ªæ¨¡æ¿ï¼‰
5. è¿­ä»£3è½®æ”¶æ•›

### â­ é¢„æœŸæˆæœ
- æ¸ é“è¯åº“ï¼šæ•°ç™¾åˆ°ä¸Šåƒ
- å¯¹è±¡è¯åº“ï¼šæ•°ç™¾åˆ°ä¸Šåƒ
- åŠŸèƒ½è¯åº“ï¼šæ•°ç™¾åˆ°ä¸Šåƒ
- ç¾¤ä½“è¯åº“ï¼šæ•°ç™¾
""")

if st.button("å¼€å§‹è¿­ä»£æå–", type="primary"):
    cmd = ["python", "scripts/run_phase5_5_templates.py"]

    with st.spinner("æ­£åœ¨æ‰§è¡Œæ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆå¯èƒ½éœ€è¦æ•°åˆ†é’Ÿï¼‰..."):
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode == 0:
            st.success("âœ… Phase 5.5å®Œæˆï¼")
            st.code(result.stdout)
        else:
            st.error("âŒ æ‰§è¡Œå¤±è´¥")
            st.code(result.stderr)
```

**é¢„æœŸäº§å‡º**ï¼š
- âœ… Phase 4.5 UIé¡µé¢
- âœ… Phase 5.5 UIé¡µé¢
- âœ… éœ€æ±‚åˆ†ç±»ç»Ÿè®¡é¡µé¢
- âœ… å®Œæ•´é›†æˆåˆ°ç°æœ‰UI

---

## ä¸å›è¨€ç³»ç»Ÿå¯¹ç…§

### æ ¸å¿ƒåˆ›æ–°å¸æ”¶å¯¹ç…§è¡¨

| å›è¨€åˆ›æ–° | å›è¨€å®ç°ï¼ˆä¸­æ–‡ï¼‰ | æˆ‘ä»¬çš„å®ç°ï¼ˆè‹±æ–‡ï¼‰ | å¸æ”¶ç¨‹åº¦ |
|---------|----------------|------------------|---------|
| **é«˜é¢‘è¯ç»„åˆ** | ä¸»è¯+é«˜é¢‘è¯ç»„åˆä¸‹è½½ | æš‚ä¸éœ€è¦ï¼ˆæ•°æ®æºä¸åŒï¼‰ | âšª ä¸é€‚ç”¨ |
| **æ–‡å­—æ’åºå»é‡** | å­—çº§æ‹¼éŸ³æ’åº | è¯çº§æ’åº+è¯å½¢è¿˜åŸ | âœ… 100%å¸æ”¶ï¼ˆæ”¹é€ ï¼‰ |
| **åˆ†æ‰¹èšç±»** | 200ä¸‡/æ‰¹å¤„ç† | å½“å‰è§„æ¨¡æ— éœ€ï¼ˆæœªæ¥å¯æ‰©å±•ï¼‰ | ğŸŸ¡ 50%å¸æ”¶ï¼ˆä¿ç•™æ‰©å±•æ€§ï¼‰ |
| **ç‰¹å¾ç‰‡æ®µæ˜ å°„** | 3-5å­—ç‰‡æ®µ â†’ 2ä¸‡æ ·æœ¬ | 2-4è¯ç‰‡æ®µ â†’ 1ä¸‡æ ·æœ¬ | âœ… 100%å¸æ”¶ï¼ˆæ”¹é€ ï¼‰ |
| **æ¨¡æ¿-å˜é‡è¿­ä»£** | 3è½®è¿­ä»£ï¼Œâ‰¥3æ¨¡æ¿è¿‡æ»¤ | 3è½®è¿­ä»£ï¼Œâ‰¥3æ¨¡æ¿è¿‡æ»¤ | âœ… 100%å¸æ”¶ |
| **éœ€æ±‚åˆ†ç±»** | 6å¤§ç±»ï¼ˆ95%å¯»æ‰¾ç±»ï¼‰ | 5å¤§ç±»+äº§å“ç±»å‹åŒç»´åº¦ | âœ… 100%å¸æ”¶ï¼ˆå¢å¼ºï¼‰ |
| **4å¤§ç‰¹å¾ç»´åº¦** | æ¸ é“/å¯¹è±¡/åŠŸèƒ½/ç¾¤ä½“ | å®Œå…¨ä¸€è‡´ | âœ… 100%å¸æ”¶ |

### æ¶æ„å¯¹ç…§

| ç»´åº¦ | å›è¨€ç³»ç»Ÿ | æˆ‘ä»¬çš„ç³»ç»Ÿ | å¯¹æ¯” |
|------|---------|-----------|------|
| **æ•°æ®åˆ†å±‚** | éšå«ï¼ˆåŸå§‹â†’æ¸…æ´—â†’èšç±»â†’æ ·æœ¬ï¼‰ | æ˜ç¡®ä¸‰å±‚ï¼ˆåŸå§‹â†’è§„èŒƒåŒ–â†’éœ€æ±‚ï¼‰ | âœ… æ›´æ¸…æ™° |
| **å…³é”®è¯ä¿ç•™** | åˆ é™¤ï¼ˆä»…ä¿ç•™ä»£è¡¨è¯ï¼‰ | å®Œå…¨ä¿ç•™ï¼ˆSEOéœ€è¦ï¼‰ | âœ… æ›´é€‚åˆåœºæ™¯ |
| **å¤„ç†å•å…ƒ** | å­—çº§ï¼ˆä¸­æ–‡ç‰¹æ€§ï¼‰ | è¯çº§ï¼ˆè‹±æ–‡ç‰¹æ€§ï¼‰ | âœ… æ­£ç¡®é€‚é… |
| **è‡ªåŠ¨åŒ–ç¨‹åº¦** | äººå·¥å®¡æ ¸ä¸ºä¸» | LLM+äººå·¥å®¡æ ¸ç»“åˆ | âœ… æ›´é«˜ |
| **æŠ€æœ¯æ ˆ** | è‡ªç ”èšç±»ç®—æ³• | HDBSCANï¼ˆæˆç†Ÿï¼‰ | âœ… æ›´ç¨³å®š |

### ä¼˜åŠ¿äº’è¡¥åˆ†æ

**æˆ‘ä»¬çš„ç‹¬ç‰¹ä¼˜åŠ¿**ï¼š
1. âœ… **ä¸‰å±‚æ•°æ®æ¨¡å‹** - å›è¨€æœªæ˜ç¡®åˆ†å±‚
2. âœ… **å®Œæ•´ä¿ç•™å…³é”®è¯** - æ”¯æŒSEOæ‰§è¡Œ
3. âœ… **LLMè‡ªåŠ¨åŒ–** - å‡å°‘äººå·¥å·¥ä½œé‡
4. âœ… **Web UI** - ç”¨æˆ·å‹å¥½
5. âœ… **è‹±æ–‡ç‰¹æ€§é€‚é…** - è¯çº§å¤„ç†

**å›è¨€çš„ç‹¬ç‰¹ä¼˜åŠ¿**ï¼š
1. â­ **ç‰¹å¾ç‰‡æ®µæ˜ å°„** - æ ¸å¿ƒåˆ›æ–°ï¼ˆâœ…å·²å¸æ”¶ï¼‰
2. â­ **æ¨¡æ¿-å˜é‡è¿­ä»£** - æ ¸å¿ƒåˆ›æ–°ï¼ˆâœ…å·²å¸æ”¶ï¼‰
3. â­ **è¶…å¤§è§„æ¨¡å¤„ç†** - 1.6äº¿æ•°æ®ï¼ˆğŸŸ¡æš‚ä¸éœ€è¦ï¼‰
4. â­ **å®Œæ•´è¯åº“** - 8000+æ¸ é“è¯ï¼ˆâœ…ç›®æ ‡å¯¹é½ï¼‰

**ç»“è®º**ï¼š
- âœ… æˆåŠŸå¸æ”¶å›è¨€æ ¸å¿ƒåˆ›æ–°ï¼ˆç‰¹å¾ç‰‡æ®µã€æ¨¡æ¿è¿­ä»£ï¼‰
- âœ… ä¿ç•™è‡ªèº«ä¼˜åŠ¿ï¼ˆä¸‰å±‚æ¨¡å‹ã€LLMè‡ªåŠ¨åŒ–ã€Web UIï¼‰
- âœ… é€‚é…è‹±æ–‡ç‰¹æ€§ï¼ˆè¯çº§å¤„ç†ï¼‰
- âœ… æ»¡è¶³SEOåœºæ™¯éœ€æ±‚ï¼ˆä¿ç•™å…³é”®è¯ï¼‰

---

## é¢„æœŸæˆæœ

### ç³»ç»Ÿèƒ½åŠ›æå‡

**ä¼˜åŒ–å‰ï¼ˆMVPç‰ˆæœ¬ï¼‰**ï¼š
```
æ•°æ®å¤„ç†ï¼š
  - åŸºç¡€å»é‡
  - å•å±‚æ•°æ®æ¨¡å‹
  - èšç±»60-100ç°‡
  - äººå·¥ç­›é€‰10-15ç»„

ç‰¹å¾æå–ï¼š
  - 26ä¸ªtokens
  - æ— è¿­ä»£æœºåˆ¶
  - æ— åˆ†ç±»ä½“ç³»

å¯å®¡æ ¸æ€§ï¼š
  - éœ€å®¡æ ¸60-100ç°‡
  - å¦‚æœæ•°æ®é‡å¤§ï¼ˆ>50ä¸‡ï¼‰â†’ ä¸å¯å®¡æ ¸

è¯åº“è§„æ¨¡ï¼š
  - 26ä¸ªtokens
```

**ä¼˜åŒ–åï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰**ï¼š
```
æ•°æ®å¤„ç†ï¼š
  - ä¸‰å±‚æ•°æ®æ¨¡å‹
  - è§„èŒƒåŒ–è¡¨è¾¾å»é‡ï¼ˆè¯çº§ï¼‰
  - åŸºäºè§„èŒƒåŒ–èšç±»ï¼ˆèŠ‚çœè®¡ç®—ï¼‰
  - åŸå§‹å…³é”®è¯å®Œæ•´ä¿ç•™

ç‰¹å¾æå–ï¼š
  - ç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼š5åƒç‰‡æ®µ â†’ 1ä¸‡æ ·æœ¬è¯
  - æ¨¡æ¿-å˜é‡è¿­ä»£ï¼š3è½®è‡ªåŠ¨æ‰©å±•
  - 4å¤§ç±»è¯åº“ï¼šé¢„è®¡æ•°åƒä¸ªå˜é‡
  - æœç´¢æ„å›¾åˆ†ç±»ï¼š5å¤§ç±»+å­ç±»å‹

å¯å®¡æ ¸æ€§ï¼š
  - æ ·æœ¬è¯å®¡æ ¸ï¼š1ä¸‡è¯ï¼ˆ<2å°æ—¶ï¼‰
  - æ”¯æŒ50ä¸‡+æ•°æ®è§„æ¨¡
  - è¦†ç›–ç‡ > 95%

è¯åº“è§„æ¨¡ï¼š
  - Channel: æ•°ç™¾åˆ°ä¸Šåƒ
  - Object: æ•°ç™¾åˆ°ä¸Šåƒ
  - Function: æ•°ç™¾åˆ°ä¸Šåƒ
  - Audience: æ•°ç™¾
```

### è¾“å‡ºè§†å›¾ï¼ˆä¸‰å±‚ï¼‰

**1. éœ€æ±‚åœ°å›¾è§†å›¾ï¼ˆå†³ç­–å±‚ï¼‰**
```
åŠŸèƒ½ï¼š
  - éœ€æ±‚åˆ—è¡¨ï¼ˆæŒ‰æœç´¢æ„å›¾åˆ†ç±»ï¼‰
  - æ¯ä¸ªéœ€æ±‚å¡ç‰‡ï¼š
    - æ ‡é¢˜ã€æè¿°
    - æ„å›¾åˆ†å¸ƒï¼ˆ70% Search, 20% Operate...ï¼‰
    - è¦†ç›–å…³é”®è¯æ•°
    - æ€»æœç´¢é‡ã€æœºä¼šè¯„åˆ†

åº”ç”¨ï¼š
  - é€‰æ‹©åšå“ªä¸ªäº§å“/æ–¹å‘
  - ä¼˜å…ˆçº§æ’åº
```

**2. å…³é”®è¯&è¯åº“è§†å›¾ï¼ˆç­–ç•¥å±‚ï¼‰**
```
åŠŸèƒ½ï¼š
  - å®Œæ•´å…³é”®è¯æ¸…å•ï¼ˆæŒ‰volume/KDæ’åºï¼‰
  - ç­›é€‰åŠŸèƒ½ï¼š
    - æŒ‰æ„å›¾ç±»å‹
    - æŒ‰volumeé˜ˆå€¼
    - æŒ‰éš¾åº¦èŒƒå›´
  - ç‰¹å¾è¯åº“å±•ç¤ºï¼š
    - Channelè¯ï¼ˆå¹³å°/æ¸ é“ï¼‰
    - Objectè¯ï¼ˆå¤„ç†å¯¹è±¡ï¼‰
    - Functionè¯ï¼ˆåŠŸèƒ½/åŠ¨ä½œï¼‰
    - Audienceè¯ï¼ˆç›®æ ‡ç¾¤ä½“ï¼‰

åº”ç”¨ï¼š
  - æ‹†ç½‘ç«™ç»“æ„
  - è®¾è®¡äº§å“åŠŸèƒ½
  - å†™landing pageæ–‡æ¡ˆ
```

**3. å…³é”®è¯-éœ€æ±‚-äº§å“æ˜ å°„è¡¨ï¼ˆæ‰§è¡Œå±‚ï¼‰**
```
åŠŸèƒ½ï¼š
  - æ¯ä¸ªå…³é”®è¯è®°å½•ï¼š
    - å…³é”®è¯æœ¬èº«
    - è§„èŒƒåŒ–è¡¨è¾¾ID
    - éœ€æ±‚ID
    - æœç´¢æ„å›¾
    - volume/KD/CPC
    - ï¼ˆæœªæ¥ï¼‰SERPç±»å‹ã€ç«å“ä¿¡æ¯

åº”ç”¨ï¼š
  - é€‰æ‹©å“ªäº›è¯åšURL
  - é€‰æ‹©å“ªäº›è¯åšé¡µé¢å†…è¦†ç›–
  - ç«å“åˆ†æ
```

### æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| **æ•°æ®å»é‡ç‡** | åŸºç¡€å»é‡ | +10-20%ï¼ˆè§„èŒƒåŒ–ï¼‰ | âœ… |
| **èšç±»è®¡ç®—é‡** | å…¨é‡è®¡ç®— | åŸºäºè§„èŒƒåŒ–ï¼ˆå‡å°‘é‡å¤ï¼‰ | âœ… -10-20% |
| **å¯å®¡æ ¸æ ·æœ¬** | 60-100ç°‡ | 1ä¸‡æ ·æœ¬è¯ | âœ… å¯æ§ |
| **å®¡æ ¸æ—¶é—´** | 2-4å°æ—¶ | < 2å°æ—¶ | âœ… |
| **è¯åº“è§„æ¨¡** | 26ä¸ª | æ•°åƒä¸ª | âœ… 100å€+ |
| **æ”¯æŒè§„æ¨¡** | 5-10ä¸‡ | 50ä¸‡+ | âœ… 5-10å€ |
| **éœ€æ±‚åˆ†ç±»** | æ—  | 5å¤§ç±»+å­ç±»å‹ | âœ… æ–°å¢ |

---

## é£é™©ä¸ç¼“è§£

### é£é™©1ï¼šè¯å½¢è¿˜åŸå¯èƒ½è¿‡åº¦å½’å¹¶

**é£é™©**ï¼š
```
"compressor" å’Œ "compress" éƒ½è¿˜åŸä¸º "compress"
ä½†åœ¨SEOä¸­ï¼Œä¸¤è€…å¯èƒ½æ˜¯ä¸åŒéœ€æ±‚ï¼š
  - "best image compressor" â†’ æ‰¾å·¥å…·
  - "how to compress image" â†’ æ‰¾æ•™ç¨‹
```

**ç¼“è§£**ï¼š
1. è§„èŒƒåŒ–åªç”¨äºç¬¬2å±‚ï¼ˆåˆ†æå±‚ï¼‰
2. åŸå§‹å…³é”®è¯å®Œæ•´ä¿ç•™åœ¨ç¬¬1å±‚
3. å¯é…ç½®ï¼šæ˜¯å¦å¯ç”¨stemmingï¼ˆå‚æ•°åŒ–ï¼‰

### é£é™©2ï¼šæ¨¡æ¿æå–å¯èƒ½è¿‡äºå®½æ³›

**é£é™©**ï¼š
```
æ¨¡æ¿ï¼š"[X] online"
å¯èƒ½åŒ¹é…å¤ªå¤šä¸ç›¸å…³çš„è¯
```

**ç¼“è§£**ï¼š
1. è®¾ç½®æœ€å°é¢‘æ¬¡é˜ˆå€¼ï¼ˆmin_template_freq=5ï¼‰
2. äººå·¥å®¡æ ¸é«˜é¢‘æ¨¡æ¿
3. è´¨é‡è¿‡æ»¤ï¼šå˜é‡å¿…é¡»é€‚é…â‰¥3ä¸ªæ¨¡æ¿

### é£é™©3ï¼šLLMåˆ†ç±»å¯èƒ½ä¸å‡†ç¡®

**é£é™©**ï¼š
```
LLMå¯èƒ½å°†"google drive"åˆ†ç±»é”™è¯¯
```

**ç¼“è§£**ï¼š
1. ä½¿ç”¨è§„åˆ™+LLMåŒé‡ç­–ç•¥
2. äººå·¥éªŒè¯é«˜é¢‘å˜é‡
3. æä¾›äººå·¥ä¿®æ­£ç•Œé¢

---

## æ€»ç»“

### æ ¸å¿ƒç›®æ ‡è¾¾æˆ

âœ… **å¸æ”¶å›è¨€æ ¸å¿ƒåˆ›æ–°**ï¼š
- ç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼ˆè¯çº§N-gramï¼‰
- æ¨¡æ¿-å˜é‡è¿­ä»£
- 4å¤§ç‰¹å¾ç»´åº¦
- éœ€æ±‚åˆ†ç±»ä½“ç³»

âœ… **ç¬¦åˆè‡ªèº«éœ€æ±‚**ï¼š
- ä¸‰å±‚æ•°æ®æ¨¡å‹ï¼ˆè§£å†³SEO vs åˆ†æçŸ›ç›¾ï¼‰
- å®Œæ•´ä¿ç•™å…³é”®è¯ï¼ˆæ”¯æŒSEOæ‰§è¡Œï¼‰
- è¯çº§å¤„ç†ï¼ˆé€‚é…è‹±æ–‡ï¼‰
- LLMè‡ªåŠ¨åŒ–ï¼ˆå‡å°‘äººå·¥ï¼‰

âœ… **æ¶æ„å‡çº§**ï¼š
- å•å±‚ â†’ ä¸‰å±‚æ•°æ®æ¨¡å‹
- å­—ç¬¦çº§ â†’ è¯çº§å¤„ç†
- åŸºç¡€å»é‡ â†’ è§„èŒƒåŒ–è¡¨è¾¾
- å°è¯åº“ â†’ å¤§è¯åº“ï¼ˆæ•°åƒä¸ªï¼‰

### å®æ–½è·¯çº¿å›¾

```
Phase 0: å‡†å¤‡ï¼ˆ1å‘¨ï¼‰
  â†’ æ•°æ®åº“å‡çº§ + ä»£ç é‡æ„

Phase 1: è§„èŒƒåŒ–å±‚ï¼ˆ2å‘¨ï¼‰
  â†’ è¯çº§è§„èŒƒåŒ– + èšç±»ä¼˜åŒ–

Phase 2: ç‰¹å¾ç‰‡æ®µï¼ˆ3å‘¨ï¼‰
  â†’ è¯çº§N-gram + æ ·æœ¬æå–

Phase 3: æ¨¡æ¿è¿­ä»£ï¼ˆ3å‘¨ï¼‰
  â†’ 4å¤§ç±»è¯åº“æ„å»º

Phase 4: éœ€æ±‚åˆ†ç±»ï¼ˆ2å‘¨ï¼‰
  â†’ æœç´¢æ„å›¾åˆ†ç±»

Phase 5: UIé›†æˆï¼ˆ1å‘¨ï¼‰
  â†’ å®Œæ•´é›†æˆåˆ°Web UI

æ€»è®¡ï¼š12å‘¨ï¼ˆ3ä¸ªæœˆï¼‰
```

### æœ€ç»ˆæˆæœ

ä¸€ä¸ª**ä¿ç•™è‹±æ–‡SEOæ ¸å¿ƒéœ€æ±‚**çš„åŒæ—¶ï¼Œ**å¸æ”¶å›è¨€æ–¹æ³•è®ºæ ¸å¿ƒä¼˜åŠ¿**çš„å®Œæ•´ç³»ç»Ÿï¼š

- âœ… ä¸‰å±‚æ•°æ®æ¨¡å‹
- âœ… è§„èŒƒåŒ–è¡¨è¾¾ï¼ˆè¯çº§ï¼‰
- âœ… ç‰¹å¾ç‰‡æ®µæ˜ å°„
- âœ… æ¨¡æ¿-å˜é‡è¿­ä»£
- âœ… 4å¤§ç±»è¯åº“ï¼ˆæ•°åƒä¸ªï¼‰
- âœ… æœç´¢æ„å›¾åˆ†ç±»
- âœ… æ”¯æŒ50ä¸‡+å…³é”®è¯
- âœ… å¯å®¡æ ¸ï¼ˆ<2å°æ—¶ï¼‰
- âœ… Web UIå‹å¥½

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-12-23
**é€‚ç”¨å¯¹è±¡**: æäº¤GPTè¯„å®¡
**ä¸‹ä¸€æ­¥**: GPTè¯„å®¡ â†’ ä¿®æ­£ â†’ å¼€å§‹å®æ–½
