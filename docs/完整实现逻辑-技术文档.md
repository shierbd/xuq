好，下面我给你一份可以直接丢给 Cloud Code 的完整流程说明，从「种子词 → 大组 → 小组 → 需求 → 词库 → 增量循环」一条线讲清楚，每一阶段都写清：
	•	目标
	•	输入是什么
	•	输出是什么
	•	用什么方式做（Python / 聚类 / 人工 / 大模型）

⸻

总览：从 0 到 N 轮的整体结构

整个系统分为两条主线、一个循环：
	1.	主线 A：从五万词 → 方向 → 需求 → 词库（第一次全量跑完）
	2.	主线 B：把所有东西存成三张“档案表”（phrases / demands / tokens）
	3.	循环：以后每次扩词，只做“增量处理”，不再从头烧脑

下面分阶段展开。

⸻

阶段 0：种子词与数据源（准备阶段）

目标：用一小批有代表性的种子词，把“世界边界”先扩出来一圈。
	•	输入：
	•	你手工选的 80+ 个种子词
	•	数据源：Semrush 导出、Google 下拉词、Google 相关搜索
	•	操作（已经在做的事情）：
	•	用种子词在 Semrush 导出 broad match 关键词
	•	用种子词抓 Google 下拉词
	•	用种子词抓 Google 相关搜索
	•	输出：
多份原始 CSV / Excel（按来源分类）

这一阶段本质是“把雷达打出去”，是人为控制方向，没问题，可以继续沿用。

⸻

阶段 1：原始数据整合 & 清洗

目标：把所有来源的数据整合成一份“干净的短语总表”。
	•	输入：
阶段 0 的所有 CSV / Excel
	•	操作逻辑：
	1.	字段统一
	•	把不同来源映射成统一核心字段：
	•	phrase：短语文本（统一用小写、去多余空格）
	•	seed_word：来自哪个种子词
	•	source_type：semrush / dropdown / related_search
	•	frequency / volume：用 Semrush 的 Volume；无 Volume 的统一给 1
	•	其他来源特有字段先保留（如 rank / expand_type）
	2.	数据清洗
	•	去掉：
	•	空字符串
	•	长度太短 (<3 字符)
	•	长度过长（比如 >100 字符，可以视作异常）
	•	文本标准化：
	•	全部转小写
	•	去头尾空格
	•	多个空格合并成一个
	3.	按 phrase 去重
	•	同一个短语，可能从多个来源来：
	•	频次统一累加（频次越大越重要）
	•	其他字段按规则合并（保留第一个或做加总）
	•	输出：
	•	一份「原始短语总表」：merged_keywords_all
	•	大概 5 万条（以后会变成 10 万、20 万…）

这一步结束后，你有一张：**“所有短语的大杂烩，但已经干净、可用”**的总表。

⸻

阶段 2：大组聚类 —— 做一张“世界大地图”

目标：把 5 万+ 短语，按语义粗分成 50–100 个 大方向（大组），成为“世界地图”。
	•	输入：
阶段 1 的短语总表（5 万+ phrase）
	•	操作逻辑：
	1.	文本转向量（Embedding）
	•	用一句话 → 向量 的模型（如 all-MiniLM 这类 sentence embedding）
	•	每个短语变成一个 384 维左右的向量
	•	相似短语在向量空间会“靠得近”
	2.	密度聚类做“大组”（HDBSCAN 类算法）
	•	不预设类别数量，让算法自己决定“这里密、那里稀”
	•	核心逻辑：
	•	文本越像 → 向量越近 → 越容易被分进同一组
	•	文本很乱、什么都不像 → 可能被标成噪音（-1）
	•	参数控制：
	•	min_cluster_size：组里至少要多少条才算一组（比如 20–40）
	•	min_samples：密度要求（越大越严格）
	3.	为每条短语记录所属大组
	•	给每条短语加字段：cluster_id_A（大组 ID）
	•	统计每个大组的：
	•	cluster_size：这个组有多少条
	•	total_frequency：总频次
	•	example_phrases：代表性的几条短语（频次最高的前 5 条）
	•	输出：
	1.	短语 + 大组结果表：
每行一条 phrase，附带：
	•	cluster_id_A：所属大组
	•	cluster_size：该大组大小
	•	is_noise：是不是噪音
	2.	大组汇总表（大地图）：
	•	每行一个 cluster_id_A（一个大组）
	•	字段包括：
	•	cluster_size
	•	total_frequency
	•	example_phrases（5 条代表短语）
	•	seed_words_in_cluster（这个组主要来自哪些种子词）

这一阶段得到的是：
一张“世界大地图”= 50–100 个大块，每块有自己代表性的例子短语。

⸻

阶段 3：从“世界地图”中选出想要的“区域”

目标：在大地图里，挑出值得深入挖掘的那些大方向，别被全部 5 万词淹死。
	•	输入：
	•	大组汇总表（大地图，cluster_summary_A）
	•	操作逻辑：
	1.	先“看懂”每个大组在说什么
	•	对于你不懂英语：
	•	把大组汇总表转成 HTML
	•	浏览器里直接“整页翻译”
	•	看：example_phrases + seed_words_in_cluster
	2.	给每个大组打一个“方向评分”
	•	维度可以包括：
	•	你是否容易理解 / 感兴趣
	•	变现潜力大不大（工具 / 内容 / 课程…）
	•	和你擅长领域贴不贴（例如：工具类、流程类、教育类 等）
	•	简单标记：
	•	⭐⭐⭐⭐⭐：必须重点挖
	•	⭐⭐⭐：有点价值，可以晚些
	•	⭐：暂时跳过
	3.	选出一批“目标大组”
	•	比如：从 60 个大组里选 10–15 个
	•	选完后，得到一个列表：
	•	selected_cluster_ids = [5, 12, 18, …]
	•	输出：
	•	一份“目标大组清单”：哪些大组要继续深入
	•	同时，我们也知道：
	•	没被选中的大组：暂时不重点看，但里面的数据不删

大地图 ≈ 世界地图
这一步就是：先画圈：我要重点开发哪些区域，其他地方先不住人。

⸻

阶段 4：小组聚类 & 需求挖掘（从“大片区域”到“一个个村子”）

目标：在每个“目标大组”内部，再切成很多更细的小组，每个小组尽量是“一个明确的需求主题”。
	•	输入：
	•	原始短语总表 + 大组分配结果
	•	目标大组 ID 列表
	•	操作逻辑：
对于每一个大组 cluster_id_A：
	1.	取出该大组的全部短语（几十～几百条）
	•	只看你选中的那些大组
	2.	在这个大组内部，再做一次“二次聚类”（小组）
	•	思路和大组类似，但参数更小、更细：
	•	min_cluster_size 调低（比如 5–15）
	•	得到：
	•	小组 ID：cluster_id_B（二级组）
	•	每个小组内部，短语相似度更高
	3.	对小组进行“人工 + AI”理解
	•	每个小组，抓几个代表短语出来（频次高的前 10 条）
	•	用中文总结：“这个小组在说什么？”
	•	看：是否是“真正可用的需求”：
	•	是否有明确对象（谁）
	•	明确行为（做什么）
	•	明确结果（想要什么）
	4.	把“小组”提升为“需求卡片（Demand）”
	•	对于那些 主题很清晰的小组：
	•	新建一条 demand：
	•	标题：一句话中文概括
	•	描述：这个需求的详细情况、用户场景
	•	把这个小组里的所有短语都绑定到这条需求卡片上：
	•	在短语表里填 mapped_demand_id
	•	输出：
	•	一批 结构化的“需求卡片”（demands 表）
	•	每条需求卡片下面，挂着很多真实的短语（phrase）

这一步结束后，你不再面对“海量零散的词”，而是面对：
一张张“需求卡片” + 每张卡片下面一堆真实搜索句子。

⸻

阶段 5：词拆解 & 需求框架词库（Tokens）

目标：把这些句子中反复出现的“关键构成要素”拆出来，形成可复用的词库，支撑后续继续扩词和分析。
	•	输入：
	•	已经选出的需求相关短语（即已经对接到 demands 的那批 phrase）
	•	操作逻辑（两层）：

5.1 先用 Python 做“粗拆 + 去重”（大批量阶段）
	1.	对所有相关短语做分词 / 拆解
	•	英文的话，按空格拆词，再做简单处理：
	•	去停用词（the / a / of / for …）
	•	去掉纯数字 / 标点
	•	做词形还原（compress / compression → compress）
	2.	统计每个 token 的出现频次
	•	得到一批候选 token：
	•	例如：compress / mp3 / pdf / remove / watermark / online / free …
	3.	按频次排序 + 粗分类
	•	用 Python 规则粗分：
	•	动作词候选：compress / remove / convert / download …
	•	对象词候选：mp3 / pdf / image / video …
	•	其他词候选：online / free / fast / best …
	4.	得到一张“候选词清单”
	•	数量从几千句 → 几百个 token

5.2 再用你 + AI 精修为“需求框架词库”
	1.	用人眼 + 大模型，最终给每个 token 贴上类型
	•	对每个 token 明确：
	•	是 [意图/动作词] / [对象词] / [结果词] / [属性词] / [条件词]…
	•	丢进你定义的那个「需求框架」里
	2.	形成一张正式的 tokens 表
	•	字段包括：
	•	token_text
	•	token_type（动词/对象/结果/属性/条件…）
	•	first_seen_round
	•	出现次数统计

	•	输出：
	•	一张干净的“需求框架词库”（tokens 表）
	•	后面所有扩词、筛选和判断，都可以用这份词库作为“识别器”。

⸻

阶段 6：三张核心档案表成型（系统骨架）

到这里，系统的“骨架”成型为三张表：
	1.	phrases 表（短语档案库）
	•	每条搜索短语一行
	•	关键字段：
	•	phrase
	•	seed_word
	•	source_type
	•	frequency / volume
	•	cluster_id_A（大组）
	•	cluster_id_B（小组，可选）
	•	mapped_demand_id（属于哪个需求卡片）
	•	processed_status：unseen / reviewed / used
	•	first_seen_round：第几轮出现
	2.	demands 表（需求卡片库）
	•	每条“抽象需求”一行
	•	字段：
	•	demand_id
	•	title（一句话）
	•	description（详细）
	•	status（想法/已验证/已上线/已赚钱…）
	•	related_phrases_count
	•	main_tokens（这个需求所对应的关键 tokens）
	3.	tokens 表（需求框架词库）
	•	每个可复用的构成要素词一行
	•	字段：
	•	token_text
	•	token_type
	•	first_seen_round
	•	in_phrase_count
	•	in_demand_count

以后所有迭代，都围绕这三张表运转，不再是把数据当一次性结果。

⸻

阶段 7：多轮扩词时的“增量逻辑”（5 万 → 10 万 → 20 万）

目标：后面每次扩到 10 万、20 万词时，不再从头烧脑，而是让系统帮你自动“剔除已处理的东西”，只关注真正新的部分。

假设你已经完成第 1 轮，接下来：

7.1 新一轮扩词导入
	•	输入：
	•	新一批基于“老种子 + 新种子”导出的 CSV
	•	操作：
	1.	合并到 phrases 总表，按 phrase 去重：
	•	如果 phrase 之前就存在：
	•	更新 frequency / volume（取最大或求和）
	•	保留原来的 first_seen_round、mapped_demand_id 等
	•	如果是全新 phrase：
	•	插入新行，first_seen_round = 当前轮数
	•	processed_status = unseen

7.2 新数据的聚类分配（增量）
	•	思路：
旧词当“锚点”，新词挂上去。
	•	操作逻辑示意：
	1.	对“新词子集”做 embedding
	2.	根据距离最近的大组中心，把新词分到某个 cluster_id_A
	•	必要时可以对“新词 + 该大组部分代表旧词”一起做一次小型聚类，保证结构不崩
	3.	更新 phrases 表中这些新行的 cluster_id_A / cluster_id_B

7.3 人工/AI 精力的“剔除规则”

从第 3 轮开始，每次你打算认真看一批词 / 小组 / 需求时，统一用以下过滤：
	1.	短语层面的过滤：不再重复理解同一类东西
	•	只把以下短语放进“候选池”：
	•	processed_status in (unseen, reviewed)
	•	mapped_demand_id 为空 或 目前关联的 demand 状态仍不明确
	•	自动跳过：
	•	已经有明确需求卡片，并且状态 = 已验证 / 已上线 的 phrase
	•	那些只是同义长尾变体，不需要你再思考的（可以只当“SEO 词库”）
	2.	需求层面的过滤：不再重复挖同一方向
	•	对于每个大组，先看：
	•	这个大组对应的需求类型，之前是否已经有稳定的 demand 卡片？
	•	新一轮里，这个大组选出来的新短语，有没有出现全新 tokens 组合？
	•	如果只是“老需求 + 新长尾”，就：
	•	把新短语挂到原有 demand 上
	•	不再新开 demand，不占你的脑力
	3.	词库层面：持续补充 tokens
	•	每轮结束前，再对“新短语集合”做一次粗分词，
	•	看有没有新出现的动词 / 对象词，添进 tokens 词库。

这样设计后：
	•	数据：永远累积（不会丢、不会乱）
	•	精力：永远只投在“真正新的东西”上
	•	每一轮都是在前一轮基础上的增量升级。

⸻

阶段 8：落地输出（给赚钱 / 产品 / 内容用）

目标：让前面这一大堆结构，最终变成可执行的东西，而不是“研究本身”。
	•	每条 demand 可以对应：
	•	一个工具产品思路
	•	一套文章 / 视频选题清单
	•	一个服务方案（例如代做 / 咨询）
	•	每条 demand 下的短语（phrase）：
	•	可以生成 landing page 的标题 / H1 / FAQ
	•	可以生成文章目录（按疑问词拆）
	•	可以生成广告投放词包
	•	可以作为 SEO 内容的“素材池”
	•	tokens 词库：
	•	可以用于自动扩词（组合动作 + 对象 + 属性形成新词）
	•	可以用于快速识别“这个需求属于哪一类”，“有没有做过类似的？”

⸻




