# ç»Ÿä¸€åˆ†è¯é€»è¾‘ä¿®æ”¹è®¡åˆ’

**åˆ›å»ºæ—¶é—´**: 2026-01-06
**ç›®æ ‡**: å®ç°ç©·å°½å¼n-gramæå–ï¼Œç»Ÿä¸€å¤„ç†1-6è¯ç»„åˆ

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

### å½“å‰å®ç°çš„é—®é¢˜ï¼š
1. **åˆ†è¯é€»è¾‘åˆ†ç¦»**ï¼š`segment_keywords_with_seed_tracking()` å°†å•è¯ï¼ˆ1-gramï¼‰å’ŒçŸ­è¯­ï¼ˆ2-6-gramï¼‰åˆ†å¼€å¤„ç†
2. **çŸ­è¯­æå–å¯é€‰**ï¼š`extract_ngrams` å‚æ•°ä½¿å¾—çŸ­è¯­æå–æ˜¯"å¯é€‰"çš„ï¼Œä¸ç¬¦åˆç©·å°½åˆ†è¯é€»è¾‘
3. **æ•°æ®ç»“æ„åˆ†ç¦»**ï¼šè¿”å› `word_counter` å’Œ `ngram_counter` ä¸¤ä¸ªç‹¬ç«‹çš„Counter
4. **UIè¯¯å¯¼æ€§**ï¼šPhase 0 æœ‰"æå–çŸ­è¯­"checkboxï¼Œæš—ç¤ºè¿™æ˜¯å¯é€‰åŠŸèƒ½

### æ­£ç¡®çš„éœ€æ±‚ï¼š
å¯¹æ‰€æœ‰é•¿å°¾è¯è¿›è¡Œ**ç©·å°½n-gramæå–**ï¼š
- æå–æ‰€æœ‰1è¯ã€2è¯ã€3è¯...6è¯çš„ç»„åˆ
- ç”¨**ç»Ÿä¸€çš„é¢‘æ¬¡é˜ˆå€¼**è¿‡æ»¤ï¼ˆä¸åŒºåˆ†å•è¯/çŸ­è¯­ï¼‰
- è¿”å›**ç»Ÿä¸€çš„åˆ†è¯ç»“æœ**

---

## ğŸ“‹ è¯¦ç»†ä¿®æ”¹è®¡åˆ’

### 1ï¸âƒ£ **ä¿®æ”¹æ ¸å¿ƒåˆ†è¯å‡½æ•°** (utils/keyword_segmentation.py)

#### æ–‡ä»¶ï¼š`utils/keyword_segmentation.py`

#### ä¿®æ”¹å†…å®¹ï¼š

**æ–°å¢å‡½æ•°**ï¼š`segment_keywords_unified()` - ç»Ÿä¸€çš„ç©·å°½n-gramæå–

```python
def segment_keywords_unified(
    phrases_objects: list,
    stopwords: Set[str],
    min_frequency: int = 2,
    max_ngram_length: int = 6
) -> Tuple[Counter, Dict[str, Set[str]]]:
    """
    ç»Ÿä¸€æå–1-6è¯çš„æ‰€æœ‰n-gramï¼ˆç©·å°½å¼åˆ†è¯ï¼‰

    Args:
        phrases_objects: Phraseå¯¹è±¡åˆ—è¡¨
        stopwords: åœç”¨è¯é›†åˆ
        min_frequency: æœ€å°é¢‘æ¬¡é˜ˆå€¼ï¼ˆé€‚ç”¨äºæ‰€æœ‰n-gramï¼‰
        max_ngram_length: æœ€å¤§n-gramé•¿åº¦ï¼ˆé»˜è®¤6ï¼‰

    Returns:
        (token_counter, token_to_seeds)
        - token_counter: {token: frequency} ç»Ÿä¸€çš„Counter
        - token_to_seeds: {token: {seed1, seed2, ...}} æ¯ä¸ªtokenå¯¹åº”çš„seeds
    """
    token_counter = Counter()
    token_to_seeds = defaultdict(set)

    for phrase_obj in phrases_objects:
        keyword = phrase_obj.phrase
        seed_word = phrase_obj.seed_word or "unknown"

        # è½¬å°å†™å¹¶åˆ†è¯
        keyword_lower = keyword.lower()
        words = re.split(r'[\s\-_]+', keyword_lower)

        # è¿‡æ»¤å•è¯
        filtered_words = [
            w for w in words
            if w not in stopwords
            and len(w) >= 2
            and re.match(r'^[a-z]+$', w)
        ]

        # ç©·å°½æå–1-gramåˆ°max_ngram_length-gram
        for n in range(1, min(max_ngram_length + 1, len(filtered_words) + 1)):
            for i in range(len(filtered_words) - n + 1):
                ngram = ' '.join(filtered_words[i:i+n])
                token_counter[ngram] += 1
                token_to_seeds[ngram].add(seed_word)

    # åº”ç”¨é¢‘æ¬¡é˜ˆå€¼è¿‡æ»¤
    filtered_tokens = {
        token: count
        for token, count in token_counter.items()
        if count >= min_frequency
    }

    # åªä¿ç•™è¢«ä¿ç•™çš„tokençš„seedsä¿¡æ¯
    filtered_token_to_seeds = {
        token: seeds
        for token, seeds in token_to_seeds.items()
        if token in filtered_tokens
    }

    return Counter(filtered_tokens), dict(filtered_token_to_seeds)
```

**ä¿ç•™æ—§å‡½æ•°**ï¼š`segment_keywords_with_seed_tracking()` - å‘åå…¼å®¹

- ä¿ç•™å‡½æ•°ç­¾åä¸å˜
- å†…éƒ¨è°ƒç”¨ `segment_keywords_unified()`
- æ ¹æ®è¿”å›å€¼æ‹†åˆ†ä¸º word_counter å’Œ ngram_counter

```python
def segment_keywords_with_seed_tracking(
    phrases_objects: list,
    stopwords: Set[str],
    extract_ngrams: bool = True,  # ä¿æŒå‚æ•°ä½†å¿½ç•¥
    min_ngram_frequency: int = 2
) -> Tuple[Counter, Dict[str, Set[str]], Counter, Dict[str, Set[str]]]:
    """
    å‘åå…¼å®¹çš„åˆ†è¯å‡½æ•°ï¼ˆå†…éƒ¨è°ƒç”¨segment_keywords_unifiedï¼‰

    æ³¨æ„ï¼šextract_ngramså‚æ•°å·²åºŸå¼ƒï¼Œæ€»æ˜¯æå–n-grams
    """
    # è°ƒç”¨ç»Ÿä¸€åˆ†è¯å‡½æ•°
    token_counter, token_to_seeds = segment_keywords_unified(
        phrases_objects,
        stopwords,
        min_frequency=min_ngram_frequency,
        max_ngram_length=6
    )

    # ä¸ºäº†å‘åå…¼å®¹ï¼Œæ‹†åˆ†ä¸ºword_counterå’Œngram_counter
    word_counter = Counter()
    word_to_seeds = {}
    ngram_counter = Counter()
    ngram_to_seeds = {}

    for token, count in token_counter.items():
        word_count = len(token.split())

        if word_count == 1:
            word_counter[token] = count
            word_to_seeds[token] = token_to_seeds[token]
        else:
            ngram_counter[token] = count
            ngram_to_seeds[token] = token_to_seeds[token]

    return word_counter, word_to_seeds, ngram_counter, ngram_to_seeds
```

**ä¿®æ”¹å¢é‡åˆ†è¯å‡½æ•°**ï¼š`segment_new_phrases_incrementally()` - ä½¿ç”¨æ–°çš„ç»Ÿä¸€é€»è¾‘

---

### 2ï¸âƒ£ **ä¿®æ”¹Phase 0 UI** (ui/pages/phase0_expansion.py)

#### æ–‡ä»¶ï¼š`ui/pages/phase0_expansion.py`

#### ä¿®æ”¹å†…å®¹ï¼š

**åˆ é™¤**ï¼š
- ç¬¬495-500è¡Œï¼š`extract_ngrams` checkbox
- ç¬¬507-520è¡Œï¼šæ¡ä»¶æ€§çš„n-gramé…ç½®UI

**ä¿®æ”¹ä¸º**ï¼š

```python
# ç»Ÿä¸€çš„åˆ†è¯é…ç½®
st.markdown("### âš™ï¸ åˆ†è¯é…ç½®")

st.info("ğŸ’¡ ç³»ç»Ÿä¼šè‡ªåŠ¨æå–æ‰€æœ‰1-6è¯çš„è¯ç»„ç»„åˆï¼ˆç©·å°½å¼n-gramæå–ï¼‰ï¼Œæ‚¨å¯ä»¥è®¾ç½®é¢‘æ¬¡é˜ˆå€¼æ¥è¿‡æ»¤ä½é¢‘è¯ç»„")

min_token_frequency = st.number_input(
    "æœ€å°è¯é¢‘é˜ˆå€¼",
    min_value=2,
    max_value=20,
    value=seg_prefs.get('min_token_frequency', 3),
    help="åªä¿ç•™å‡ºç°æ¬¡æ•° >= æ­¤å€¼çš„è¯ç»„ï¼ˆæ— è®º1è¯ã€2è¯...è¿˜æ˜¯6è¯ï¼‰",
    key="min_token_frequency_input"
)

st.markdown("**æå–èŒƒå›´**: è‡ªåŠ¨æå–1è¯ã€2è¯ã€3è¯ã€4è¯ã€5è¯ã€6è¯çš„æ‰€æœ‰ç»„åˆ")
```

**ä¿®æ”¹è°ƒç”¨é€»è¾‘**ï¼ˆç¬¬618-626è¡Œï¼‰ï¼š

```python
# åŸä»£ç ï¼š
# word_counter, word_to_seeds, ngram_counter, ngram_to_seeds = segment_keywords_with_seed_tracking(
#     st.session_state.phrases_cache,
#     st.session_state.stopwords,
#     extract_ngrams=extract_ngrams,
#     min_ngram_frequency=min_ngram_frequency if extract_ngrams else 2
# )

# æ–°ä»£ç ï¼š
word_counter, word_to_seeds, ngram_counter, ngram_to_seeds = segment_keywords_with_seed_tracking(
    st.session_state.phrases_cache,
    st.session_state.stopwords,
    extract_ngrams=True,  # æ€»æ˜¯True
    min_ngram_frequency=min_token_frequency
)
```

**ä¿®æ”¹ä¿å­˜é€»è¾‘**ï¼ˆç¬¬576-603è¡Œï¼‰ï¼š

```python
# åŸä»£ç ï¼š
# ngram_counter=merged_ngram_counter if extract_ngrams else None

# æ–°ä»£ç ï¼š
ngram_counter=merged_ngram_counter  # æ€»æ˜¯ä¿å­˜ngrams
```

**ä¿®æ”¹é…ç½®ä¿å­˜**ï¼ˆæ›´æ–°seg_prefsï¼‰ï¼š

```python
# ä¿å­˜é…ç½®
seg_prefs = {
    'min_token_frequency': min_token_frequency,  # ç»Ÿä¸€çš„é¢‘æ¬¡é˜ˆå€¼
    # åˆ é™¤ 'extract_ngrams': extract_ngrams
    # åˆ é™¤ 'min_ngram_frequency': min_ngram_frequency
}
```

---

### 3ï¸âƒ£ **ä¿®æ”¹WordSegmentRepository** (storage/word_segment_repository.py)

#### æ–‡ä»¶ï¼š`storage/word_segment_repository.py`

#### ä¿®æ”¹å†…å®¹ï¼š

**æ–°å¢æ–¹æ³•**ï¼š`load_all_tokens()` - ç»Ÿä¸€åŠ è½½æ‰€æœ‰tokens

```python
def load_all_tokens(
    self,
    min_frequency: int = 1
) -> Tuple[Counter, Dict, Dict, SegmentationBatch]:
    """
    ç»Ÿä¸€åŠ è½½æ‰€æœ‰tokensï¼ˆä¸åŒºåˆ†å•è¯/çŸ­è¯­ï¼‰

    Args:
        min_frequency: æœ€å°é¢‘æ¬¡é˜ˆå€¼

    Returns:
        (
            token_counter: {token: frequency} æ‰€æœ‰tokensçš„ç»Ÿä¸€Counter,
            pos_tags: {word: (pos_tag, pos_category, pos_chinese)} ä»…1-gramæœ‰è¯æ€§,
            translations: {token: translation},
            latest_batch: æœ€æ–°æ‰¹æ¬¡ä¿¡æ¯
        )
    """
    # åŠ è½½æ‰€æœ‰word_segmentsï¼ˆä¸åŒºåˆ†word_countï¼‰
    all_tokens_query = self.session.query(WordSegment).filter(
        WordSegment.frequency >= min_frequency
    ).all()

    token_counter = Counter()
    pos_tags = {}
    translations = {}

    for ws in all_tokens_query:
        token_counter[ws.word] = ws.frequency

        # è¯æ€§ä¿¡æ¯ï¼ˆä»…å•è¯æœ‰ï¼‰
        if ws.word_count == 1 and ws.pos_tag:
            pos_tags[ws.word] = (ws.pos_tag, ws.pos_category, ws.pos_chinese)

        # ç¿»è¯‘
        if ws.translation:
            translations[ws.word] = ws.translation

    latest_batch = self.get_latest_batch()

    return token_counter, pos_tags, translations, latest_batch
```

**ä¿ç•™æ–¹æ³•**ï¼š`load_segmentation_results()` - å‘åå…¼å®¹ï¼Œä½†æ ‡è®°ä¸ºdeprecated

```python
def load_segmentation_results(
    self,
    min_word_frequency: int = 1,
    min_ngram_frequency: int = 1
) -> Tuple[Counter, Counter, Dict, Dict, Dict, SegmentationBatch]:
    """
    ä»æ•°æ®åº“åŠ è½½å®Œæ•´çš„åˆ†è¯ç»“æœï¼ˆå•è¯+çŸ­è¯­ï¼‰

    æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨ load_all_tokens()
    ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå‘åå…¼å®¹
    """
    # ä¿æŒç°æœ‰å®ç°ä¸å˜
    ...
```

**ä¿®æ”¹ `save_word_segments()`**ï¼šå¼ºåˆ¶ä¿å­˜ngrams

```python
def save_word_segments(
    self,
    word_counter: Counter,
    pos_tags: Optional[Dict[str, Tuple[str, str, str]]] = None,
    translations: Optional[Dict[str, str]] = None,
    batch_id: Optional[int] = None,
    ngram_counter: Optional[Counter] = None,  # ä¿æŒå¯é€‰ä»¥å…¼å®¹
    ngram_translations: Optional[Dict[str, str]] = None
) -> Tuple[int, int]:
    """
    ä¿å­˜æˆ–æ›´æ–°åˆ†è¯ç»“æœï¼ˆæ”¯æŒå•è¯å’ŒçŸ­è¯­ï¼‰

    æ³¨æ„ï¼šè™½ç„¶ngram_counteræ˜¯å¯é€‰å‚æ•°ï¼Œä½†å¼ºçƒˆå»ºè®®æ€»æ˜¯æä¾›
    å› ä¸ºç»Ÿä¸€åˆ†è¯é€»è¾‘æ€»æ˜¯ä¼šæå–n-grams
    """
    # ä¿æŒç°æœ‰å®ç°ä¸å˜
    ...
```

---

### 4ï¸âƒ£ **ä¿®æ”¹Phase 2Eæ•°æ®åŠ è½½** (ui/pages/phase2e_junyan.py)

#### æ–‡ä»¶ï¼š`ui/pages/phase2e_junyan.py`

#### ä¿®æ”¹å†…å®¹ï¼š

**æ–¹æ¡ˆAï¼šä½¿ç”¨æ–°çš„load_all_tokens()æ–¹æ³•**

```python
# ç¬¬134-156è¡Œï¼š
with st.spinner(f"æ­£åœ¨ä»Phase 0åŠ è½½åˆ†è¯ç»“æœ..."):
    # ä½¿ç”¨æ–°çš„ç»Ÿä¸€åŠ è½½æ–¹æ³•
    with WordSegmentRepository() as ws_repo:
        token_counter, pos_tags, translations, latest_batch = ws_repo.load_all_tokens(
            min_frequency=min_word_freq
        )

    # è¿‡æ»¤é•¿åº¦ï¼ˆè‡³å°‘2ä¸ªå­—ç¬¦ï¼‰
    filtered_candidates = [
        (token, freq)
        for token, freq in token_counter.most_common()
        if len(token) >= 2
    ]

    # ä¿å­˜åˆ°session_state
    st.session_state.candidate_seeds = filtered_candidates[:max_display]

st.success(f"âœ… å·²åŠ è½½ {len(token_counter):,} ä¸ªå€™é€‰ç§å­è¯ï¼ˆåŒ…æ‹¬å•è¯å’ŒçŸ­è¯­ï¼‰ï¼Œç­›é€‰åæ˜¾ç¤º {len(st.session_state.candidate_seeds)} ä¸ª")
```

**æ–¹æ¡ˆBï¼šç»§ç»­ä½¿ç”¨æ—§æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰**

ä¿æŒç°æœ‰ä»£ç ä¸å˜ï¼Œå› ä¸º `load_segmentation_results()` å·²ç»åˆå¹¶äº†word_counterå’Œngram_counterã€‚

---

### 5ï¸âƒ£ **ä¿®æ”¹Tokenæå–å‡½æ•°** (core/token_extraction.py)

#### æ–‡ä»¶ï¼š`core/token_extraction.py`

#### ä¿®æ”¹å†…å®¹ï¼š

**æ–¹æ¡ˆAï¼šä½¿ç”¨æ–°çš„load_all_tokens()æ–¹æ³•**

```python
def extract_tokens_from_word_segments(
    min_frequency: int = 3
) -> List[Tuple[str, int]]:
    """
    ä»word_segmentsè¡¨æå–Tokensï¼ˆç»Ÿä¸€çš„1-6-gramï¼‰

    Args:
        min_frequency: æœ€å°é¢‘æ¬¡ï¼ˆè¿‡æ»¤ä½é¢‘è¯ï¼‰

    Returns:
        [(token, frequency), ...] æŒ‰é¢‘æ¬¡é™åºæ’åˆ—
    """
    print(f"\nä»word_segmentsæå–Tokensï¼ˆæœ€å°é¢‘æ¬¡: {min_frequency}ï¼‰...")

    with WordSegmentRepository() as ws_repo:
        # ä½¿ç”¨æ–°çš„ç»Ÿä¸€åŠ è½½æ–¹æ³•
        token_counter, _, _, _ = ws_repo.load_all_tokens(
            min_frequency=min_frequency
        )

    print(f"  å‘ç° {len(token_counter):,} ä¸ªTokensï¼ˆ1-6è¯ï¼‰")

    # è¿”å›æ’åºåˆ—è¡¨
    return token_counter.most_common()
```

**æ–¹æ¡ˆBï¼šç»§ç»­ä½¿ç”¨æ—§æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰**

ä¿æŒç°æœ‰ä»£ç ä¸å˜ã€‚

---

### 6ï¸âƒ£ **ä¿®æ”¹Phase 4 UI** (ui/pages/phase4_tokens.py)

#### æ–‡ä»¶ï¼š`ui/pages/phase4_tokens.py`

#### ä¿®æ”¹å†…å®¹ï¼š

**æ›´æ–°æè¿°æ–‡å­—**ï¼ˆç¬¬72è¡Œï¼‰ï¼š

```python
with col2:
    st.markdown("### âš™ï¸ æå–å‚æ•°")

    # Check word_segments status
    try:
        with WordSegmentRepository() as ws_repo:
            stats = ws_repo.get_statistics()
            total_words = stats.get('total_words', 0)

        if total_words == 0:
            st.warning("âš ï¸ æœªæ‰¾åˆ°åˆ†è¯ç»“æœï¼è¯·å…ˆå‰å¾€ **Phase 0 Tab 1** æ‰§è¡Œåˆ†è¯")
        else:
            st.info(f"ğŸ“Š åˆ†è¯ç»“æœï¼š{total_words:,} ä¸ªè¯ç»„ï¼ˆ1-6è¯çš„ç»Ÿä¸€åˆ†è¯ï¼‰")  # â† æ›´æ–°æè¿°
    except Exception as e:
        st.error(f"æ— æ³•è·å–åˆ†è¯æ•°æ®: {str(e)}")
```

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’

### æµ‹è¯•ç”¨ä¾‹1ï¼šPhase 0åˆ†è¯
**è¾“å…¥**ï¼š
```
best running shoes
running shoes for women
cheap running shoes
```

**é¢„æœŸè¾“å‡º**ï¼ˆå‡è®¾min_frequency=2ï¼‰ï¼š
```
Token Counter:
  running: 3
  shoes: 3
  running shoes: 3
  ï¼ˆå…¶ä»–é¢‘æ¬¡<2çš„è¢«è¿‡æ»¤ï¼‰
```

**éªŒè¯ç‚¹**ï¼š
- word_segmentsè¡¨ä¸­æœ‰1-gramï¼ˆrunning, shoesï¼‰
- word_segmentsè¡¨ä¸­æœ‰2-gramï¼ˆrunning shoesï¼‰
- æ²¡æœ‰"æå–çŸ­è¯­"checkbox
- åªæœ‰ä¸€ä¸ª"æœ€å°è¯é¢‘é˜ˆå€¼"é…ç½®

---

### æµ‹è¯•ç”¨ä¾‹2ï¼šPhase 2EåŠ è½½ç§å­è¯
**å‰æ**ï¼šå·²å®ŒæˆPhase 0åˆ†è¯

**æ“ä½œ**ï¼šç‚¹å‡»"ä»Phase 0åŠ è½½åˆ†è¯ç»“æœ"

**é¢„æœŸç»“æœ**ï¼š
- æ˜¾ç¤ºç»Ÿä¸€çš„å€™é€‰ç§å­è¯åˆ—è¡¨ï¼ˆåŒ…å«å•è¯å’ŒçŸ­è¯­ï¼‰
- é¢‘æ¬¡ç»Ÿè®¡æ­£ç¡®
- åŠ è½½æ—¶é—´ < 3ç§’

---

### æµ‹è¯•ç”¨ä¾‹3ï¼šPhase 4æå–Tokens
**å‰æ**ï¼šå·²å®ŒæˆPhase 0åˆ†è¯

**æ“ä½œ**ï¼šè®¾ç½®min_frequency=3ï¼Œç‚¹å‡»"å¼€å§‹æå–"

**é¢„æœŸç»“æœ**ï¼š
- æå–çš„tokensåŒ…æ‹¬å•è¯å’ŒçŸ­è¯­
- é¢‘æ¬¡â‰¥3çš„éƒ½è¢«ä¿ç•™
- æ•°æ®ä¸Phase 0ä¸€è‡´

---

### æµ‹è¯•ç”¨ä¾‹4ï¼šå®Œæ•´æµç¨‹
**æµç¨‹**ï¼š
1. Phase 1å¯¼å…¥CSVæ•°æ®
2. Phase 0æ‰§è¡Œç»Ÿä¸€åˆ†è¯ï¼ˆè®¾ç½®min_frequency=3ï¼‰
3. Phase 2EåŠ è½½ç§å­è¯
4. Phase 4æå–tokens

**éªŒè¯ç‚¹**ï¼š
- æ•´ä¸ªæµç¨‹æ— æŠ¥é”™
- åŒä¸€ä¸ªè¯åœ¨Phase 0ã€2Eã€4ä¸­é¢‘æ¬¡ä¸€è‡´
- çŸ­è¯­å’Œå•è¯ç»Ÿä¸€å¤„ç†ï¼Œæ— åŒºåˆ†

---

## ğŸ“Š æ•°æ®åº“å…¼å®¹æ€§

### word_segmentsè¡¨ç»“æ„
**ä¿æŒä¸å˜**ï¼š
- `word_count` å­—æ®µï¼ˆ1-6ï¼‰ï¼šæ ‡è¯†tokençš„è¯æ•°
- æ–¹ä¾¿åç»­ç­›é€‰å’Œåˆ†æ

### æ•°æ®è¿ç§»
**ä¸éœ€è¦è¿ç§»**ï¼š
- ç°æœ‰æ•°æ®ç»“æ„å®Œå…¨å…¼å®¹
- åªæ˜¯é€»è¾‘ä¸Šç»Ÿä¸€å¤„ç†

---

## ğŸ”„ å‘åå…¼å®¹æ€§

### ä¿æŒå…¼å®¹çš„APIï¼š
1. `segment_keywords_with_seed_tracking()` - ä¿ç•™ä½†å†…éƒ¨è°ƒç”¨æ–°å‡½æ•°
2. `WordSegmentRepository.load_segmentation_results()` - ä¿ç•™ä½†æ ‡è®°deprecated
3. `WordSegmentRepository.save_word_segments()` - ä¿æŒç­¾åä¸å˜

### æ–°å¢APIï¼š
1. `segment_keywords_unified()` - æ¨èä½¿ç”¨
2. `WordSegmentRepository.load_all_tokens()` - æ¨èä½¿ç”¨

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ¸è¿›å¼è¿ç§»**ï¼š
   - å…ˆå®ç°æ–°å‡½æ•°
   - ä¿ç•™æ—§å‡½æ•°ä½œä¸ºwrapper
   - é€æ­¥è¿ç§»è°ƒç”¨ç‚¹

2. **UIå˜åŒ–**ï¼š
   - Phase 0 UIä¼šæœ‰æ˜æ˜¾å˜åŒ–ï¼ˆåˆ é™¤checkboxï¼‰
   - éœ€è¦åœ¨æ–‡æ¡£ä¸­è¯´æ˜

3. **æ€§èƒ½å½±å“**ï¼š
   - ç»Ÿä¸€æå–1-6-gramå¯èƒ½æ¯”ä¹‹å‰å¤šæå–1-gram
   - ä½†ç”±äºæ—§å®ç°å·²ç»åœ¨æå–ï¼Œå½±å“ä¸å¤§

4. **æ•°æ®ä¸€è‡´æ€§**ï¼š
   - ç¡®ä¿åŒä¸€ä¸ªtokenåœ¨å„ä¸ªPhaseæ˜¾ç¤ºçš„é¢‘æ¬¡ä¸€è‡´
   - å¢é‡åˆ†è¯æ—¶æ­£ç¡®ç´¯åŠ 

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | ä¿®æ”¹ç±»å‹ | ä¼˜å…ˆçº§ |
|------|---------|--------|
| `utils/keyword_segmentation.py` | æ–°å¢+ä¿®æ”¹ | P0ï¼ˆæ ¸å¿ƒï¼‰ |
| `ui/pages/phase0_expansion.py` | ä¿®æ”¹UI | P0ï¼ˆæ ¸å¿ƒï¼‰ |
| `storage/word_segment_repository.py` | æ–°å¢æ–¹æ³• | P1ï¼ˆå»ºè®®ï¼‰ |
| `ui/pages/phase2e_junyan.py` | å¯é€‰ä¿®æ”¹ | P2ï¼ˆå¯é€‰ï¼‰ |
| `core/token_extraction.py` | å¯é€‰ä¿®æ”¹ | P2ï¼ˆå¯é€‰ï¼‰ |
| `ui/pages/phase4_tokens.py` | UIæ–‡å­— | P2ï¼ˆå¯é€‰ï¼‰ |

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶ï¼š
- [ ] Phase 0åˆ†è¯æå–æ‰€æœ‰1-6-gram
- [ ] åªæœ‰ä¸€ä¸ªç»Ÿä¸€çš„min_frequencyé˜ˆå€¼
- [ ] åˆ é™¤äº†"æå–çŸ­è¯­"checkbox
- [ ] Phase 2Eèƒ½æ­£ç¡®åŠ è½½å•è¯+çŸ­è¯­
- [ ] Phase 4èƒ½æ­£ç¡®åŠ è½½å•è¯+çŸ­è¯­
- [ ] æ•°æ®æµç»Ÿä¸€åˆ°word_segmentsè¡¨

### æ€§èƒ½éªŒæ”¶ï¼š
- [ ] Phase 0åˆ†è¯è€—æ—¶ä¸ä¹‹å‰ç›¸å½“ï¼ˆå› ä¸ºé€»è¾‘åŸºæœ¬ç›¸åŒï¼‰
- [ ] Phase 2EåŠ è½½ < 3ç§’
- [ ] Phase 4åŠ è½½ < 3ç§’

### æ•°æ®è´¨é‡ï¼š
- [ ] åŒä¸€è¯åœ¨Phase 0ã€2Eã€4é¢‘æ¬¡ä¸€è‡´
- [ ] å¢é‡åˆ†è¯åé¢‘æ¬¡æ­£ç¡®ç´¯åŠ 
- [ ] å•è¯å’ŒçŸ­è¯­ç»Ÿä¸€å¤„ç†

---

## ğŸš€ å®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šæ ¸å¿ƒå‡½æ•°ä¿®æ”¹ï¼ˆå¿…é¡»ï¼‰
1. ä¿®æ”¹ `utils/keyword_segmentation.py`
   - æ–°å¢ `segment_keywords_unified()`
   - ä¿®æ”¹ `segment_keywords_with_seed_tracking()` ä¸ºwrapper
2. ä¿®æ”¹ `ui/pages/phase0_expansion.py`
   - åˆ é™¤extract_ngrams checkbox
   - ç®€åŒ–UIä¸ºç»Ÿä¸€çš„min_frequencyé…ç½®
   - æ›´æ–°è°ƒç”¨é€»è¾‘

### ç¬¬äºŒé˜¶æ®µï¼šRepositoryä¼˜åŒ–ï¼ˆå»ºè®®ï¼‰
3. ä¿®æ”¹ `storage/word_segment_repository.py`
   - æ–°å¢ `load_all_tokens()` æ–¹æ³•
   - æ ‡è®° `load_segmentation_results()` ä¸ºdeprecated

### ç¬¬ä¸‰é˜¶æ®µï¼šè°ƒç”¨ç‚¹è¿ç§»ï¼ˆå¯é€‰ï¼‰
4. æ›´æ–° `ui/pages/phase2e_junyan.py`
5. æ›´æ–° `core/token_extraction.py`
6. æ›´æ–° `ui/pages/phase4_tokens.py`

### ç¬¬å››é˜¶æ®µï¼šæµ‹è¯•å’ŒéªŒè¯
7. å®Œæ•´æµç¨‹æµ‹è¯•
8. æ€§èƒ½æµ‹è¯•
9. æ•°æ®ä¸€è‡´æ€§éªŒè¯

---

**é¢„è®¡å·¥ä½œé‡**ï¼š2-3å°æ—¶
**å½±å“èŒƒå›´**ï¼šPhase 0 UI + æ ¸å¿ƒåˆ†è¯é€»è¾‘
**é£é™©ç­‰çº§**ï¼šä¸­ï¼ˆUIå˜åŒ–æ˜æ˜¾ï¼Œä½†é€»è¾‘æ”¹è¿›ï¼‰
