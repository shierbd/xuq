# è‹±æ–‡å…³é”®è¯éœ€æ±‚æŒ–æ˜ç³»ç»Ÿ - å¼€å‘ä¼˜åŒ–è®¡åˆ’ï¼ˆæœ€ç»ˆç‰ˆï¼‰

> **åŸºäºè¯æ®çš„ä¼˜åŒ–è·¯çº¿å›¾**
>
> åˆ›å»ºæ—¥æœŸï¼š2025-12-23
> ç‰ˆæœ¬ï¼šv1.0 æœ€ç»ˆç‰ˆ
> åŸºäºï¼šå›è¨€ç³»ç»Ÿæ–¹æ³•è®º + GPTåé¦ˆ + æŠ€æœ¯ä¿®æ­£ + æ‰¹åˆ¤æ€§åˆ†æ

---

## ğŸ“‹ è®¡åˆ’è¯´æ˜

### æ ¸å¿ƒä¾æ®

æœ¬è®¡åˆ’åŸºäºä»¥ä¸‹ä¸‰ä»½ææ–™çš„ç»¼åˆåˆ†æï¼š

1. **GPTåé¦ˆ**ï¼ˆ`docs/gptå›å¤.md`ï¼‰ï¼šæä¾›äº†Phase 0åŸºçº¿æµ‹é‡æ¡†æ¶å’Œæ•´ä½“ä¼˜åŒ–æ€è·¯
2. **å›è¨€ç³»ç»Ÿæºç åˆ†æ**ï¼ˆ`docs/å›è¨€ç³»ç»Ÿæºç åˆ†æ-è‹±æ–‡é€‚é…è¯„ä¼°.md`ï¼‰ï¼šæä¾›äº†å®ç°ç»†èŠ‚å’Œé€‚é…æ€§è¯„ä¼°
3. **æ‰¹åˆ¤æ€§åˆ†æ**ï¼šå¯¹GPTå»ºè®®è¿›è¡ŒæŠ€æœ¯ä¿®æ­£ï¼Œæ ‡æ³¨âœ…ï¼ˆé‡‡çº³ï¼‰ã€âŒï¼ˆä¿®æ­£ï¼‰ã€ğŸŸ¡ï¼ˆè°ƒæ•´ï¼‰

### æ–‡æ¡£ç›®çš„

- âœ… å»ºç«‹**è¯æ®é©±åŠ¨**çš„ä¼˜åŒ–è·¯çº¿ï¼Œé¿å…ç›²ç›®ä¼˜åŒ–
- âœ… æ˜ç¡®**è¯­è¨€é€‚é…**è¦æ±‚ï¼Œç¡®ä¿è‹±æ–‡è¯çº§å¤„ç†çš„æ­£ç¡®æ€§
- âœ… æä¾›**å¯æ‰§è¡Œ**çš„å®æ–½æ–¹æ¡ˆï¼ŒåŒ…å«å†³ç­–æ ‘å’ŒæˆåŠŸæ ‡å‡†
- âœ… ä¿æŒ**ç°å®æ—¶é—´çº¿**ï¼Œé¿å…è¿‡äºä¹è§‚çš„ä¼°ç®—

---

## ğŸ¯ æ ¸å¿ƒåŸåˆ™

### åŸåˆ™1ï¼šè¯æ®ä¼˜å…ˆï¼Œæµ‹é‡å…ˆè¡Œ

```
âŒ é”™è¯¯æ€ç»´ï¼šçœ‹åˆ°å›è¨€çš„åŠŸèƒ½ â†’ å‡è®¾æˆ‘ä»¬ä¹Ÿéœ€è¦ â†’ ç›´æ¥å®ç°
âœ… æ­£ç¡®æ€ç»´ï¼šåˆ†æå½“å‰é—®é¢˜ â†’ åŸºçº¿æµ‹é‡ â†’ æ ¹æ®æ•°æ®å†³ç­–

ç¤ºä¾‹ï¼š
- å›è¨€æœ‰"ç‰¹å¾ç‰‡æ®µæ˜ å°„"ï¼ˆå¤„ç†180ä¸‡ç°‡é—®é¢˜ï¼‰
- æˆ‘ä»¬æœ‰60-100ä¸ªç°‡
- â“ æ˜¯å¦éœ€è¦ç‰‡æ®µæ˜ å°„ï¼Ÿâ†’ å…ˆæµ‹é‡å®¡æ ¸æ—¶é—´å’Œé—æ¼ç‡
- å¦‚æœ<1å°æ—¶ä¸”é—æ¼<10% â†’ æš‚ä¸éœ€è¦
- å¦‚æœ>3å°æ—¶æˆ–é—æ¼>30% â†’ å†è€ƒè™‘
```

### åŸåˆ™2ï¼šè‹±æ–‡è¯çº§å¤„ç†ï¼Œéå­—ç¬¦çº§

```
æ ¸å¿ƒå·®å¼‚ï¼š
ä¸­æ–‡ï¼šå­—ç¬¦ = è¯­ä¹‰å•ä½
  "å›¾"(image) + "ç‰‡"(piece) + "å‹"(compress) + "ç¼©"(shrink)
  â†’ å­—ç¬¦æ’åºä¸ç ´åè¯­ä¹‰

è‹±æ–‡ï¼šè¯ = è¯­ä¹‰å•ä½
  "compress" = c+o+m+p+r+e+s+sï¼ˆå•ç‹¬å­—ç¬¦æ— æ„ä¹‰ï¼‰
  â†’ å¿…é¡»è¯çº§å¤„ç†

å®æ–½è¦æ±‚ï¼š
âœ… åˆ†è¯ â†’ è¯çº§å»åœç”¨è¯ â†’ è¯å½¢è¿˜åŸ â†’ è¯çº§æ’åº
âŒ å­—ç¬¦çº§æ’åºï¼ˆä¼šç ´åå•è¯ç»“æ„ï¼‰
```

### åŸåˆ™3ï¼šæŒ‰éœ€ä¼˜åŒ–ï¼Œä¸ä¸ºä¼˜åŒ–è€Œä¼˜åŒ–

```
å†³ç­–æ ‡å‡†ï¼š
1. å½“å‰æœ‰æ˜ç¡®ç—›ç‚¹ï¼Ÿï¼ˆå®¡æ ¸ç´¯ã€è¯ä¸å¤Ÿã€å†—ä½™å¤šï¼‰
2. æœ‰æ•°æ®æ”¯æŒï¼Ÿï¼ˆæµ‹é‡æ˜¾ç¤ºé—®é¢˜ä¸¥é‡ï¼‰
3. æŠ•å…¥äº§å‡ºæ¯”ï¼Ÿï¼ˆå·¥ä½œé‡ vs å®é™…ä»·å€¼ï¼‰

åªæœ‰ä¸‰è€…éƒ½æ»¡è¶³ï¼Œæ‰è¿›å…¥å®æ–½é˜¶æ®µ
```

### åŸåˆ™4ï¼šä¿æŒMVPä¼˜åŠ¿ï¼Œå¢å¼ºä¸é‡æ„

```
å½“å‰ç³»ç»Ÿä¼˜åŠ¿ï¼ˆä¸è¦ç ´åï¼‰ï¼š
âœ… HDBSCANèšç±»æˆç†Ÿç¨³å®š
âœ… LLMè‡ªåŠ¨åŒ–ç¨‹åº¦é«˜
âœ… Streamlit UIå‹å¥½
âœ… é€‚ç”¨5-10ä¸‡è§„æ¨¡

å®æ–½ç­–ç•¥ï¼š
åœ¨ç°æœ‰æ¶æ„ä¸Šå¢åŠ æ¨¡å—ï¼Œè€Œéæ¨å€’é‡æ¥
```

---

## Phase 0: åŸºçº¿æµ‹é‡ï¼ˆ1å‘¨ï¼‰â­â­â­â­â­

### ç›®æ ‡

å»ºç«‹è¯æ®åŸºç¡€ï¼Œè¯†åˆ«çœŸå®é—®é¢˜ï¼Œä¸ºæ‰€æœ‰åç»­ä¼˜åŒ–æä¾›å†³ç­–ä¾æ®ã€‚

**é‡è¦æ€§**ï¼šæ²¡æœ‰è¿™ä¸ªåŸºçº¿ï¼Œæ‰€æœ‰ä¼˜åŒ–éƒ½æ˜¯ç›²ç›®çš„ï¼

### Experiment A: èšç±»å®¡æ ¸æˆæœ¬æµ‹é‡

#### ç›®çš„
å›ç­”é—®é¢˜ï¼šå½“å‰60-100ä¸ªç°‡çš„äººå·¥ç­›é€‰ï¼ˆé€‰å‡º10-15ä¸ªé‡ç‚¹ç°‡ï¼‰æ˜¯å¦æ˜¯ç“¶é¢ˆï¼Ÿ

#### æ“ä½œæµç¨‹

```
1. å‡†å¤‡æ•°æ®ï¼š
   é€‰æ‹©ä¸€æ‰¹æ–°çš„çœŸå®è‹±æ–‡çŸ­è¯­ï¼ˆ5-10ä¸‡æ¡ï¼‰

2. è¿è¡Œå½“å‰Phase 2å¤§ç»„èšç±»ï¼š
   python scripts/run_phase2_clustering.py

3. è®°å½•ç°‡çš„æ•°é‡ï¼š
   ä¾‹å¦‚ï¼šç”Ÿæˆäº†85ä¸ªç°‡

4. æ­£å¸¸å·¥ä½œæ–¹å¼ç­›é€‰ï¼š
   - ä»è¿™85ä¸ªç°‡ä¸­é€‰å‡ºä½ è®¤ä¸º"æœ€å€¼å¾—æ·±æŒ–"çš„10-15ä¸ª
   - è®¡æ—¶ï¼šä»å¼€å§‹æŸ¥çœ‹ç°‡åˆ—è¡¨åˆ°æœ€ç»ˆç¡®å®šé€‰æ‹©ï¼ŒèŠ±äº†å¤šä¹…
   - è®°å½•ç­›é€‰è¿‡ç¨‹ï¼š
     * å“ªäº›ç°‡è®©ä½ çŠ¹è±«ï¼Ÿ
     * æ˜¯å¦æœ‰"åº”è¯¥é€‰ä½†æ¼æ‰"çš„ç°‡ï¼Ÿ
     * ä¸»è§‚æ„Ÿå—ï¼šè½»æ¾/ä¸€èˆ¬/å¾ˆç´¯

5. ä¸€å‘¨åå›æº¯éªŒè¯ï¼š
   - é‡æ–°å®¡è§†å½“åˆæœªé€‰çš„ç°‡
   - è®°å½•"é—æ¼çš„å¥½ç°‡"æ•°é‡
```

#### åˆ¤æ–­æ ‡å‡†

```
âœ… æš‚ä¸éœ€è¦ä¼˜åŒ–ï¼ˆä¼˜å…ˆçº§ä½ï¼‰ï¼š
  - å®¡æ ¸æ—¶é—´ < 60åˆ†é’Ÿ
  - é—æ¼ç‡ < 10%
  - ä¸»è§‚æ„Ÿå—ï¼šè¿˜è¡Œï¼Œä¸å¤ªç´¯

ğŸŸ¡ å¯ä»¥è€ƒè™‘ä¼˜åŒ–ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰ï¼š
  - å®¡æ ¸æ—¶é—´ï¼š60-120åˆ†é’Ÿ
  - é—æ¼ç‡ï¼š10-30%
  - ä¸»è§‚æ„Ÿå—ï¼šæœ‰ç‚¹ç´¯ï¼Œä½†å¯æ¥å—

âŒ å¿…é¡»ä¼˜åŒ–ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰ï¼š
  - å®¡æ ¸æ—¶é—´ > 120åˆ†é’Ÿ
  - é—æ¼ç‡ > 30%
  - ä¸»è§‚æ„Ÿå—ï¼šå¾ˆç´¯ï¼Œç»å¸¸æ¼é€‰

å¦‚æœåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡ï¼š
  â†’ Phase 1.2"èšç±»è´¨é‡è¯„åˆ†è¾…åŠ©"ä¼˜å…ˆçº§æå‡
```

#### è®°å½•è¡¨æ ¼

```markdown
| ç»´åº¦ | æµ‹é‡ç»“æœ | åˆ¤å®š |
|------|---------|------|
| æ€»ç°‡æ•° | XXä¸ª | - |
| å®¡æ ¸æ—¶é—´ | XXåˆ†é’Ÿ | âœ…/ğŸŸ¡/âŒ |
| æœ€ç»ˆé€‰æ‹© | XXä¸ªç°‡ | - |
| é—æ¼æ•°é‡ | XXä¸ª | âœ…/ğŸŸ¡/âŒ |
| ä¸»è§‚æ„Ÿå— | è½»æ¾/ä¸€èˆ¬/å¾ˆç´¯ | âœ…/ğŸŸ¡/âŒ |
| **ç»¼åˆåˆ¤å®š** | - | **âœ…/ğŸŸ¡/âŒ** |
```

---

### Experiment B: Tokenï¼ˆç‰¹å¾è¯ï¼‰è¦†ç›–ç‡æµ‹é‡

#### ç›®çš„
å›ç­”é—®é¢˜ï¼šå½“å‰çš„26ä¸ªtokenå¤Ÿç”¨å—ï¼Ÿæ˜¯å¦éœ€è¦æ‰©å±•è¯åº“ï¼Ÿ

#### æ“ä½œæµç¨‹

```
1. ç»Ÿè®¡å½“å‰tokenåº“ï¼š
   SELECT * FROM tokens;
   â†’ ç¡®è®¤å½“å‰æœ‰26ä¸ªtoken

2. è®¡ç®—è¦†ç›–ç‡ï¼š
   å¯¹æ‰€æœ‰çŸ­è¯­ï¼Œæ£€æŸ¥æ˜¯å¦å‘½ä¸­è‡³å°‘1ä¸ªtoken

   SQLç¤ºä¾‹ï¼š
   -- å‘½ä¸­è‡³å°‘1ä¸ªtokençš„çŸ­è¯­æ•°
   SELECT COUNT(DISTINCT p.phrase_id)
   FROM phrases p
   JOIN phrase_tokens pt ON p.phrase_id = pt.phrase_id;

   -- æ€»çŸ­è¯­æ•°
   SELECT COUNT(*) FROM phrases;

   -- è¦†ç›–ç‡
   coverage_rate = å‘½ä¸­çŸ­è¯­æ•° / æ€»çŸ­è¯­æ•°

3. æŸ¥çœ‹æœªè¦†ç›–çŸ­è¯­æ ·æœ¬ï¼š
   -- æœªè¢«ä»»ä½•tokenè¦†ç›–çš„çŸ­è¯­
   SELECT phrase_text
   FROM phrases
   WHERE phrase_id NOT IN (
       SELECT phrase_id FROM phrase_tokens
   )
   LIMIT 100;

   äººå·¥æŸ¥çœ‹è¿™100ä¸ªæ ·æœ¬ï¼š
   - æ˜¯å¦æœ‰è§„å¾‹ï¼Ÿ
   - æ˜¯å¦ç¼ºå°‘æŸç±»tokenï¼Ÿ
   - å“ªäº›å…³é”®è¯åº”è¯¥è¢«æå–ä½†æ²¡æœ‰ï¼Ÿ

4. åˆ†ç±»ç»Ÿè®¡ï¼š
   å½“å‰26ä¸ªtokençš„ç±»åˆ«åˆ†å¸ƒï¼š
   - intentï¼ˆæ„å›¾è¯ï¼‰ï¼šXXä¸ª
   - actionï¼ˆåŠ¨ä½œè¯ï¼‰ï¼šXXä¸ª
   - objectï¼ˆå¯¹è±¡è¯ï¼‰ï¼šXXä¸ª
   - otherï¼šXXä¸ª
```

#### åˆ¤æ–­æ ‡å‡†

```
âœ… Tokenå¤Ÿç”¨ï¼ˆæš‚ä¸æ‰©å±•ï¼‰ï¼š
  - è¦†ç›–ç‡ â‰¥ 80%
  - æœªè¦†ç›–çŸ­è¯­å¤§å¤šæ˜¯å™ªå£°æˆ–æé•¿å°¾
  - ä¸»è§‚åˆ¤æ–­ï¼šå½“å‰tokenèƒ½æè¿°å¤§éƒ¨åˆ†éœ€æ±‚

ğŸŸ¡ Tokenå‹‰å¼ºå¤Ÿç”¨ï¼ˆå¯é€‰æ‰©å±•ï¼‰ï¼š
  - è¦†ç›–ç‡ï¼š60-80%
  - æœªè¦†ç›–çŸ­è¯­ä¸­æœ‰ä¸€äº›é«˜è´¨é‡éœ€æ±‚
  - ä¸»è§‚åˆ¤æ–­ï¼šå¶å°”æ„Ÿè§‰"è¯ä¸å¤Ÿç”¨"

âŒ Tokenä¸å¤Ÿç”¨ï¼ˆå¿…é¡»æ‰©å±•ï¼‰ï¼š
  - è¦†ç›–ç‡ < 60%
  - å¤§é‡é«˜è´¨é‡çŸ­è¯­æ— æ³•ç”¨tokenæè¿°
  - ä¸»è§‚åˆ¤æ–­ï¼šç»å¸¸æ„Ÿè§‰"æ— è¯å¯æŒ‚"

å¦‚æœåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡ï¼š
  â†’ Phase 2.2"æ¨¡æ¿-å˜é‡è¿­ä»£æå–"ä¼˜å…ˆçº§æå‡
```

#### è®°å½•è¡¨æ ¼

```markdown
| ç»´åº¦ | æµ‹é‡ç»“æœ | åˆ¤å®š |
|------|---------|------|
| å½“å‰tokenæ•° | 26ä¸ª | - |
| æ€»çŸ­è¯­æ•° | XXæ¡ | - |
| å‘½ä¸­çŸ­è¯­æ•° | XXæ¡ | - |
| è¦†ç›–ç‡ | XX% | âœ…/ğŸŸ¡/âŒ |
| æœªè¦†ç›–çŸ­è¯­è´¨é‡ | é«˜/ä¸­/ä½ | - |
| **ç»¼åˆåˆ¤å®š** | - | **âœ…/ğŸŸ¡/âŒ** |
```

---

### Experiment C: æ•°æ®å†—ä½™ç‡æµ‹é‡

#### ç›®çš„
å›ç­”é—®é¢˜ï¼šæ˜¯å¦æœ‰å¤§é‡"åŒä¸€éœ€æ±‚ï¼Œä¸åŒè¡¨è¾¾"çš„å†—ä½™æ•°æ®ï¼Ÿ

#### æ“ä½œæµç¨‹

```
1. éšæœºæŠ½æ ·ï¼š
   SELECT phrase_text
   FROM phrases
   ORDER BY RANDOM()
   LIMIT 1000;

2. äººå·¥é…å¯¹ï¼š
   é€æ¡æŸ¥çœ‹ï¼Œè¯†åˆ«"è¡¨è¾¾ç›¸åŒéœ€æ±‚"çš„çŸ­è¯­ç»„

   å†—ä½™ç¤ºä¾‹ï¼ˆåº”é…å¯¹ï¼‰ï¼š
   - "best calculator for students"
   - "calculator best for students"
   - "students best calculator"
   â†’ 3ä¸ªè¡¨è¾¾ â†’ 1ä¸ªéœ€æ±‚ï¼ˆå†—ä½™ç»„ï¼‰

   éå†—ä½™ç¤ºä¾‹ï¼ˆä¸é…å¯¹ï¼‰ï¼š
   - "best calculator" ï¼ˆæ¨èæ„å›¾ï¼‰
   - "free calculator" ï¼ˆå…è´¹æ„å›¾ï¼‰
   â†’ 2ä¸ªä¸åŒéœ€æ±‚

   æ³¨æ„äº‹é¡¹ï¼š
   - è¯åºä¸åŒä½†æ„å›¾ç›¸åŒ â†’ å†—ä½™
   - åœç”¨è¯å·®å¼‚ï¼ˆfor/at/inï¼‰ â†’ å†—ä½™
   - æ ¸å¿ƒæ„å›¾è¯ä¸åŒï¼ˆbest/freeï¼‰ â†’ éå†—ä½™
   - å¯¹è±¡ä¸åŒï¼ˆimage/videoï¼‰ â†’ éå†—ä½™

3. ç»Ÿè®¡å†—ä½™ç‡ï¼š
   å†—ä½™ç‡ = è½åœ¨å†—ä½™ç»„çš„çŸ­è¯­æ•° / æŠ½æ ·æ€»æ•°

   ç¤ºä¾‹ï¼š
   - æŠ½æ ·1000æ¡
   - è¯†åˆ«å‡º120ä¸ªå†—ä½™ç»„ï¼ˆå…±280æ¡çŸ­è¯­ï¼‰
   - å†—ä½™ç‡ = 280 / 1000 = 28%

4. è®°å½•å…¸å‹å†—ä½™æ¨¡å¼ï¼š
   æœ€å¸¸è§çš„å†—ä½™ç±»å‹ï¼š
   - è¯åºé¢ å€’ï¼šXXç»„
   - åœç”¨è¯å·®å¼‚ï¼šXXç»„
   - è¯å½¢å˜åŒ–ï¼šXXç»„ï¼ˆcompressor/compressï¼‰
```

#### åˆ¤æ–­æ ‡å‡†

```
âœ… å†—ä½™ä¸ä¸¥é‡ï¼ˆæš‚ä¸å»é‡ï¼‰ï¼š
  - å†—ä½™ç‡ < 10%
  - å¤§éƒ¨åˆ†çŸ­è¯­è¡¨è¾¾ç‹¬ç‰¹éœ€æ±‚
  - å»é‡å¸¦æ¥çš„æ”¶ç›Š < å®æ–½æˆæœ¬

ğŸŸ¡ å†—ä½™ä¸­ç­‰ï¼ˆå¯é€‰å»é‡ï¼‰ï¼š
  - å†—ä½™ç‡ï¼š10-20%
  - æœ‰ä¸€å®šå»é‡ç©ºé—´
  - å®æ–½åå¯èƒ½æå‡5-10%æ•ˆç‡

âŒ å†—ä½™ä¸¥é‡ï¼ˆå¿…é¡»å»é‡ï¼‰ï¼š
  - å†—ä½™ç‡ > 20%
  - å¤§é‡éœ€æ±‚è¢«å¤šç§è¡¨è¾¾ç¨€é‡Š
  - å»é‡èƒ½æ˜¾è‘—æå‡æ•°æ®è´¨é‡

å¦‚æœåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡ï¼š
  â†’ Phase 2.1"è¯çº§è§„èŒƒåŒ–å»é‡"ä¼˜å…ˆçº§æå‡
```

#### è®°å½•è¡¨æ ¼

```markdown
| ç»´åº¦ | æµ‹é‡ç»“æœ | åˆ¤å®š |
|------|---------|------|
| æŠ½æ ·æ•°é‡ | 1000æ¡ | - |
| å†—ä½™ç»„æ•°é‡ | XXç»„ | - |
| å†—ä½™çŸ­è¯­æ•° | XXæ¡ | - |
| å†—ä½™ç‡ | XX% | âœ…/ğŸŸ¡/âŒ |
| ä¸»è¦å†—ä½™ç±»å‹ | è¯åº/åœç”¨è¯/è¯å½¢ | - |
| **ç»¼åˆåˆ¤å®š** | - | **âœ…/ğŸŸ¡/âŒ** |
```

---

### Experiment D: æœç´¢æ„å›¾åˆ†å¸ƒç»Ÿè®¡

#### ç›®çš„
å›ç­”é—®é¢˜ï¼šè‹±æ–‡å…³é”®è¯çš„æœç´¢æ„å›¾åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Ÿæ˜¯å¦å’Œå›è¨€çš„"å¯»æ‰¾ç±»å 95%"ç±»ä¼¼ï¼Ÿ

#### æ“ä½œæµç¨‹

```
1. éšæœºæŠ½æ ·ï¼š
   SELECT phrase_text
   FROM phrases
   ORDER BY RANDOM()
   LIMIT 1000;

2. å®šä¹‰æ„å›¾åˆ†ç±»ï¼ˆå‚è€ƒå›è¨€+è‹±æ–‡ç‰¹ç‚¹ï¼‰ï¼š

   find_toolï¼ˆå¯»æ‰¾å·¥å…·/äº§å“ï¼‰ï¼š
   - å…³é”®è¯ï¼šbest, top, recommend, popular, tool, software
   - ç¤ºä¾‹ï¼šbest image compressor, top photo editor

   compareï¼ˆå¯¹æ¯”/æ›¿ä»£ï¼‰ï¼š
   - å…³é”®è¯ï¼švs, versus, compare, difference, alternative
   - ç¤ºä¾‹ï¼šphotoshop vs gimp, slack alternative

   learn_howï¼ˆå­¦ä¹ /æ•™ç¨‹ï¼‰ï¼š
   - å…³é”®è¯ï¼šhow to, tutorial, guide, learn, step
   - ç¤ºä¾‹ï¼šhow to compress image, excel tutorial

   solve_problemï¼ˆè§£å†³é—®é¢˜/æ•…éšœï¼‰ï¼š
   - å…³é”®è¯ï¼šfix, error, not working, problem, issue
   - ç¤ºä¾‹ï¼šchrome not working, fix wifi

   find_freeï¼ˆå¯»æ‰¾å…è´¹ï¼‰ï¼š
   - å…³é”®è¯ï¼šfree, open source, no cost
   - ç¤ºä¾‹ï¼šfree video editor, open source CRM

   otherï¼ˆå…¶ä»–ï¼‰ï¼š
   - ä¸å±äºä»¥ä¸Šä»»ä½•ç±»åˆ«

3. äººå·¥æ ‡æ³¨ï¼š
   é€æ¡æ ‡æ³¨1000æ¡çŸ­è¯­çš„æ„å›¾ç±»åˆ«
   è®°å½•æ ‡æ³¨æ—¶é—´ï¼ˆäº†è§£å·¥ä½œé‡ï¼‰

4. ç»Ÿè®¡åˆ†å¸ƒï¼š
   find_tool: XX% (XXæ¡)
   compare: XX% (XXæ¡)
   learn_how: XX% (XXæ¡)
   solve_problem: XX% (XXæ¡)
   find_free: XX% (XXæ¡)
   other: XX% (XXæ¡)

5. å¯¹æ¯”å›è¨€å‘ç°ï¼š
   å›è¨€ï¼ˆä¸­æ–‡è½¯ä»¶ï¼‰ï¼šå¯»æ‰¾ç±» 95%+
   æˆ‘ä»¬ï¼ˆè‹±æ–‡äº§å“ï¼‰ï¼šfind_tool + compare + find_free = XX%
```

#### åˆ¤æ–­æ ‡å‡†

```
âœ… å¯»æ‰¾ç±»å ä¸»å¯¼ï¼ˆç±»ä¼¼å›è¨€ï¼‰ï¼š
  - find_tool + compare + find_free > 70%
  - è¯´æ˜è‹±æ–‡åœºæ™¯ä¹Ÿæ˜¯"å¯»æ‰¾"ä¸ºä¸»
  - å¯ä»¥é‡‡ç”¨å›è¨€çš„æ„å›¾åˆ†ç±»æ¡†æ¶

ğŸŸ¡ åˆ†å¸ƒç›¸å¯¹å‡åŒ€ï¼š
  - å„æ„å›¾å æ¯”éƒ½åœ¨10-30%ä¹‹é—´
  - è¯´æ˜è‹±æ–‡åœºæ™¯æ›´å¤šæ ·åŒ–
  - éœ€è¦æ›´ç»†åŒ–çš„æ„å›¾ç»´åº¦

âŒ å…¶ä»–ç±»å ä¸»å¯¼ï¼š
  - other > 40%
  - è¯´æ˜å½“å‰åˆ†ç±»ä½“ç³»ä¸é€‚ç”¨
  - éœ€è¦é‡æ–°è®¾è®¡åˆ†ç±»

å¦‚æœåˆ¤å®šä¸ºâœ…ï¼š
  â†’ Phase 2.3"æœç´¢æ„å›¾åˆ†ç±»"ä¼˜å…ˆçº§æå‡
  â†’ é‡‡ç”¨å›è¨€å¼çš„æ„å›¾é©±åŠ¨ç­–ç•¥
```

#### è®°å½•è¡¨æ ¼

```markdown
| æ„å›¾ç±»åˆ« | æ•°é‡ | å æ¯” | å…¸å‹ç¤ºä¾‹ |
|---------|------|------|---------|
| find_tool | XXæ¡ | XX% | best calculator |
| compare | XXæ¡ | XX% | excel vs sheets |
| learn_how | XXæ¡ | XX% | how to resize image |
| solve_problem | XXæ¡ | XX% | zoom not working |
| find_free | XXæ¡ | XX% | free vpn |
| other | XXæ¡ | XX% | - |
| **æ€»è®¡** | 1000æ¡ | 100% | - |

ç»¼åˆåˆ¤å®šï¼šâœ…/ğŸŸ¡/âŒ
```

---

### Phase 0 äº§å‡ºï¼šåŸºçº¿æŠ¥å‘Š

#### æ–‡æ¡£åç§°
`docs/è‹±æ–‡èšç±»ç³»ç»ŸåŸºçº¿æŠ¥å‘Š-2025-12.md`

#### æŠ¥å‘Šç»“æ„

```markdown
# è‹±æ–‡å…³é”®è¯éœ€æ±‚æŒ–æ˜ç³»ç»Ÿ - åŸºçº¿æµ‹é‡æŠ¥å‘Š

## æµ‹é‡æ—¥æœŸ
2025-12-XX

## æ•°æ®è§„æ¨¡
- æ€»çŸ­è¯­æ•°ï¼šXX,XXXæ¡
- èšç±»ç°‡æ•°ï¼šXXä¸ª
- Tokenæ•°ï¼š26ä¸ª

## Experiment A: èšç±»å®¡æ ¸æˆæœ¬
- å®¡æ ¸æ—¶é—´ï¼šXXåˆ†é’Ÿ
- é—æ¼ç‡ï¼šXX%
- ä¸»è§‚æ„Ÿå—ï¼šè½»æ¾/ä¸€èˆ¬/å¾ˆç´¯
- **åˆ¤å®š**ï¼šâœ…/ğŸŸ¡/âŒ

## Experiment B: Tokenè¦†ç›–ç‡
- è¦†ç›–ç‡ï¼šXX%
- æœªè¦†ç›–çŸ­è¯­è´¨é‡ï¼šé«˜/ä¸­/ä½
- **åˆ¤å®š**ï¼šâœ…/ğŸŸ¡/âŒ

## Experiment C: æ•°æ®å†—ä½™ç‡
- å†—ä½™ç‡ï¼šXX%
- ä¸»è¦å†—ä½™ç±»å‹ï¼šè¯åº/åœç”¨è¯/è¯å½¢
- **åˆ¤å®š**ï¼šâœ…/ğŸŸ¡/âŒ

## Experiment D: æœç´¢æ„å›¾åˆ†å¸ƒ
- å¯»æ‰¾ç±»å æ¯”ï¼šXX%
- åˆ†å¸ƒç‰¹ç‚¹ï¼šé›†ä¸­/å‡åŒ€/åˆ†æ•£
- **åˆ¤å®š**ï¼šâœ…/ğŸŸ¡/âŒ

## å†³ç­–æ ‘ï¼šåç»­ä¼˜åŒ–æ–¹å‘

åŸºäºä»¥ä¸Šæµ‹é‡ç»“æœï¼Œå»ºè®®çš„ä¼˜åŒ–ä¼˜å…ˆçº§ï¼š

### é«˜ä¼˜å…ˆçº§ï¼ˆå¿…åšï¼‰ï¼š
[ ] Phase 1.1: éœ€æ±‚å¡ç‰‡ç»“æ„ç»Ÿä¸€ï¼ˆåŸºç¡€è®¾æ–½ï¼‰

### ä¸­ä¼˜å…ˆçº§ï¼ˆæ ¹æ®æµ‹é‡ç»“æœï¼‰ï¼š
[ ] Phase 1.2: èšç±»è´¨é‡è¯„åˆ†ï¼ˆå¦‚æœExp Aåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡ï¼‰
[ ] Phase 2.1: è¯çº§è§„èŒƒåŒ–å»é‡ï¼ˆå¦‚æœExp Cåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡ï¼‰
[ ] Phase 2.2: æ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆå¦‚æœExp Båˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡ï¼‰
[ ] Phase 2.3: æœç´¢æ„å›¾åˆ†ç±»ï¼ˆå¦‚æœExp Dåˆ¤å®šä¸ºâœ…ï¼‰

### ä½ä¼˜å…ˆçº§ï¼ˆæš‚ä¸å®æ–½ï¼‰ï¼š
[ ] Phase 2.4: è¯çº§N-gramåˆ†æï¼ˆå½“å‰è§„æ¨¡ä¸éœ€è¦ï¼‰

## 3ä¸ªæœˆä¼˜åŒ–è®¡åˆ’

æ ¹æ®å†³ç­–æ ‘ï¼Œæ¥ä¸‹æ¥3ä¸ªæœˆæœ€å€¼å¾—åšçš„æ˜¯ï¼š
1. XXXï¼ˆåŸå› ï¼šXXXï¼‰
2. XXXï¼ˆåŸå› ï¼šXXXï¼‰

æš‚ä¸åšçš„ä¼˜åŒ–ï¼š
- XXXï¼ˆåŸå› ï¼šå½“å‰ä¸æ˜¯ç—›ç‚¹ï¼‰
```

---

## Phase 1: åŸºç¡€è®¾æ–½ï¼ˆ2-3å‘¨ï¼‰

è¿™ä¸€éƒ¨åˆ†æ˜¯**ä¸ç®¡Phase 0ç»“æœå¦‚ä½•éƒ½å€¼å¾—åšçš„åŸºç¡€è®¾æ–½**ï¼Œå› ä¸ºå®ƒä»¬åªæ˜¯åœ¨ç°æœ‰æ¶æ„å¤–åŠ è¾…åŠ©åŠŸèƒ½ï¼Œä¸åŠ¨åº•å±‚é€»è¾‘ï¼Œé£é™©ä½æ”¶ç›Šç¨³ã€‚

---

### 1.1 éœ€æ±‚å¡ç‰‡ç»“æ„ç»Ÿä¸€ âœ… å¿…åš

#### ç›®æ ‡
æ— è®ºåç»­è¦ä¸è¦åšæ¨¡æ¿-å˜é‡è¿­ä»£ï¼Œéœ€æ±‚å¡ç‰‡æœ¬èº«çš„ç»“æ„åŒ–è‚¯å®šæœ‰ä»·å€¼ã€‚

#### å½“å‰é—®é¢˜
```
ç°çŠ¶ï¼š
- éœ€æ±‚å¡ç‰‡æœ‰demand_typeï¼ˆtool/content/service/educationï¼‰
- ä½†ç¼ºå°‘æ›´ç»†ç²’åº¦çš„ç»“æ„åŒ–å­—æ®µ
- æ— æ³•æ–¹ä¾¿åœ°ç­›é€‰"é«˜ä»·å€¼ + è§†é¢‘é¢†åŸŸ + å…è´¹ç±»"çš„éœ€æ±‚
```

#### æ–°å¢å¡ç‰‡ç»“æ„ï¼ˆå‚è€ƒå›è¨€+GPTå»ºè®®ï¼‰

```python
# æ‰©å±•æ•°æ®æ¨¡å‹ï¼šstorage/models.py

class Demand:
    """
    éœ€æ±‚å¡ç‰‡ï¼ˆæ‰©å±•ç‰ˆï¼‰
    """
    # === ç°æœ‰å­—æ®µ ===
    demand_id: int
    summary: str  # ä¸€å¥è¯éœ€æ±‚
    demand_type: str  # tool/content/service/education/other
    cluster_id: int

    # === æ–°å¢å­—æ®µï¼šäº”ç±»è¯ç»“æ„ï¼ˆå‚è€ƒå›è¨€ï¼‰ ===
    intent_words: List[str]  # æ„å›¾æ¡†æ¶è¯ï¼šbest, free, top, compare
    core_need_words: List[str]  # æ ¸å¿ƒéœ€æ±‚è¯ï¼šcompress, resize, convert
    domain_words: List[str]  # é¢†åŸŸ/å¯¹è±¡è¯ï¼šimage, video, pdf
    result_words: List[str]  # æœŸæœ›ç»“æœ/å±æ€§ï¼šfast, high quality, simple
    constraint_words: List[str]  # æ¡ä»¶/é™åˆ¶ï¼šfor students, online, free

    # === æ–°å¢å­—æ®µï¼šæœç´¢æ„å›¾åˆ†ç±» ===
    search_intent: str  # find_tool/compare/learn_how/solve_problem/find_free/other
    search_intent_confidence: str  # high/medium/low

    # === æ–°å¢å­—æ®µï¼šä»·å€¼è¯„åˆ† ===
    value_score: str  # high/medium/lowï¼ˆå•†ä¸šä»·å€¼ï¼‰
    feasibility_score: str  # easy/medium/hardï¼ˆå¯è¡Œæ€§ï¼‰

    # === å…ƒæ•°æ® ===
    created_at: datetime
    updated_at: datetime
```

#### æ•°æ®åº“è¿ç§»

```sql
-- migrations/add_demand_structure.sql

-- æ–°å¢äº”ç±»è¯å­—æ®µï¼ˆJSONæ ¼å¼å­˜å‚¨ï¼‰
ALTER TABLE demands ADD COLUMN intent_words JSON;
ALTER TABLE demands ADD COLUMN core_need_words JSON;
ALTER TABLE demands ADD COLUMN domain_words JSON;
ALTER TABLE demands ADD COLUMN result_words JSON;
ALTER TABLE demands ADD COLUMN constraint_words JSON;

-- æ–°å¢æœç´¢æ„å›¾å­—æ®µ
ALTER TABLE demands ADD COLUMN search_intent VARCHAR(50);
ALTER TABLE demands ADD COLUMN search_intent_confidence VARCHAR(20);

-- æ–°å¢è¯„åˆ†å­—æ®µ
ALTER TABLE demands ADD COLUMN value_score VARCHAR(20);
ALTER TABLE demands ADD COLUMN feasibility_score VARCHAR(20);

-- åˆ›å»ºç´¢å¼•ï¼ˆæ–¹ä¾¿ç­›é€‰ï¼‰
CREATE INDEX idx_search_intent ON demands(search_intent);
CREATE INDEX idx_value_score ON demands(value_score);
```

#### LLMæå–äº”ç±»è¯çš„Promptè®¾è®¡

```python
# ai/prompts.py

FIVE_WORDS_EXTRACTION_PROMPT = """
You are analyzing a product demand for keyword research.

Demand Summary: "{demand_summary}"
Related Keywords: {related_keywords}

Please extract and classify words into 5 categories:

1. **Intent Words** (æ„å›¾æ¡†æ¶è¯):
   Words indicating user intent, such as: best, top, free, compare, vs, how to, alternative, etc.

2. **Core Need Words** (æ ¸å¿ƒéœ€æ±‚è¯):
   Action/function words indicating what the user wants to do, such as: compress, resize, convert, edit, analyze, etc.

3. **Domain Words** (é¢†åŸŸ/å¯¹è±¡è¯):
   Objects or domains being processed, such as: image, video, pdf, text, data, etc.

4. **Result Words** (æœŸæœ›ç»“æœ/å±æ€§è¯):
   Desired qualities or outcomes, such as: fast, high quality, simple, professional, etc.

5. **Constraint Words** (æ¡ä»¶/é™åˆ¶è¯):
   Constraints or scenarios, such as: for students, online, offline, free, paid, etc.

Return in JSON format:
{{
  "intent_words": ["best", "free"],
  "core_need_words": ["compress"],
  "domain_words": ["image"],
  "result_words": ["fast", "high quality"],
  "constraint_words": ["online", "for students"]
}}

Only return the JSON, no explanation.
"""

SEARCH_INTENT_CLASSIFICATION_PROMPT = """
Classify the search intent of this keyword demand.

Demand: "{demand_summary}"

Intent Categories:
- find_tool: Looking for a tool/product/service (e.g., best calculator, top photo editor)
- compare: Comparing options or seeking alternatives (e.g., slack vs teams, photoshop alternative)
- learn_how: Learning how to do something (e.g., how to compress image, excel tutorial)
- solve_problem: Fixing an issue (e.g., zoom not working, fix wifi)
- find_free: Looking for free options (e.g., free vpn, open source CRM)
- other: None of the above

Return in JSON format:
{{
  "search_intent": "find_tool",
  "confidence": "high"
}}

Only return the JSON, no explanation.
"""
```

#### å®æ–½æ­¥éª¤

```
Week 1-2: æ•°æ®æ¨¡å‹ä¸LLMæå–
  1. ä¿®æ”¹æ•°æ®æ¨¡å‹ï¼šstorage/models.py
  2. æ•°æ®åº“è¿ç§»ï¼šæ·»åŠ æ–°å­—æ®µ
  3. å®ç°LLMæå–é€»è¾‘ï¼šai/five_words_extractor.py
  4. æ‰¹é‡å¤„ç†ç°æœ‰éœ€æ±‚ï¼špython scripts/enrich_demands.py
  5. éªŒè¯æå–è´¨é‡ï¼šäººå·¥æŠ½æ ·100æ¡

Week 2-3: UIå±•ç¤ºä¸ç­›é€‰
  1. UIæ˜¾ç¤ºäº”ç±»è¯ï¼šui/demand_viewer.py
  2. å¢åŠ ç­›é€‰å™¨ï¼šæŒ‰æ„å›¾ã€å¯¹è±¡ã€çº¦æŸç­›é€‰
  3. å¢åŠ è¯„åˆ†ç•Œé¢ï¼šäººå·¥æ‰“åˆ†value_scoreå’Œfeasibility_score
  4. æµ‹è¯•å®Œæ•´æµç¨‹
```

#### æˆåŠŸæ ‡å‡†

```
âœ… æ‰€æœ‰éœ€æ±‚å¡ç‰‡éƒ½åŒ…å«äº”ç±»è¯ç»“æ„
âœ… LLMæå–å‡†ç¡®ç‡ > 80%ï¼ˆäººå·¥æŠ½æ ·100æ¡éªŒè¯ï¼‰
âœ… UIèƒ½æŒ‰äº”ç±»è¯ç­›é€‰éœ€æ±‚
âœ… èƒ½å¿«é€Ÿæ‰¾åˆ°"é«˜ä»·å€¼ + å¯è¡Œ + ç‰¹å®šé¢†åŸŸ"çš„éœ€æ±‚ç»„åˆ
```

---

### 1.2 èšç±»è´¨é‡è¯„åˆ†è¾…åŠ© ğŸŸ¡ æ¡ä»¶æ‰§è¡Œ

#### å‰ç½®æ¡ä»¶
ä»…å½“**Experiment Aåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡**æ—¶æ‰å®æ–½ï¼ˆå³èšç±»å®¡æ ¸ç¡®å®è´¹æ—¶è´¹åŠ›ï¼‰

#### ç›®æ ‡
è®©ä½ åœ¨çœ‹ç°‡åˆ—è¡¨æ—¶ï¼Œå…ˆçœ‹åˆ°"æœ€æœ‰æ½œåŠ›çš„é‚£æ‰¹"ï¼Œè€Œä¸æ˜¯ä»å¤´ç¿»åˆ°å°¾ã€‚

#### å½“å‰é—®é¢˜
```
ç°çŠ¶ï¼š
- 60-100ä¸ªç°‡ï¼Œæ— ä»»ä½•æ’åº
- åªèƒ½é€ä¸ªæŸ¥çœ‹ï¼Œå…¨é è‚‰çœ¼åˆ¤æ–­
- å®¹æ˜“æ¼æ‰é‡è¦çš„ç°‡

ç†æƒ³ï¼š
- ç³»ç»Ÿè‡ªåŠ¨æ¨è"TOP 15é«˜è´¨é‡ç°‡"
- ä¼˜å…ˆå®¡æ ¸æ¨èç°‡
- èŠ‚çœ50%+å®¡æ ¸æ—¶é—´
```

#### è®¾è®¡æ–¹æ¡ˆ

##### 1.2.1 èšç±»è´¨é‡è¯„åˆ†ç®—æ³•

```python
# core/cluster_scorer.py

from typing import Dict, List
import numpy as np
from storage.repository import ClusterRepository

class ClusterQualityScorer:
    """
    èšç±»è´¨é‡è¯„åˆ†å™¨

    è¯„åˆ†ç»´åº¦ï¼ˆå‚è€ƒGPTå»ºè®®ï¼‰ï¼š
    1. ç°‡å¤§å°ï¼ˆå¤ªå°å¤ªå¤§éƒ½æ‰£åˆ†ï¼‰
    2. ç°‡å†…ç´§å¯†åº¦ï¼ˆå¹³å‡è·ç¦»è¶Šå°è¶Šå¥½ï¼‰
    3. ç°‡é—´åˆ†ç¦»åº¦ï¼ˆä¸å…¶ä»–ç°‡è·ç¦»è¶Šå¤§è¶Šå¥½ï¼‰
    4. å¹³å‡æœç´¢é‡ï¼ˆå¦‚æœæœ‰æœç´¢é‡æ•°æ®ï¼‰
    """

    def __init__(self):
        self.weights = {
            'size_score': 0.25,
            'cohesion_score': 0.35,
            'separation_score': 0.20,
            'volume_score': 0.20
        }

    def calculate_size_score(self, cluster_size: int) -> float:
        """
        ç°‡å¤§å°è¯„åˆ†ï¼ˆUå‹æ›²çº¿ï¼‰

        æœ€ä½³èŒƒå›´ï¼š50-500
        å¤ªå°ï¼ˆ<20ï¼‰ï¼šå¯èƒ½æ˜¯å™ªå£°
        å¤ªå¤§ï¼ˆ>2000ï¼‰ï¼šå¤ªæ³›åŒ–
        """
        if cluster_size < 20:
            return cluster_size / 20 * 60  # 0-60åˆ†
        elif 20 <= cluster_size <= 50:
            return 60 + (cluster_size - 20) / 30 * 20  # 60-80åˆ†
        elif 50 < cluster_size <= 500:
            return 80 + (500 - cluster_size) / 450 * 20  # 80-100åˆ†
        elif 500 < cluster_size <= 2000:
            return 80 - (cluster_size - 500) / 1500 * 30  # 80-50åˆ†
        else:
            return max(50 - (cluster_size - 2000) / 1000 * 20, 20)  # 50-20åˆ†

    def calculate_cohesion_score(self, cluster_id: int, embeddings: np.ndarray) -> float:
        """
        ç°‡å†…ç´§å¯†åº¦è¯„åˆ†

        è®¡ç®—ç°‡å†…æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒçš„å¹³å‡è·ç¦»
        è·ç¦»è¶Šå° = è¶Šç´§å¯† = åˆ†æ•°è¶Šé«˜
        """
        center = embeddings.mean(axis=0)
        distances = np.linalg.norm(embeddings - center, axis=1)
        avg_distance = distances.mean()

        # å½’ä¸€åŒ–åˆ°0-100
        # å‡è®¾è·ç¦»èŒƒå›´ï¼š0-2ï¼ˆæ ¹æ®å®é™…embeddingè°ƒæ•´ï¼‰
        score = max(0, 100 - (avg_distance / 2) * 100)
        return score

    def calculate_separation_score(
        self,
        cluster_id: int,
        cluster_center: np.ndarray,
        all_cluster_centers: List[np.ndarray]
    ) -> float:
        """
        ç°‡é—´åˆ†ç¦»åº¦è¯„åˆ†

        è®¡ç®—è¯¥ç°‡ä¸­å¿ƒåˆ°æœ€è¿‘é‚»ç°‡ä¸­å¿ƒçš„è·ç¦»
        è·ç¦»è¶Šå¤§ = è¶Šç‹¬ç‰¹ = åˆ†æ•°è¶Šé«˜
        """
        distances = [
            np.linalg.norm(cluster_center - other_center)
            for other_center in all_cluster_centers
            if not np.array_equal(cluster_center, other_center)
        ]

        if not distances:
            return 50  # åªæœ‰1ä¸ªç°‡ï¼Œç»™ä¸­ç­‰åˆ†

        min_distance = min(distances)

        # å½’ä¸€åŒ–åˆ°0-100
        # å‡è®¾è·ç¦»èŒƒå›´ï¼š0-3
        score = min(100, (min_distance / 3) * 100)
        return score

    def calculate_volume_score(self, cluster_id: int) -> float:
        """
        æœç´¢é‡è¯„åˆ†ï¼ˆå¯é€‰ï¼Œå¦‚æœæœ‰æœç´¢é‡æ•°æ®ï¼‰

        ç°‡å†…çŸ­è¯­çš„å¹³å‡æœç´¢é‡
        é‡è¶Šå¤§ = è¶Šçƒ­é—¨ = å•†ä¸šä»·å€¼è¶Šé«˜
        """
        # TODO: å¦‚æœæœ‰æœç´¢é‡æ•°æ®ï¼Œä»æ•°æ®åº“è¯»å–
        # æš‚æ—¶è¿”å›50ï¼ˆä¸­ç­‰åˆ†ï¼‰
        return 50.0

    def score_cluster(
        self,
        cluster_id: int,
        cluster_size: int,
        cluster_embeddings: np.ndarray,
        cluster_center: np.ndarray,
        all_cluster_centers: List[np.ndarray]
    ) -> Dict:
        """
        ç»¼åˆè¯„åˆ†
        """
        size_score = self.calculate_size_score(cluster_size)
        cohesion_score = self.calculate_cohesion_score(cluster_id, cluster_embeddings)
        separation_score = self.calculate_separation_score(
            cluster_id, cluster_center, all_cluster_centers
        )
        volume_score = self.calculate_volume_score(cluster_id)

        # åŠ æƒç»¼åˆ
        quality_score = (
            self.weights['size_score'] * size_score +
            self.weights['cohesion_score'] * cohesion_score +
            self.weights['separation_score'] * separation_score +
            self.weights['volume_score'] * volume_score
        )

        return {
            'quality_score': round(quality_score, 2),
            'size_score': round(size_score, 2),
            'cohesion_score': round(cohesion_score, 2),
            'separation_score': round(separation_score, 2),
            'volume_score': round(volume_score, 2)
        }
```

##### 1.2.2 æ•°æ®åº“æ‰©å±•

```sql
-- ä¸ºclustersè¡¨æ·»åŠ è¯„åˆ†å­—æ®µ
ALTER TABLE clusters ADD COLUMN quality_score FLOAT;
ALTER TABLE clusters ADD COLUMN size_score FLOAT;
ALTER TABLE clusters ADD COLUMN cohesion_score FLOAT;
ALTER TABLE clusters ADD COLUMN separation_score FLOAT;
ALTER TABLE clusters ADD COLUMN volume_score FLOAT;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_quality_score ON clusters(quality_score DESC);
```

##### 1.2.3 UIå±•ç¤º

```python
# ui/pages/clusters.py

def show_cluster_list_with_scores():
    """
    æ˜¾ç¤ºå¸¦è¯„åˆ†çš„ç°‡åˆ—è¡¨
    """
    st.title("ğŸ¯ Phase 2: å¤§ç»„èšç±»ç»“æœï¼ˆæŒ‰è´¨é‡æ’åºï¼‰")

    # ä»æ•°æ®åº“åŠ è½½ç°‡åŠå…¶è¯„åˆ†
    with ClusterRepository() as repo:
        clusters = repo.get_all_clusters_with_scores(
            order_by='quality_score',
            order_direction='DESC'
        )

    # === é¡¶éƒ¨ï¼šæ¨èç°‡åŒºåŸŸ ===
    st.header("â­ æ¨èç°‡ï¼ˆTOP 15ï¼‰")
    st.caption("ç³»ç»Ÿæ ¹æ®è´¨é‡è¯„åˆ†æ¨èçš„æœ€æœ‰æ½œåŠ›çš„ç°‡")

    top_15 = clusters[:15]

    for rank, cluster in enumerate(top_15, 1):
        with st.expander(
            f"#{rank} - ç°‡ {cluster.cluster_id} "
            f"(è´¨é‡åˆ†: {cluster.quality_score:.1f}/100, "
            f"å¤§å°: {cluster.size})"
        ):
            # æ˜¾ç¤ºè¯„åˆ†ç»†èŠ‚
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("å¤§å°åˆ†", f"{cluster.size_score:.1f}")
            col2.metric("ç´§å¯†åº¦", f"{cluster.cohesion_score:.1f}")
            col3.metric("åˆ†ç¦»åº¦", f"{cluster.separation_score:.1f}")
            col4.metric("æœç´¢é‡", f"{cluster.volume_score:.1f}")

            # æ˜¾ç¤ºä»£è¡¨çŸ­è¯­
            st.subheader("ä»£è¡¨çŸ­è¯­ï¼ˆå‰10æ¡ï¼‰")
            sample_phrases = repo.get_cluster_phrases(
                cluster.cluster_id, limit=10
            )
            for phrase in sample_phrases:
                st.text(f"  â€¢ {phrase.phrase_text}")

            # æ˜¾ç¤ºTopç‰¹å¾è¯ï¼ˆå¦‚æœæœ‰ï¼‰
            top_tokens = repo.get_cluster_top_tokens(
                cluster.cluster_id, limit=10
            )
            if top_tokens:
                st.subheader("Topç‰¹å¾è¯")
                st.write(", ".join([t.token_text for t in top_tokens]))

            # æ“ä½œæŒ‰é’®
            if st.button(f"é€‰æ‹©ç°‡ {cluster.cluster_id}", key=f"select_{cluster.cluster_id}"):
                st.session_state.selected_clusters.append(cluster.cluster_id)
                st.success(f"å·²é€‰æ‹©ç°‡ {cluster.cluster_id}")

    # === å®Œæ•´ç°‡åˆ—è¡¨ï¼ˆæŠ˜å ï¼‰ ===
    with st.expander("ğŸ“‹ å®Œæ•´ç°‡åˆ—è¡¨ï¼ˆæŒ‰è´¨é‡é™åºï¼‰"):
        for cluster in clusters:
            st.write(
                f"ç°‡ {cluster.cluster_id}: "
                f"è´¨é‡åˆ† {cluster.quality_score:.1f}, "
                f"å¤§å° {cluster.size}"
            )
```

#### å®æ–½æ­¥éª¤

```
Week 1: è¯„åˆ†ç®—æ³•å®ç°
  1. å®ç°ClusterQualityScorerï¼šcore/cluster_scorer.py
  2. ç¼–å†™è¯„åˆ†è„šæœ¬ï¼šscripts/score_clusters.py
  3. å¯¹ç°æœ‰ç°‡æ‰¹é‡è¯„åˆ†
  4. éªŒè¯è¯„åˆ†åˆç†æ€§ï¼šäººå·¥æŸ¥çœ‹TOP 15æ˜¯å¦ç¡®å®é«˜è´¨é‡

Week 2: UIé›†æˆ
  1. ä¿®æ”¹ç°‡åˆ—è¡¨é¡µé¢ï¼šui/pages/clusters.py
  2. å¢åŠ "æ¨èç°‡"åŒºåŸŸ
  3. å¢åŠ è¯„åˆ†ç»†èŠ‚å±•ç¤º
  4. æµ‹è¯•ç”¨æˆ·ä½“éªŒ

Week 3: æ•ˆæœéªŒè¯
  1. ç”¨æ–°æ•°æ®è·‘å®Œæ•´æµç¨‹
  2. å¯¹æ¯”ç­›é€‰æ—¶é—´ï¼šæœ‰è¯„åˆ† vs æ— è¯„åˆ†
  3. å¯¹æ¯”é—æ¼ç‡ï¼šæ¨èç°‡å‘½ä¸­ç‡
  4. æ”¶é›†ç”¨æˆ·åé¦ˆ
```

#### æˆåŠŸæ ‡å‡†

```
âœ… è¯„åˆ†ç®—æ³•èƒ½åŒºåˆ†é«˜è´¨é‡ç°‡å’Œä½è´¨é‡ç°‡
âœ… TOP 15æ¨èç°‡çš„å‡†ç¡®ç‡ > 70%ï¼ˆä½ æœ€ç»ˆé€‰çš„ç°‡æœ‰70%åœ¨TOP 15å†…ï¼‰
âœ… å®¡æ ¸æ—¶é—´ä¸‹é™ 30-50%
âœ… é—æ¼ç‡ä¸‹é™ï¼ˆæ›´å®¹æ˜“å‘ç°å¥½ç°‡ï¼‰
```

---

## Phase 2: æŒ‰éœ€ä¼˜åŒ–ï¼ˆ6-8å‘¨ï¼‰

è¿™ä¸€éƒ¨åˆ†æ˜¯**æ ¹æ®Phase 0åŸºçº¿æµ‹é‡ç»“æœå†³å®šæ˜¯å¦å®æ–½**çš„ä¼˜åŒ–ã€‚

---

### 2.1 è¯çº§è§„èŒƒåŒ–å»é‡ ğŸŸ¡ æ¡ä»¶æ‰§è¡Œ

#### å‰ç½®æ¡ä»¶
ä»…å½“**Experiment Cåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡**æ—¶æ‰å®æ–½ï¼ˆå†—ä½™ç‡>20%ï¼‰

#### ç›®æ ‡
æŠŠ"åŒä¸€éœ€æ±‚çš„ä¸åŒè¡¨è¾¾"æ”¶æ•›åˆ°ä¸€ä¸ªè§„èŒƒåŒ–å½¢å¼ï¼Œå‡å°‘å™ªå£°ã€‚

#### âš ï¸ å…³é”®ä¿®æ­£ï¼šä½¿ç”¨ä¿å®ˆç®—æ³•

**GPTå»ºè®®çš„é—®é¢˜**ï¼š
```python
# âŒ GPTå»ºè®®å»é™¤çš„åœç”¨è¯
STOP_WORDS = ['for', 'at', 'in', 'on', 'with', 'the', 'a', 'an']

# é—®é¢˜ç¤ºä¾‹ï¼š
"calculator for students" â†’ "calculator students"
"calculator for business" â†’ "calculator business"
# å»æ‰"for"åï¼Œæ— æ³•åŒºåˆ†ç›®æ ‡äººç¾¤ï¼

"compressor at home" â†’ "compressor home"  # ä¸¢å¤±åœºæ™¯ä¿¡æ¯
"best at compression" â†’ "best compression"  # è¯­ä¹‰æ”¹å˜
```

**ä¿®æ­£åçš„ä¿å®ˆç®—æ³•**ï¼š

```python
# core/canonicalizer_en.py

from typing import List, Set
from nltk.stem import WordNetLemmatizer
import re

class ConservativeCanonicalizer:
    """
    ä¿å®ˆçš„è‹±æ–‡è§„èŒƒåŒ–å™¨

    æ ¸å¿ƒåŸåˆ™ï¼š
    1. ä»…å»é™¤æ˜ç¡®æ— è¯­ä¹‰çš„å† è¯ï¼ˆa/an/theï¼‰
    2. ä¿ç•™æ‰€æœ‰æ„å›¾è¯ï¼ˆbest/free/online/topï¼‰
    3. ä¿ç•™æ‰€æœ‰ä»‹è¯ï¼ˆfor/at/inè¡¨ç¤ºåœºæ™¯/ç¾¤ä½“ï¼‰
    4. è¯å½¢è¿˜åŸï¼ˆå¯é€‰é…ç½®ï¼‰
    5. è¯çº§æ’åºï¼ˆå¯é€‰é…ç½®ï¼‰

    ä¸ºä»€ä¹ˆä¿å®ˆï¼Ÿ
    - å®å¯å¤šä¸€äº›å†—ä½™ï¼Œä¹Ÿä¸è¦ä¸¢å¤±è¯­ä¹‰
    - å»é‡ä¸æ˜¯ç›®çš„ï¼Œä¿æŒéœ€æ±‚å®Œæ•´æ€§æ‰æ˜¯
    """

    def __init__(
        self,
        enable_lemmatization: bool = False,
        enable_sorting: bool = False
    ):
        # ä»…å»é™¤æœ€åŸºç¡€çš„å† è¯
        self.MINIMAL_STOP_WORDS: Set[str] = {'a', 'an', 'the'}

        self.enable_lemmatization = enable_lemmatization
        self.enable_sorting = enable_sorting

        if enable_lemmatization:
            self.lemmatizer = WordNetLemmatizer()

    def canonicalize(self, phrase: str) -> str:
        """
        ç”Ÿæˆè§„èŒƒåŒ–å½¢å¼

        æ­¥éª¤ï¼š
        1. å°å†™åŒ–
        2. åˆ†è¯
        3. å»é™¤å† è¯ï¼ˆä»…a/an/theï¼‰
        4. ã€å¯é€‰ã€‘è¯å½¢è¿˜åŸ
        5. ã€å¯é€‰ã€‘è¯çº§æ’åº
        6. æ‹¼æ¥
        """
        # 1. å°å†™åŒ–
        phrase = phrase.lower().strip()

        # 2. åˆ†è¯ï¼ˆä¿ç•™è¯è¾¹ç•Œï¼‰
        words = re.findall(r'\b\w+\b', phrase)

        # 3. å»é™¤å† è¯ï¼ˆä»…a/an/theï¼‰
        words = [w for w in words if w not in self.MINIMAL_STOP_WORDS]

        # 4. è¯å½¢è¿˜åŸï¼ˆå¯é€‰ï¼‰
        if self.enable_lemmatization:
            words = [self.lemmatizer.lemmatize(w) for w in words]

        # 5. è¯çº§æ’åºï¼ˆå¯é€‰ï¼‰
        if self.enable_sorting:
            words.sort()

        # 6. æ‹¼æ¥
        return ' '.join(words)

    def get_canonical_id(self, phrase: str) -> str:
        """
        è·å–è§„èŒƒåŒ–IDï¼ˆç”¨äºå»é‡ï¼‰
        """
        canonical_text = self.canonicalize(phrase)

        # ä½¿ç”¨MD5ç”ŸæˆçŸ­IDï¼ˆå¯é€‰ï¼‰
        # import hashlib
        # return hashlib.md5(canonical_text.encode()).hexdigest()

        return canonical_text

# æµ‹è¯•ä¿å®ˆç®—æ³•
canonicalizer = ConservativeCanonicalizer(
    enable_lemmatization=False,  # é…ç½®1ï¼šä¸åšè¯å½¢è¿˜åŸ
    enable_sorting=False          # é…ç½®2ï¼šä¸åšæ’åºï¼ˆä¿æŒè¯åºï¼‰
)

# ç¤ºä¾‹1ï¼šä»…å»é™¤å† è¯
print(canonicalizer.canonicalize("best calculator for students"))
# â†’ "best calculator for students"ï¼ˆä¿æŒå®Œæ•´ï¼‰

print(canonicalizer.canonicalize("a best calculator for the students"))
# â†’ "best calculator for students"ï¼ˆä»…å»é™¤a/theï¼‰

# ç¤ºä¾‹2ï¼šè¯åºä¸åŒä½†æ„å›¾ç›¸åŒï¼ˆéœ€è¦enable_sorting=Trueæ‰èƒ½è¯†åˆ«ï¼‰
canonicalizer_sort = ConservativeCanonicalizer(
    enable_lemmatization=False,
    enable_sorting=True  # å¼€å¯æ’åº
)

print(canonicalizer_sort.canonicalize("best calculator for students"))
# â†’ "best calculator for students"ï¼ˆæ’åºåï¼‰

print(canonicalizer_sort.canonicalize("calculator best for students"))
# â†’ "best calculator for students"ï¼ˆç›¸åŒï¼ï¼‰

# ç¤ºä¾‹3ï¼šä¿ç•™ä»‹è¯çš„é‡è¦æ€§
print(canonicalizer.canonicalize("calculator for students"))
# â†’ "calculator for students"ï¼ˆä¿ç•™forï¼ŒåŒºåˆ†äººç¾¤ï¼‰

print(canonicalizer.canonicalize("calculator for business"))
# â†’ "calculator for business"ï¼ˆä¸åŒéœ€æ±‚ï¼ï¼‰
```

**ä¸ºä»€ä¹ˆä¸ç”¨GPTçš„ç®—æ³•ï¼ˆè¯¦ç»†å¯¹æ¯”ï¼‰**ï¼š

```
âŒ GPTç‰ˆæœ¬ï¼š
STOP_WORDS = ['for', 'at', 'in', 'on', 'with', 'the', 'a', 'an']

é—®é¢˜1ï¼šä¸¢å¤±äººç¾¤/åœºæ™¯ä¿¡æ¯
  "calculator for students" â†’ "calculator students"
  "calculator for business" â†’ "calculator business"
  "VPN for China" â†’ "VPN China"
  â†’ å»æ‰"for"åï¼Œæ— æ³•åŒºåˆ†ç›®æ ‡ç¾¤ä½“å’Œä½¿ç”¨åœºæ™¯

é—®é¢˜2ï¼šä¸¢å¤±æ„å›¾å®Œæ•´æ€§
  "best at compression" â†’ "best compression"ï¼ˆè¯­ä¹‰å˜åŒ–ï¼‰
  "work on Windows" â†’ "work Windows"ï¼ˆä¸é€šé¡ºï¼‰

é—®é¢˜3ï¼šä»‹è¯åœ¨è‹±æ–‡ä¸­æ‰¿è½½è¯­ä¹‰
  "tool for image" â‰  "tool in image" â‰  "tool at image"
  ä»‹è¯ä¸æ˜¯çº¯åŠŸèƒ½è¯ï¼Œå»é™¤ä¼šç ´åè¯­ä¹‰

âœ… ä¿å®ˆç‰ˆæœ¬ï¼š
MINIMAL_STOP_WORDS = {'a', 'an', 'the'}

ä¼˜ç‚¹1ï¼šåªå»é™¤çº¯åŠŸèƒ½æ€§å† è¯
  "a calculator" â†’ "calculator"
  "the best tool" â†’ "best tool"
  â†’ è¿™äº›å† è¯ç¡®å®æ— è¯­ä¹‰

ä¼˜ç‚¹2ï¼šä¿ç•™æ‰€æœ‰æœ‰è¯­ä¹‰çš„è¯
  "for students" ä¿ç•™ï¼ˆåŒºåˆ†äººç¾¤ï¼‰
  "at home" ä¿ç•™ï¼ˆåŒºåˆ†åœºæ™¯ï¼‰
  "best" ä¿ç•™ï¼ˆæ„å›¾è¯ï¼‰
  "free" ä¿ç•™ï¼ˆé™åˆ¶è¯ï¼‰

ä¼˜ç‚¹3ï¼šå¯é€‰é…ç½®çµæ´»è°ƒæ•´
  enable_lemmatization: æ˜¯å¦è¯å½¢è¿˜åŸ
    True: "compressor" â†’ "compress"ï¼ˆå¯èƒ½è¿‡åº¦å½’å¹¶ï¼‰
    False: ä¿ç•™åŸè¯å½¢ï¼ˆæ›´ä¿å®ˆï¼‰

  enable_sorting: æ˜¯å¦è¯çº§æ’åº
    True: "best calculator" = "calculator best"ï¼ˆè¯†åˆ«è¯åºå·®å¼‚ï¼‰
    False: ä¿æŒåŸè¯åºï¼ˆæ›´ä¿å®ˆï¼‰
```

#### é…ç½®ç­–ç•¥å»ºè®®

```python
# æ¨èé…ç½®ï¼ˆæ ¹æ®Phase 0ç»“æœé€‰æ‹©ï¼‰

# é…ç½®Aï¼šæä¿å®ˆï¼ˆå†—ä½™ç‡10-15%æ—¶ï¼‰
canonicalizer = ConservativeCanonicalizer(
    enable_lemmatization=False,  # ä¸è¿˜åŸè¯å½¢
    enable_sorting=False         # ä¸æ’åº
)
# æ•ˆæœï¼šä»…å»é™¤å† è¯ï¼Œé¢å¤–å»é‡5-8%

# é…ç½®Bï¼šä¸­ç­‰ä¿å®ˆï¼ˆå†—ä½™ç‡15-25%æ—¶ï¼‰
canonicalizer = ConservativeCanonicalizer(
    enable_lemmatization=False,  # ä¸è¿˜åŸè¯å½¢
    enable_sorting=True          # å¼€å¯æ’åº
)
# æ•ˆæœï¼šå»é™¤å† è¯+è¯†åˆ«è¯åºå·®å¼‚ï¼Œé¢å¤–å»é‡10-15%

# é…ç½®Cï¼šç§¯æï¼ˆå†—ä½™ç‡>25%æ—¶ï¼‰
canonicalizer = ConservativeCanonicalizer(
    enable_lemmatization=True,   # è¯å½¢è¿˜åŸ
    enable_sorting=True          # è¯çº§æ’åº
)
# æ•ˆæœï¼šå»é™¤å† è¯+è¯å½¢è¿˜åŸ+è¯åºï¼Œé¢å¤–å»é‡15-25%
# âš ï¸ é£é™©ï¼šå¯èƒ½è¯¯åˆå¹¶ï¼ˆéœ€è¦äººå·¥éªŒè¯ï¼‰
```

#### æ•°æ®åº“è®¾è®¡

```sql
-- æ–°å¢è§„èŒƒåŒ–è¡¨
CREATE TABLE canonical_forms (
    canonical_id INT PRIMARY KEY AUTO_INCREMENT,
    canonical_text VARCHAR(500) NOT NULL,
    phrase_count INT DEFAULT 0,  -- æœ‰å¤šå°‘çŸ­è¯­æ˜ å°„åˆ°è¿™ä¸ªè§„èŒƒå½¢å¼
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE KEY idx_canonical_text (canonical_text)
);

-- ä¿®æ”¹phrasesè¡¨
ALTER TABLE phrases ADD COLUMN canonical_id INT;
ALTER TABLE phrases ADD FOREIGN KEY (canonical_id)
    REFERENCES canonical_forms(canonical_id);

-- ç´¢å¼•
CREATE INDEX idx_phrases_canonical ON phrases(canonical_id);
```

#### å®æ–½æ­¥éª¤

```
Week 1: ç®—æ³•å®ç°ä¸æµ‹è¯•
  1. å®ç°ConservativeCanonicalizerï¼šcore/canonicalizer_en.py
  2. å•å…ƒæµ‹è¯•ï¼štests/test_canonicalizer.py
  3. åœ¨Phase 0æŠ½æ ·çš„1000æ¡ä¸Šæµ‹è¯•
  4. äººå·¥éªŒè¯è¯¯åˆå¹¶ç‡ï¼ˆfalse positiveï¼‰
  5. æ ¹æ®ç»“æœè°ƒæ•´é…ç½®

Week 2-3: æ‰¹é‡å¤„ç†ä¸éªŒè¯
  1. ç¼–å†™æ‰¹é‡è§„èŒƒåŒ–è„šæœ¬ï¼šscripts/canonicalize_phrases.py
  2. å¯¹æ‰€æœ‰çŸ­è¯­ç”Ÿæˆcanonical_id
  3. ç»Ÿè®¡å»é‡æ•ˆæœï¼šè§„èŒƒåŒ–å‰åæ•°é‡å¯¹æ¯”
  4. äººå·¥æŠ½æ ·200æ¡æ£€æŸ¥ï¼š
     - æ­£ç¡®åˆå¹¶ï¼ˆtrue positiveï¼‰
     - è¯¯åˆå¹¶ï¼ˆfalse positiveï¼‰
     - æœªåˆå¹¶ä½†åº”åˆå¹¶ï¼ˆfalse negativeï¼‰
  5. å¦‚æœè¯¯åˆå¹¶ç‡>5%ï¼Œè°ƒæ•´é…ç½®é‡æ–°è·‘

Week 3-4: èšç±»é‡è·‘ä¸æ•ˆæœå¯¹æ¯”
  1. ç”¨è§„èŒƒåŒ–åçš„æ•°æ®é‡è·‘Phase 2èšç±»
  2. å¯¹æ¯”ç°‡çš„è´¨é‡ï¼šè§„èŒƒåŒ–å‰ vs å
  3. æ£€æŸ¥æ˜¯å¦æœ‰ç°‡å˜å¾—æ›´æ¸…æ™°
  4. æ£€æŸ¥æ˜¯å¦æœ‰éœ€æ±‚è¢«é”™è¯¯åˆå¹¶
```

#### æˆåŠŸæ ‡å‡†

```
âœ… é¢å¤–å»é‡10-20%ï¼ˆæ ¹æ®é…ç½®ï¼‰
âœ… è¯¯åˆå¹¶ç‡ < 5%ï¼ˆäººå·¥æŠ½æ ·éªŒè¯ï¼‰
âœ… èšç±»ç°‡æ›´æ¸…æ™°ï¼ˆç°‡å†…çŸ­è¯­æ›´ä¸€è‡´ï¼‰
âœ… æ²¡æœ‰ç ´åé«˜è´¨é‡éœ€æ±‚çš„ç‹¬ç‰¹æ€§
```

---

### 2.2 æ¨¡æ¿-å˜é‡è¿­ä»£æå– ğŸŸ¡ æ¡ä»¶æ‰§è¡Œ

#### å‰ç½®æ¡ä»¶
ä»…å½“**Experiment Båˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡**æ—¶æ‰å®æ–½ï¼ˆtokenè¦†ç›–ç‡<60%ï¼‰

#### ç›®æ ‡
æŠŠç°åœ¨çš„26ä¸ªtokenæ‰©å±•åˆ°200-500ä¸ªå¯ç®¡ç†çš„ç»“æ„è¯åº“ï¼Œç”¨äºè§£é‡Šç°‡ã€è¾…åŠ©æ„å›¾è¯†åˆ«ã€‚

#### âš ï¸ å…³é”®ä¿®æ­£ï¼šå¢åŠ "ä»æ•°æ®æå–ç§å­"æ­¥éª¤

**GPTå»ºè®®çš„å±€é™**ï¼š
```
GPTçš„ç§å­å˜é‡æ¥æºï¼ˆä»…2ä¸ªï¼‰ï¼š
1. ä»ç°æœ‰26ä¸ªtokenä¸­é€‰æ‹©
2. æ‰‹å·¥å®šä¹‰ç§å­å˜é‡

é—®é¢˜ï¼š
- ç°æœ‰tokenå¤ªå°‘ï¼ˆ26ä¸ªï¼‰
- æ‰‹å·¥å®šä¹‰æœ‰ä¸»è§‚æ€§
- å¯èƒ½é—æ¼æ•°æ®ä¸­çœŸå®å­˜åœ¨çš„é«˜é¢‘ç‰¹å¾
```

**ä¿®æ­£åçš„ç§å­é€‰æ‹©ï¼ˆ3ä¸ªæ¥æºï¼ŒæŒ‰ä¼˜å…ˆçº§ï¼‰**ï¼š

```python
# core/seed_selector.py

from typing import List, Dict, Set
from collections import Counter
from storage.repository import ClusterRepository, TokenRepository

class SeedVariableSelector:
    """
    ç§å­å˜é‡é€‰æ‹©å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰

    3ä¸ªæ¥æºï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. ä»å™ªå£°ç‚¹ï¼ˆHDBSCAN cluster_id=-1ï¼‰ä¸­æå– â­ æ–°å¢
    2. ä»ç°æœ‰26ä¸ªtokensä¸­é€‰æ‹©é«˜é¢‘çš„
    3. æ‰‹å·¥è¡¥å……é€šç”¨å˜é‡ï¼ˆfallbackï¼‰
    """

    def select_seeds(self, min_seed_count: int = 20) -> Dict[str, List[str]]:
        """
        é€‰æ‹©ç§å­å˜é‡

        è¿”å›ï¼šæŒ‰ç±»åˆ«åˆ†ç»„çš„ç§å­å˜é‡
        {
            'function': ['compress', 'resize', ...],
            'object': ['image', 'video', ...],
            'channel': ['google', 'youtube', ...],
            'audience': ['students', 'developers', ...]
        }
        """
        seeds: Dict[str, Set[str]] = {
            'function': set(),
            'object': set(),
            'channel': set(),
            'audience': set(),
            'other': set()
        }

        # === æ¥æº1ï¼šä»å™ªå£°ç‚¹ä¸­æå–ï¼ˆæœ€é‡è¦ï¼ï¼‰ ===
        noise_seeds = self._extract_from_noise_points()
        for category, words in noise_seeds.items():
            seeds[category].update(words)

        print(f"[æ¥æº1] ä»å™ªå£°ç‚¹æå–: {sum(len(v) for v in seeds.values())} ä¸ª")

        # === æ¥æº2ï¼šä»ç°æœ‰tokensä¸­é€‰æ‹© ===
        existing_seeds = self._select_from_existing_tokens(min_frequency=10)
        for category, words in existing_seeds.items():
            seeds[category].update(words)

        print(f"[æ¥æº2] ä»ç°æœ‰tokens: {sum(len(v) for v in seeds.values())} ä¸ª")

        # === æ¥æº3ï¼šæ‰‹å·¥è¡¥å……ï¼ˆå¦‚æœå‰ä¸¤ä¸ªæ¥æºä¸è¶³ï¼‰ ===
        total_seeds = sum(len(v) for v in seeds.values())
        if total_seeds < min_seed_count:
            fallback_seeds = self._get_fallback_seeds()
            for category, words in fallback_seeds.items():
                seeds[category].update(words)

            print(f"[æ¥æº3] æ‰‹å·¥è¡¥å……: {sum(len(v) for v in seeds.values())} ä¸ª")

        # è½¬æ¢ä¸ºåˆ—è¡¨
        return {k: list(v) for k, v in seeds.items()}

    def _extract_from_noise_points(self) -> Dict[str, Set[str]]:
        """
        ä»å™ªå£°ç‚¹ä¸­æå–ç§å­å˜é‡ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

        åŸç†ï¼ˆå‚è€ƒå›è¨€ï¼‰ï¼š
        - å™ªå£°ç‚¹ = HDBSCANæ— æ³•èšç±»çš„çŸ­è¯­ï¼ˆcluster_id=-1ï¼‰
        - æ— æ³•èšç±» = åŒ…å«æä¸ªæ€§åŒ–çš„è¯æ±‡
        - æä¸ªæ€§åŒ–è¯æ±‡ = ä¼˜è´¨çš„ç§å­å˜é‡

        ç¤ºä¾‹ï¼š
        å™ªå£°ç‚¹ï¼š"compress pdf using python api"
        æå–tokenï¼š"python", "api"ï¼ˆé«˜ç‰¹å¼‚æ€§è¯æ±‡ï¼‰

        è¿™äº›è¯è™½ç„¶é¢‘æ¬¡ä¸é«˜ï¼Œä½†ç‰¹å¾æ€§å¼ºï¼Œ
        é€‚åˆä½œä¸ºç§å­å˜é‡æ¥æå–æ¨¡æ¿
        """
        with ClusterRepository() as repo:
            # è·å–æ‰€æœ‰å™ªå£°ç‚¹çŸ­è¯­
            noise_phrases = repo.get_phrases_by_cluster(cluster_id=-1)

        print(f"  å™ªå£°ç‚¹çŸ­è¯­æ•°: {len(noise_phrases)}")

        if len(noise_phrases) < 100:
            print("  å™ªå£°ç‚¹å¤ªå°‘ï¼Œè·³è¿‡æ­¤æ¥æº")
            return {'function': set(), 'object': set(),
                    'channel': set(), 'audience': set(), 'other': set()}

        # åˆ†è¯ç»Ÿè®¡
        token_counter = Counter()
        for phrase in noise_phrases:
            words = phrase.phrase_text.lower().split()
            # è¿‡æ»¤ï¼šé•¿åº¦2+ï¼Œéåœç”¨è¯
            words = [
                w for w in words
                if len(w) >= 2 and w not in {'a', 'an', 'the', 'for', 'with'}
            ]
            token_counter.update(words)

        # é€‰æ‹©ä¸­é¢‘è¯ï¼ˆé¢‘æ¬¡3-20ï¼‰
        # åŸå› ï¼š
        # - é¢‘æ¬¡å¤ªä½ï¼ˆ<3ï¼‰ï¼šå¯èƒ½æ˜¯æ‹¼å†™é”™è¯¯æˆ–æç«¯é•¿å°¾
        # - é¢‘æ¬¡å¤ªé«˜ï¼ˆ>20ï¼‰ï¼šå¯èƒ½å·²è¢«èšç±»è¦†ç›–
        # - ä¸­é¢‘è¯ï¼ˆ3-20ï¼‰ï¼šç‰¹å¾æ€§å¼ºä½†æœªè¢«å……åˆ†åˆ©ç”¨
        candidate_tokens = [
            token for token, freq in token_counter.items()
            if 3 <= freq <= 20
        ]

        print(f"  å€™é€‰tokenæ•°: {len(candidate_tokens)}")

        # ä½¿ç”¨LLMåˆ†ç±»è¿™äº›tokens
        categorized = self._categorize_tokens_by_llm(candidate_tokens[:50])

        return categorized

    def _select_from_existing_tokens(self, min_frequency: int = 10) -> Dict[str, Set[str]]:
        """
        ä»ç°æœ‰26ä¸ªtokensä¸­é€‰æ‹©é«˜é¢‘çš„
        """
        with TokenRepository() as repo:
            tokens = repo.get_all_tokens()

        # è¿‡æ»¤ï¼šé¢‘æ¬¡ >= min_frequency
        high_freq_tokens = [
            t for t in tokens
            if t.total_frequency >= min_frequency
        ]

        print(f"  é«˜é¢‘tokenæ•°: {len(high_freq_tokens)}")

        # æŒ‰ç±»åˆ«åˆ†ç»„
        seeds: Dict[str, Set[str]] = {
            'function': set(),
            'object': set(),
            'channel': set(),
            'audience': set(),
            'other': set()
        }

        for token in high_freq_tokens:
            category = token.token_category or 'other'
            seeds[category].add(token.token_text)

        return seeds

    def _get_fallback_seeds(self) -> Dict[str, Set[str]]:
        """
        æ‰‹å·¥å®šä¹‰çš„é€šç”¨ç§å­å˜é‡ï¼ˆfallbackï¼‰

        ä»…åœ¨å‰ä¸¤ä¸ªæ¥æºä¸è¶³æ—¶ä½¿ç”¨
        """
        return {
            'function': {
                'compress', 'resize', 'convert', 'edit',
                'generate', 'analyze', 'create', 'download',
                'upload', 'share', 'merge', 'split'
            },
            'object': {
                'image', 'video', 'pdf', 'text', 'file',
                'photo', 'document', 'data', 'audio'
            },
            'channel': {
                'google', 'youtube', 'facebook', 'twitter',
                'instagram', 'shopify', 'wordpress'
            },
            'audience': {
                'students', 'developers', 'business',
                'marketers', 'designers', 'beginners'
            }
        }

    def _categorize_tokens_by_llm(self, tokens: List[str]) -> Dict[str, Set[str]]:
        """
        ä½¿ç”¨LLMå¯¹tokensè¿›è¡Œåˆ†ç±»
        """
        from ai.client import get_llm_client

        client = get_llm_client()

        prompt = f"""
Classify these keywords into categories:

Categories:
- function: actions/capabilities (compress, resize, convert)
- object: things being processed (image, video, pdf)
- channel: platforms/websites/apps (google, youtube, shopify)
- audience: target user groups (students, developers, marketers)
- other: none of the above

Keywords: {', '.join(tokens)}

Return JSON format:
{{
  "function": ["compress", "resize"],
  "object": ["image", "video"],
  "channel": ["google"],
  "audience": ["students"],
  "other": ["misc"]
}}

Only return JSON, no explanation.
"""

        response = client.chat([{"role": "user", "content": prompt}])

        # è§£æJSON
        import json
        categorized = json.loads(response)

        # è½¬æ¢ä¸ºSet
        return {k: set(v) for k, v in categorized.items()}

# ä½¿ç”¨ç¤ºä¾‹
selector = SeedVariableSelector()
seeds = selector.select_seeds(min_seed_count=20)

print("\nç§å­å˜é‡ï¼ˆæŒ‰ç±»åˆ«ï¼‰ï¼š")
for category, words in seeds.items():
    print(f"\n{category} ({len(words)}ä¸ª):")
    print(f"  {', '.join(list(words)[:10])}")
```

**ä¸ºä»€ä¹ˆè¦å¢åŠ "ä»å™ªå£°ç‚¹æå–"ï¼ˆè¯¦ç»†è¯´æ˜ï¼‰**ï¼š

```
å›è¨€çš„ç»éªŒï¼ˆæºç è¯æ®ï¼‰ï¼š
"ç§å­å˜é‡ä»å“ªé‡ŒæŒ‘é€‰ï¼Ÿâ†’ ä»æ— æ³•èšç±»çš„é•¿å°¾è¯ä¸­æŒ‘é€‰"
"åŸå› ï¼šæ— æ³•èšç±» = åŒ…å«æä¸ªæ€§åŒ–çš„è¯æ±‡ = ä¼˜è´¨ç§å­"

å…·ä½“æ¡ˆä¾‹ï¼š
å™ªå£°ç‚¹1: "compress pdf using python api"
  â†’ æå–: "python", "api"
  â†’ è¿™äº›è¯ç‰¹å¾æ€§å¼ºï¼Œé€‚åˆåšç§å­

å™ªå£°ç‚¹2: "best image editor for photographers on mac"
  â†’ æå–: "photographers", "mac"
  â†’ ç»†åˆ†äººç¾¤å’Œå¹³å°ï¼Œé«˜ä»·å€¼ç§å­

å™ªå£°ç‚¹3: "free online video compressor no watermark"
  â†’ æå–: "watermark", "online"
  â†’ ç»†åˆ†éœ€æ±‚ï¼Œä¼˜è´¨ç§å­

å¯¹æ¯”GPTæ–¹æ¡ˆçš„ä¸è¶³ï¼š
âŒ GPTä»…ç”¨"ç°æœ‰26ä¸ªtokens"ï¼š
   â†’ å¯èƒ½é—æ¼æ•°æ®ä¸­çœŸå®å­˜åœ¨çš„é«˜é¢‘ç‰¹å¾
   â†’ å¦‚æœ26ä¸ªtokenæœ¬èº«ä¸å…¨é¢ï¼Œæ— æ³•æ‰©å±•

âŒ GPTä»…ç”¨"æ‰‹å·¥å®šä¹‰"ï¼š
   â†’ ä¸»è§‚æ€§å¼ºï¼Œå¯èƒ½åç¦»å®é™…æ•°æ®
   â†’ æ— æ³•å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼

âœ… å¢åŠ "ä»å™ªå£°ç‚¹æå–"ï¼š
   â†’ æ•°æ®é©±åŠ¨ï¼Œå‘ç°çœŸå®ç‰¹å¾
   â†’ æ•è·é•¿å°¾ä½†é«˜ä»·å€¼çš„è¯æ±‡
   â†’ ç¬¦åˆå›è¨€çš„æ ¸å¿ƒæ–¹æ³•è®º
```

#### æ¨¡æ¿-å˜é‡è¿­ä»£ç®—æ³•ï¼ˆè‹±æ–‡é€‚é…ç‰ˆï¼‰

```python
# core/template_variable_extractor.py

import re
from typing import List, Dict, Set, Tuple
from collections import Counter, defaultdict

class TemplateVariableExtractor:
    """
    æ¨¡æ¿-å˜é‡è¿­ä»£æå–å™¨ï¼ˆè‹±æ–‡é€‚é…ç‰ˆï¼‰

    æ ¸å¿ƒæ€æƒ³ï¼ˆä¸å›è¨€ä¸€è‡´ï¼‰ï¼š
    - å˜é‡ â†’ æå–æ¨¡æ¿ â†’ æå–æ›´å¤šå˜é‡ â†’ ...
    - è´¨é‡è¿‡æ»¤ï¼šå˜é‡å¿…é¡»é€‚é… â‰¥3ä¸ªæ¨¡æ¿
    - è¿­ä»£3è½®ç›´è‡³æ”¶æ•›
    """

    def __init__(self, phrases: List[str], seed_variables: Dict[str, List[str]]):
        self.phrases = phrases
        self.seed_variables = seed_variables

        # åˆå¹¶æ‰€æœ‰ç§å­å˜é‡
        self.all_seed_vars = []
        for category, words in seed_variables.items():
            self.all_seed_vars.extend(words)

    def extract_templates_from_variables(
        self,
        variables: List[str],
        min_template_freq: int = 5
    ) -> List[str]:
        """
        Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿

        é€»è¾‘ï¼š
        1. å¯¹æ¯ä¸ªçŸ­è¯­ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æŸä¸ªå˜é‡
        2. å¦‚æœåŒ…å«ï¼Œå°†å˜é‡æ›¿æ¢ä¸ºå ä½ç¬¦[X]
        3. ç»Ÿè®¡æ¨¡æ¿é¢‘æ¬¡
        4. ä¿ç•™é¢‘æ¬¡ >= min_template_freq çš„æ¨¡æ¿
        """
        template_counter = Counter()

        for phrase in self.phrases:
            phrase_lower = phrase.lower()

            for var in variables:
                # å®Œæ•´è¯åŒ¹é…ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
                # ä¾‹å¦‚ï¼š"press" ä¸åº”åŒ¹é… "compressor" ä¸­çš„ "press"
                pattern = r'\b' + re.escape(var) + r'\w*\b'

                if re.search(pattern, phrase_lower):
                    # æ›¿æ¢å˜é‡ä¸º[X]
                    template = re.sub(pattern, '[X]', phrase_lower)
                    template_counter[template] += 1

        # è¿‡æ»¤ï¼šé¢‘æ¬¡ >= min_template_freq
        valid_templates = [
            template for template, freq in template_counter.items()
            if freq >= min_template_freq
        ]

        print(f"  æå–æ¨¡æ¿: {len(valid_templates)} ä¸ªï¼ˆé¢‘æ¬¡>={min_template_freq}ï¼‰")

        return valid_templates

    def extract_variables_from_templates(
        self,
        templates: List[str],
        min_var_freq: int = 5,
        min_template_match: int = 3
    ) -> List[Tuple[str, int, int]]:
        """
        Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡

        é€»è¾‘ï¼š
        1. å¯¹æ¯ä¸ªæ¨¡æ¿ï¼Œå°†[X]è½¬ä¸ºæ­£åˆ™è¡¨è¾¾å¼(.+?)
        2. åœ¨æ‰€æœ‰çŸ­è¯­ä¸­åŒ¹é…
        3. æå–[X]ä½ç½®çš„è¯æ±‡ä½œä¸ºå€™é€‰å˜é‡
        4. è´¨é‡è¿‡æ»¤ï¼š
           - é¢‘æ¬¡ >= min_var_freq
           - é€‚é…æ¨¡æ¿æ•° >= min_template_matchï¼ˆå›è¨€çš„å…³é”®è´¨é‡ä¿è¯ï¼‰
        """
        variable_freq = Counter()
        variable_templates = defaultdict(set)  # æ¯ä¸ªå˜é‡é€‚é…äº†å“ªäº›æ¨¡æ¿

        for template in templates:
            # å°†[X]è½¬ä¸ºæ­£åˆ™è¡¨è¾¾å¼
            pattern = template.replace('[X]', r'(\w+(?:\s+\w+)?)')  # åŒ¹é…1-2ä¸ªè¯

            for phrase in self.phrases:
                phrase_lower = phrase.lower()
                match = re.search(pattern, phrase_lower)

                if match:
                    var = match.group(1).strip()

                    # è¿‡æ»¤ï¼šé•¿åº¦åˆç†ï¼ˆ2-30å­—ç¬¦ï¼‰
                    if 2 <= len(var) <= 30:
                        variable_freq[var] += 1
                        variable_templates[var].add(template)

        # è´¨é‡è¿‡æ»¤
        valid_variables = [
            (var, freq, len(variable_templates[var]))
            for var, freq in variable_freq.items()
            if freq >= min_var_freq and len(variable_templates[var]) >= min_template_match
        ]

        print(f"  æå–å˜é‡: {len(valid_variables)} ä¸ªï¼ˆé¢‘æ¬¡>={min_var_freq}, æ¨¡æ¿>={min_template_match}ï¼‰")

        return valid_variables

    def iterative_extraction(self, max_rounds: int = 3) -> Tuple[List[str], List[str]]:
        """
        è¿­ä»£æå–

        Round 1: ç§å­å˜é‡ â†’ æå–æ¨¡æ¿ â†’ æå–å˜é‡
        Round 2: æ–°å˜é‡ â†’ æå–æ›´å¤šæ¨¡æ¿ â†’ æå–æ›´å¤šå˜é‡
        Round 3: ç»§ç»­è¿­ä»£ â†’ ç›´è‡³æ”¶æ•›
        """
        all_templates = set()
        all_variables = set(self.all_seed_vars)

        for round_num in range(max_rounds):
            print(f"\n=== Round {round_num + 1} ===")
            print(f"å½“å‰å˜é‡æ•°: {len(all_variables)}")

            # Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿
            new_templates = self.extract_templates_from_variables(
                variables=list(all_variables),
                min_template_freq=5
            )

            before_template_count = len(all_templates)
            all_templates.update(new_templates)
            after_template_count = len(all_templates)

            print(f"  æ–°å¢æ¨¡æ¿: {after_template_count - before_template_count} ä¸ª")

            # Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡
            new_vars_tuples = self.extract_variables_from_templates(
                templates=new_templates,
                min_var_freq=5,
                min_template_match=3  # å›è¨€çš„è´¨é‡ä¿è¯
            )

            new_vars = [var for var, freq, template_count in new_vars_tuples]

            before_var_count = len(all_variables)
            all_variables.update(new_vars)
            after_var_count = len(all_variables)

            print(f"  æ–°å¢å˜é‡: {after_var_count - before_var_count} ä¸ª")

            # æ”¶æ•›åˆ¤æ–­
            if after_var_count == before_var_count:
                print("\nâœ… å·²æ”¶æ•›ï¼ˆå˜é‡ä¸å†å¢åŠ ï¼‰")
                break

        print(f"\næœ€ç»ˆç»Ÿè®¡:")
        print(f"  æ€»æ¨¡æ¿æ•°: {len(all_templates)}")
        print(f"  æ€»å˜é‡æ•°: {len(all_variables)}")

        return list(all_templates), list(all_variables)

# ä½¿ç”¨ç¤ºä¾‹
from storage.repository import PhraseRepository

# 1. åŠ è½½çŸ­è¯­
with PhraseRepository() as repo:
    phrases = [p.phrase_text for p in repo.get_all_phrases()]

# 2. é€‰æ‹©ç§å­å˜é‡ï¼ˆä½¿ç”¨ä¿®æ­£åçš„é€‰æ‹©å™¨ï¼‰
selector = SeedVariableSelector()
seed_variables = selector.select_seeds(min_seed_count=20)

# 3. è¿­ä»£æå–
extractor = TemplateVariableExtractor(phrases, seed_variables)
templates, variables = extractor.iterative_extraction(max_rounds=3)

# 4. ç»“æœç»Ÿè®¡
print("\n=== æå–ç»“æœ ===")
print(f"æ¨¡æ¿æ€»æ•°: {len(templates)}")
print(f"å˜é‡æ€»æ•°: {len(variables)}")

print("\næ¨¡æ¿ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
for template in templates[:10]:
    print(f"  â€¢ {template}")

print("\nå˜é‡ç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰:")
for var in variables[:20]:
    print(f"  â€¢ {var}")
```

#### å˜é‡åˆ†ç±»ï¼ˆä½¿ç”¨LLMï¼‰

```python
# core/variable_classifier.py

from typing import List, Dict
from ai.client import get_llm_client
import json

def classify_variables_batch(variables: List[str], batch_size: int = 50) -> Dict[str, str]:
    """
    æ‰¹é‡å¯¹å˜é‡è¿›è¡Œåˆ†ç±»

    è¿”å›: {variable: category}
    """
    client = get_llm_client()

    results = {}

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(variables), batch_size):
        batch = variables[i:i+batch_size]

        prompt = f"""
Classify these keywords into categories:

Categories:
- function: actions/capabilities (compress, resize, convert, analyze)
- object: things being processed (image, video, pdf, text, data)
- channel: platforms/websites/apps (google, youtube, shopify, facebook)
- audience: target user groups (students, developers, marketers, beginners)
- other: none of the above

Keywords: {', '.join(batch)}

Return JSON format (keyword: category):
{{
  "compress": "function",
  "image": "object",
  "google": "channel",
  "students": "audience",
  "misc": "other"
}}

Only return JSON, no explanation.
"""

        response = client.chat([{"role": "user", "content": prompt}])

        # è§£æJSON
        batch_results = json.loads(response)
        results.update(batch_results)

    return results
```

#### æ•°æ®åº“è®¾è®¡

```sql
-- æ‰©å±•tokensè¡¨
ALTER TABLE tokens ADD COLUMN extraction_round INT;  -- ç¬¬å‡ è½®æå–
ALTER TABLE tokens ADD COLUMN template_count INT;    -- é€‚é…æ¨¡æ¿æ•°
ALTER TABLE tokens ADD COLUMN extraction_source VARCHAR(50);  -- æ¥æºï¼šnoise/existing/fallback

-- æ–°å¢templatesè¡¨
CREATE TABLE templates (
    template_id INT PRIMARY KEY AUTO_INCREMENT,
    template_text VARCHAR(500) NOT NULL,
    frequency INT DEFAULT 0,
    extraction_round INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE KEY idx_template_text (template_text)
);

-- æ¨¡æ¿-å˜é‡å…³è”è¡¨
CREATE TABLE template_variables (
    template_id INT,
    token_id INT,
    frequency INT DEFAULT 0,
    PRIMARY KEY (template_id, token_id),
    FOREIGN KEY (template_id) REFERENCES templates(template_id),
    FOREIGN KEY (token_id) REFERENCES tokens(token_id)
);
```

#### å®æ–½æ­¥éª¤

```
Week 1-2: ç§å­é€‰æ‹©ä¸è¿­ä»£æå–
  1. å®ç°SeedVariableSelectorï¼šcore/seed_selector.py
  2. å®ç°TemplateVariableExtractorï¼šcore/template_variable_extractor.py
  3. è¿è¡Œè¿­ä»£æå–ï¼špython scripts/extract_templates_variables.py
  4. éªŒè¯æå–è´¨é‡ï¼šäººå·¥æŸ¥çœ‹ç»“æœ

Week 3-4: å˜é‡åˆ†ç±»ä¸å…¥åº“
  1. å®ç°å˜é‡åˆ†ç±»ï¼šcore/variable_classifier.py
  2. æ‰¹é‡åˆ†ç±»æ‰€æœ‰å˜é‡ï¼ˆä½¿ç”¨LLMï¼‰
  3. äººå·¥æŠ½æ ·ä¿®æ­£åˆ†ç±»é”™è¯¯
  4. å…¥åº“ï¼šå†™å…¥tokensè¡¨

Week 5-6: æ•ˆæœéªŒè¯ä¸è°ƒæ•´
  1. ç»Ÿè®¡tokenè¦†ç›–ç‡ï¼šæå–å‰ vs æå–å
  2. åœ¨Phase 2èšç±»ä¸­ä½¿ç”¨æ–°token
  3. æ£€æŸ¥ç°‡çš„å¯è§£é‡Šæ€§æ˜¯å¦æå‡
  4. æ ¹æ®æ•ˆæœè°ƒæ•´å‚æ•°é‡æ–°æå–
```

#### æˆåŠŸæ ‡å‡†

```
âœ… Tokenæ•°é‡ä»26ä¸ªæ‰©å±•åˆ°200-500ä¸ª
âœ… Tokenè¦†ç›–ç‡æå‡åˆ°70%+
âœ… æå–å˜é‡çš„è´¨é‡é«˜ï¼ˆäººå·¥æŠ½æ ·100ä¸ªï¼Œå‡†ç¡®ç‡>80%ï¼‰
âœ… æ¯ä¸ªå˜é‡é€‚é…â‰¥3ä¸ªæ¨¡æ¿ï¼ˆè´¨é‡ä¿è¯ï¼‰
âœ… ç°‡çš„Topç‰¹å¾è¯æ›´ä¸°å¯Œã€æ›´å‡†ç¡®
```

---

### 2.3 æœç´¢æ„å›¾åˆ†ç±» ğŸŸ¡ æ¡ä»¶æ‰§è¡Œ

#### å‰ç½®æ¡ä»¶
ä»…å½“**Experiment Dåˆ¤å®šä¸ºâœ…**æ—¶æ‰å®æ–½ï¼ˆå¯»æ‰¾ç±»>70%ï¼Œç±»ä¼¼å›è¨€ï¼‰

#### ç›®æ ‡
ç»™æ¯ä¸ªç°‡å’Œéœ€æ±‚å¢åŠ æœç´¢æ„å›¾ç»´åº¦ï¼Œæ–¹ä¾¿ä»"æ„å›¾Ã—äº§å“ç±»å‹"çŸ©é˜µä¸­å¿«é€Ÿå®šä½é‡ç‚¹æ–¹å‘ã€‚

#### å®æ–½æ–¹æ¡ˆ

```python
# core/intent_classifier.py

from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class IntentClassification:
    """æ„å›¾åˆ†ç±»ç»“æœ"""
    intent: str
    confidence: str  # high/medium/low
    matched_keywords: List[str]
    reasoning: Optional[str] = None

class SearchIntentClassifier:
    """
    æœç´¢æ„å›¾åˆ†ç±»å™¨ï¼ˆè‹±æ–‡ç‰ˆï¼‰

    åŸºäºå›è¨€çš„6å¤§ç±»ï¼Œé€‚é…è‹±æ–‡å…³é”®è¯
    """

    # æ„å›¾å…³é”®è¯åº“ï¼ˆå‚è€ƒå›è¨€+è‹±æ–‡ç‰¹ç‚¹ï¼‰
    INTENT_KEYWORDS = {
        'find_tool': {
            'keywords': [
                'best', 'top', 'recommend', 'popular', 'tool',
                'software', 'app', 'platform', 'service', 'solution'
            ],
            'description': 'å¯»æ‰¾å·¥å…·/äº§å“/æœåŠ¡',
            'examples': ['best image compressor', 'top photo editor']
        },
        'compare': {
            'keywords': [
                'vs', 'versus', 'compare', 'comparison', 'difference',
                'alternative', 'instead', 'or', 'better than'
            ],
            'description': 'å¯¹æ¯”/æ›¿ä»£',
            'examples': ['photoshop vs gimp', 'slack alternative']
        },
        'learn_how': {
            'keywords': [
                'how to', 'tutorial', 'guide', 'learn', 'course',
                'steps', 'instructions', 'teach', 'training'
            ],
            'description': 'å­¦ä¹ /æ•™ç¨‹',
            'examples': ['how to compress image', 'excel tutorial']
        },
        'solve_problem': {
            'keywords': [
                'fix', 'error', 'not working', 'problem', 'issue',
                'troubleshoot', 'repair', 'broken', 'crash', 'bug'
            ],
            'description': 'è§£å†³é—®é¢˜/æ•…éšœ',
            'examples': ['chrome not working', 'fix wifi']
        },
        'find_free': {
            'keywords': [
                'free', 'open source', 'no cost', 'without payment',
                'freeware', 'gratis', 'libre'
            ],
            'description': 'å¯»æ‰¾å…è´¹èµ„æº',
            'examples': ['free video editor', 'open source CRM']
        },
        'inquiry_price': {
            'keywords': [
                'price', 'cost', 'pricing', 'how much', 'expensive',
                'cheap', 'affordable', 'budget'
            ],
            'description': 'è¯¢ä»·',
            'examples': ['photoshop price', 'zoom cost']
        }
    }

    def classify(self, text: str) -> IntentClassification:
        """
        å¯¹çŸ­è¯­æˆ–éœ€æ±‚è¿›è¡Œæ„å›¾åˆ†ç±»

        ç­–ç•¥ï¼š
        1. è§„åˆ™åŒ¹é…ï¼ˆå¿«é€Ÿã€å‡†ç¡®ï¼‰
        2. å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å› 'other' + low confidence
        """
        text_lower = text.lower()

        # è®°å½•åŒ¹é…ç»“æœ
        matches: Dict[str, List[str]] = {}

        for intent, config in self.INTENT_KEYWORDS.items():
            matched = []
            for keyword in config['keywords']:
                if keyword in text_lower:
                    matched.append(keyword)

            if matched:
                matches[intent] = matched

        # åˆ¤æ–­ç»“æœ
        if not matches:
            return IntentClassification(
                intent='other',
                confidence='low',
                matched_keywords=[],
                reasoning='No intent keywords matched'
            )

        # å¦‚æœåªåŒ¹é…1ä¸ªæ„å›¾ï¼Œé«˜ç½®ä¿¡åº¦
        if len(matches) == 1:
            intent = list(matches.keys())[0]
            return IntentClassification(
                intent=intent,
                confidence='high',
                matched_keywords=matches[intent],
                reasoning=f'Matched keywords: {", ".join(matches[intent])}'
            )

        # å¦‚æœåŒ¹é…å¤šä¸ªæ„å›¾ï¼Œé€‰æ‹©åŒ¹é…å…³é”®è¯æœ€å¤šçš„
        best_intent = max(matches.items(), key=lambda x: len(x[1]))

        return IntentClassification(
            intent=best_intent[0],
            confidence='medium',
            matched_keywords=best_intent[1],
            reasoning=f'Multiple intents matched, chose based on keyword count'
        )

    def classify_batch(self, texts: List[str]) -> List[IntentClassification]:
        """æ‰¹é‡åˆ†ç±»"""
        return [self.classify(text) for text in texts]

    def get_intent_distribution(self, texts: List[str]) -> Dict[str, int]:
        """
        ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ

        ç”¨äºPhase 0åŸºçº¿æµ‹é‡
        """
        classifications = self.classify_batch(texts)

        distribution = {}
        for cls in classifications:
            intent = cls.intent
            distribution[intent] = distribution.get(intent, 0) + 1

        return distribution

# ä½¿ç”¨ç¤ºä¾‹
classifier = SearchIntentClassifier()

# å•ä¸ªåˆ†ç±»
result = classifier.classify("best image compressor online")
print(f"Intent: {result.intent}")
print(f"Confidence: {result.confidence}")
print(f"Matched: {result.matched_keywords}")

# æ‰¹é‡ç»Ÿè®¡
phrases = [
    "best image compressor",
    "photoshop vs gimp",
    "how to compress pdf",
    "zoom not working",
    "free video editor",
    "excel price"
]

distribution = classifier.get_intent_distribution(phrases)
print("\nIntent Distribution:")
for intent, count in distribution.items():
    print(f"  {intent}: {count}")
```

#### ç°‡çº§åˆ«æ„å›¾èšåˆ

```python
# core/cluster_intent_aggregator.py

from typing import Dict
from storage.repository import ClusterRepository
from core.intent_classifier import SearchIntentClassifier

class ClusterIntentAggregator:
    """
    ç°‡çº§åˆ«æ„å›¾èšåˆå™¨

    ä¸ºæ¯ä¸ªç°‡è®¡ç®—ä¸»å¯¼æ„å›¾
    """

    def __init__(self):
        self.classifier = SearchIntentClassifier()

    def aggregate_cluster_intent(self, cluster_id: int) -> Dict:
        """
        è®¡ç®—ç°‡çš„ä¸»å¯¼æ„å›¾

        é€»è¾‘ï¼š
        1. è·å–ç°‡å†…æ‰€æœ‰çŸ­è¯­
        2. å¯¹æ¯ä¸ªçŸ­è¯­åˆ†ç±»
        3. ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ
        4. é€‰æ‹©å æ¯”æœ€é«˜çš„æ„å›¾ä½œä¸ºä¸»å¯¼æ„å›¾
        """
        with ClusterRepository() as repo:
            phrases = repo.get_cluster_phrases(cluster_id)

        # åˆ†ç±»
        texts = [p.phrase_text for p in phrases]
        classifications = self.classifier.classify_batch(texts)

        # ç»Ÿè®¡åˆ†å¸ƒ
        distribution = {}
        for cls in classifications:
            intent = cls.intent
            distribution[intent] = distribution.get(intent, 0) + 1

        # è®¡ç®—ç™¾åˆ†æ¯”
        total = len(phrases)
        distribution_pct = {
            intent: (count / total) * 100
            for intent, count in distribution.items()
        }

        # ä¸»å¯¼æ„å›¾ï¼ˆå æ¯”æœ€é«˜ï¼‰
        dominant_intent = max(distribution.items(), key=lambda x: x[1])

        return {
            'cluster_id': cluster_id,
            'dominant_intent': dominant_intent[0],
            'dominant_intent_pct': distribution_pct[dominant_intent[0]],
            'distribution': distribution,
            'distribution_pct': distribution_pct
        }

    def aggregate_all_clusters(self):
        """
        æ‰¹é‡èšåˆæ‰€æœ‰ç°‡çš„æ„å›¾
        """
        with ClusterRepository() as repo:
            clusters = repo.get_all_clusters()

        results = []
        for cluster in clusters:
            result = self.aggregate_cluster_intent(cluster.cluster_id)
            results.append(result)

            # æ›´æ–°æ•°æ®åº“
            repo.update_cluster_intent(
                cluster_id=cluster.cluster_id,
                dominant_intent=result['dominant_intent'],
                intent_distribution=result['distribution_pct']
            )

        return results
```

#### æ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µè§†å›¾

```python
# ui/pages/intent_product_matrix.py

import streamlit as st
import pandas as pd
from storage.repository import DemandRepository

def show_intent_product_matrix():
    """
    æ˜¾ç¤ºæ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µ

    ç±»ä¼¼å›è¨€çš„"æ•´ä½“è§†å›¾"
    """
    st.title("ğŸ¯ æ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µ")

    with DemandRepository() as repo:
        demands = repo.get_all_demands()

    # æ„å»ºçŸ©é˜µ
    matrix_data = {}

    intents = ['find_tool', 'compare', 'learn_how', 'solve_problem', 'find_free', 'other']
    product_types = ['tool', 'content', 'service', 'education', 'other']

    for intent in intents:
        matrix_data[intent] = {}
        for ptype in product_types:
            count = len([
                d for d in demands
                if d.search_intent == intent and d.demand_type == ptype
            ])
            matrix_data[intent][ptype] = count

    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(matrix_data).T

    # æ˜¾ç¤ºçƒ­åŠ›å›¾
    st.subheader("éœ€æ±‚åˆ†å¸ƒçƒ­åŠ›å›¾")
    st.dataframe(
        df.style.background_gradient(cmap='YlOrRd', axis=None),
        use_container_width=True
    )

    # ç­›é€‰å™¨
    st.subheader("ç­›é€‰éœ€æ±‚")

    col1, col2 = st.columns(2)

    with col1:
        selected_intent = st.selectbox(
            "é€‰æ‹©æ„å›¾",
            ['å…¨éƒ¨'] + intents
        )

    with col2:
        selected_ptype = st.selectbox(
            "é€‰æ‹©äº§å“ç±»å‹",
            ['å…¨éƒ¨'] + product_types
        )

    # è¿‡æ»¤éœ€æ±‚
    filtered_demands = demands

    if selected_intent != 'å…¨éƒ¨':
        filtered_demands = [
            d for d in filtered_demands
            if d.search_intent == selected_intent
        ]

    if selected_ptype != 'å…¨éƒ¨':
        filtered_demands = [
            d for d in filtered_demands
            if d.demand_type == selected_ptype
        ]

    st.write(f"æ‰¾åˆ° {len(filtered_demands)} ä¸ªéœ€æ±‚")

    # æ˜¾ç¤ºéœ€æ±‚åˆ—è¡¨
    for demand in filtered_demands[:20]:
        st.write(f"- {demand.summary}")
```

#### å®æ–½æ­¥éª¤

```
Week 1: åˆ†ç±»å™¨å®ç°ä¸éªŒè¯
  1. å®ç°SearchIntentClassifierï¼šcore/intent_classifier.py
  2. åœ¨Phase 0æŠ½æ ·çš„1000æ¡ä¸Šæµ‹è¯•å‡†ç¡®ç‡
  3. æ ¹æ®åé¦ˆè°ƒæ•´å…³é”®è¯åº“

Week 2: æ‰¹é‡åˆ†ç±»ä¸å…¥åº“
  1. å¯¹æ‰€æœ‰çŸ­è¯­åˆ†ç±»ï¼špython scripts/classify_intents.py
  2. å¯¹æ‰€æœ‰ç°‡èšåˆæ„å›¾ï¼šClusterIntentAggregator
  3. å¯¹æ‰€æœ‰éœ€æ±‚åˆ†ç±»
  4. æ›´æ–°æ•°æ®åº“

Week 3: UIå®ç°
  1. å®ç°æ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µï¼šui/pages/intent_product_matrix.py
  2. åœ¨ç°‡åˆ—è¡¨é¡µæ˜¾ç¤ºæ„å›¾æ ‡ç­¾
  3. åœ¨éœ€æ±‚å¡ç‰‡æ˜¾ç¤ºæ„å›¾
  4. æµ‹è¯•ç­›é€‰åŠŸèƒ½
```

#### æˆåŠŸæ ‡å‡†

```
âœ… æ„å›¾åˆ†ç±»å‡†ç¡®ç‡ > 75%ï¼ˆäººå·¥æŠ½æ ·200æ¡éªŒè¯ï¼‰
âœ… ç°‡çš„ä¸»å¯¼æ„å›¾è¯†åˆ«å‡†ç¡®ç‡ > 80%
âœ… æ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µèƒ½æ¸…æ™°æ˜¾ç¤ºåˆ†å¸ƒ
âœ… èƒ½å¿«é€Ÿç­›é€‰ç‰¹å®šæ„å›¾+ç±»å‹çš„éœ€æ±‚
```

---

## Phase 3: ç»“æœè§†å›¾ä¸å¤ç›˜ï¼ˆ1å‘¨ï¼‰

### ç›®æ ‡
æ¯è·‘ä¸€æ¬¡å®Œæ•´æµç¨‹ï¼Œä¸åªæ˜¯çœ‹å®Œå°±å®Œï¼Œè€Œæ˜¯æ²‰æ·€ä¸ºä¸€å¼ "å¯å¤ç›˜çš„åˆ†ææŠ¥å‘Š"ã€‚

### Run Overviewï¼ˆè¿è¡Œæ¦‚è§ˆï¼‰è®¾è®¡

#### 3.1 è‡ªåŠ¨ç”ŸæˆRunæŠ¥å‘Š

```python
# core/run_reporter.py

from datetime import datetime
from typing import Dict, List
from storage.repository import (
    ClusterRepository, DemandRepository,
    TokenRepository, PhraseRepository
)

class RunReporter:
    """
    è¿è¡ŒæŠ¥å‘Šç”Ÿæˆå™¨

    æ¯æ¬¡å®Œæ•´æµç¨‹ç»“æŸåï¼Œè‡ªåŠ¨ç”Ÿæˆæ¦‚è§ˆæŠ¥å‘Š
    """

    def generate_report(self, run_id: int) -> Dict:
        """
        ç”ŸæˆRunæŠ¥å‘Š

        åŒ…å«ï¼š
        1. èšç±»æ€»ä½“æƒ…å†µ
        2. æ„å›¾åˆ†å¸ƒ
        3. Topç‰¹å¾è¯
        4. é«˜ä»·å€¼éœ€æ±‚
        """
        report = {
            'run_id': run_id,
            'generated_at': datetime.now().isoformat(),
            'sections': {}
        }

        # === Section 1: èšç±»æ€»ä½“æƒ…å†µ ===
        report['sections']['clustering'] = self._generate_clustering_section()

        # === Section 2: æ„å›¾åˆ†å¸ƒ ===
        report['sections']['intent_distribution'] = self._generate_intent_section()

        # === Section 3: Topç‰¹å¾è¯ ===
        report['sections']['top_tokens'] = self._generate_token_section()

        # === Section 4: é«˜ä»·å€¼éœ€æ±‚ ===
        report['sections']['high_value_demands'] = self._generate_demand_section()

        return report

    def _generate_clustering_section(self) -> Dict:
        """èšç±»æ€»ä½“æƒ…å†µ"""
        with ClusterRepository() as repo:
            clusters = repo.get_all_clusters()
            selected_clusters = repo.get_selected_clusters()

        with PhraseRepository() as repo:
            total_phrases = repo.count_all_phrases()

        return {
            'total_phrases': total_phrases,
            'total_clusters': len(clusters),
            'selected_clusters': len(selected_clusters),
            'selection_rate': f"{len(selected_clusters) / len(clusters) * 100:.1f}%",
            'avg_cluster_size': sum(c.size for c in clusters) / len(clusters),
            'largest_cluster_size': max(c.size for c in clusters),
            'smallest_cluster_size': min(c.size for c in clusters)
        }

    def _generate_intent_section(self) -> Dict:
        """æ„å›¾åˆ†å¸ƒ"""
        with DemandRepository() as repo:
            demands = repo.get_all_demands()

        # ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ
        intent_counts = {}
        for demand in demands:
            intent = demand.search_intent or 'unknown'
            intent_counts[intent] = intent_counts.get(intent, 0) + 1

        # æ’åº
        sorted_intents = sorted(
            intent_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return {
            'total_demands': len(demands),
            'intent_distribution': {
                intent: {
                    'count': count,
                    'percentage': f"{count / len(demands) * 100:.1f}%"
                }
                for intent, count in sorted_intents
            }
        }

    def _generate_token_section(self) -> Dict:
        """Topç‰¹å¾è¯"""
        with TokenRepository() as repo:
            tokens = repo.get_all_tokens()

        # æŒ‰é¢‘æ¬¡æ’åº
        sorted_tokens = sorted(
            tokens,
            key=lambda t: t.total_frequency,
            reverse=True
        )[:50]

        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = {}
        for token in sorted_tokens:
            category = token.token_category or 'other'
            if category not in by_category:
                by_category[category] = []
            by_category[category].append({
                'token': token.token_text,
                'frequency': token.total_frequency,
                'cluster_count': token.cluster_count
            })

        return {
            'total_tokens': len(tokens),
            'top_50_tokens': by_category
        }

    def _generate_demand_section(self) -> Dict:
        """é«˜ä»·å€¼éœ€æ±‚"""
        with DemandRepository() as repo:
            demands = repo.get_all_demands()

        # ç­›é€‰é«˜ä»·å€¼éœ€æ±‚
        high_value = [
            d for d in demands
            if d.value_score == 'high'
        ]

        # æŒ‰å¯è¡Œæ€§åˆ†ç»„
        by_feasibility = {
            'easy': [],
            'medium': [],
            'hard': []
        }

        for demand in high_value:
            feasibility = demand.feasibility_score or 'medium'
            by_feasibility[feasibility].append({
                'summary': demand.summary,
                'intent': demand.search_intent,
                'type': demand.demand_type
            })

        return {
            'total_demands': len(demands),
            'high_value_count': len(high_value),
            'high_value_by_feasibility': {
                feasibility: {
                    'count': len(demands),
                    'examples': demands[:5]  # å‰5ä¸ªç¤ºä¾‹
                }
                for feasibility, demands in by_feasibility.items()
            }
        }

    def save_report(self, report: Dict, output_path: str):
        """ä¿å­˜æŠ¥å‘Šä¸ºMarkdown"""
        md_content = self._render_markdown(report)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    def _render_markdown(self, report: Dict) -> str:
        """æ¸²æŸ“Markdownæ ¼å¼æŠ¥å‘Š"""
        lines = []

        lines.append(f"# Run Report #{report['run_id']}")
        lines.append(f"\nç”Ÿæˆæ—¶é—´: {report['generated_at']}\n")

        # Section 1: èšç±»
        clustering = report['sections']['clustering']
        lines.append("## 1. èšç±»æ€»ä½“æƒ…å†µ\n")
        lines.append(f"- æ€»çŸ­è¯­æ•°: {clustering['total_phrases']:,}")
        lines.append(f"- ç”Ÿæˆç°‡æ•°: {clustering['total_clusters']}")
        lines.append(f"- é€‰ä¸­ç°‡æ•°: {clustering['selected_clusters']}")
        lines.append(f"- é€‰æ‹©ç‡: {clustering['selection_rate']}")
        lines.append(f"- å¹³å‡ç°‡å¤§å°: {clustering['avg_cluster_size']:.1f}")
        lines.append(f"- æœ€å¤§ç°‡: {clustering['largest_cluster_size']}")
        lines.append(f"- æœ€å°ç°‡: {clustering['smallest_cluster_size']}\n")

        # Section 2: æ„å›¾åˆ†å¸ƒ
        intent = report['sections']['intent_distribution']
        lines.append("## 2. æ„å›¾åˆ†å¸ƒ\n")
        lines.append(f"æ€»éœ€æ±‚æ•°: {intent['total_demands']}\n")
        for intent_type, data in intent['intent_distribution'].items():
            lines.append(f"- {intent_type}: {data['count']} ({data['percentage']})")
        lines.append("")

        # Section 3: Topç‰¹å¾è¯
        token = report['sections']['top_tokens']
        lines.append("## 3. Topç‰¹å¾è¯\n")
        lines.append(f"æ€»tokenæ•°: {token['total_tokens']}\n")
        for category, tokens in token['top_50_tokens'].items():
            lines.append(f"### {category.upper()}\n")
            for t in tokens[:10]:
                lines.append(f"- {t['token']} (é¢‘æ¬¡: {t['frequency']}, ç°‡æ•°: {t['cluster_count']})")
            lines.append("")

        # Section 4: é«˜ä»·å€¼éœ€æ±‚
        demand = report['sections']['high_value_demands']
        lines.append("## 4. é«˜ä»·å€¼éœ€æ±‚\n")
        lines.append(f"æ€»éœ€æ±‚æ•°: {demand['total_demands']}")
        lines.append(f"é«˜ä»·å€¼éœ€æ±‚: {demand['high_value_count']}\n")
        for feasibility, data in demand['high_value_by_feasibility'].items():
            lines.append(f"### {feasibility.upper()} ({data['count']}ä¸ª)\n")
            for ex in data['examples']:
                lines.append(f"- {ex['summary']} ({ex['intent']} / {ex['type']})")
            lines.append("")

        return '\n'.join(lines)

# ä½¿ç”¨ç¤ºä¾‹
reporter = RunReporter()
report = reporter.generate_report(run_id=1)
reporter.save_report(report, 'reports/run_001_report.md')
```

#### 3.2 UIå±•ç¤ºRunå†å²

```python
# ui/pages/run_history.py

import streamlit as st
import os
import glob

def show_run_history():
    """
    æ˜¾ç¤ºå†å²RunæŠ¥å‘Š
    """
    st.title("ğŸ“Š Runå†å²è®°å½•")

    # æŸ¥æ‰¾æ‰€æœ‰æŠ¥å‘Š
    reports_dir = 'reports'
    report_files = sorted(
        glob.glob(f"{reports_dir}/run_*_report.md"),
        reverse=True
    )

    if not report_files:
        st.info("æš‚æ— å†å²æŠ¥å‘Š")
        return

    # æ˜¾ç¤ºæŠ¥å‘Šåˆ—è¡¨
    st.subheader(f"å…± {len(report_files)} æ¬¡è¿è¡Œè®°å½•")

    for report_file in report_files:
        # æå–run_id
        basename = os.path.basename(report_file)
        run_id = basename.split('_')[1]

        with st.expander(f"Run #{run_id}"):
            # è¯»å–æŠ¥å‘Šå†…å®¹
            with open(report_file, 'r', encoding='utf-8') as f:
                content = f.read()

            st.markdown(content)

            # ä¸‹è½½æŒ‰é’®
            st.download_button(
                label="ä¸‹è½½æŠ¥å‘Š",
                data=content,
                file_name=basename,
                mime='text/markdown'
            )
```

### å®æ–½æ­¥éª¤

```
Week 1: æŠ¥å‘Šç”Ÿæˆå™¨å®ç°
  1. å®ç°RunReporterï¼šcore/run_reporter.py
  2. ç¼–å†™æŠ¥å‘Šç”Ÿæˆè„šæœ¬ï¼šscripts/generate_run_report.py
  3. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

Week 1: UIé›†æˆ
  1. å®ç°Runå†å²é¡µé¢ï¼šui/pages/run_history.py
  2. åœ¨ä¸»é¡µå¢åŠ "ç”ŸæˆæŠ¥å‘Š"æŒ‰é’®
  3. æµ‹è¯•å®Œæ•´æµç¨‹
```

### æˆåŠŸæ ‡å‡†

```
âœ… æ¯æ¬¡è¿è¡Œåè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
âœ… æŠ¥å‘ŠåŒ…å«4ä¸ªæ ¸å¿ƒsection
âœ… æŠ¥å‘Šæ ¼å¼æ¸…æ™°æ˜“è¯»
âœ… UIèƒ½æŸ¥çœ‹å’Œä¸‹è½½å†å²æŠ¥å‘Š
```

---

## ğŸ“… å®æ–½æ—¶é—´çº¿ï¼ˆæ€»è®¡10-13å‘¨ï¼‰

### Week 1: Phase 0 åŸºçº¿æµ‹é‡ â­â­â­â­â­

```
ç›®æ ‡ï¼šå»ºç«‹è¯æ®åŸºç¡€
ä»»åŠ¡ï¼š
  âœ… Experiment A: èšç±»å®¡æ ¸æˆæœ¬æµ‹é‡
  âœ… Experiment B: Tokenè¦†ç›–ç‡æµ‹é‡
  âœ… Experiment C: æ•°æ®å†—ä½™ç‡æµ‹é‡
  âœ… Experiment D: æœç´¢æ„å›¾åˆ†å¸ƒç»Ÿè®¡
  âœ… ç”ŸæˆåŸºçº¿æŠ¥å‘Š
  âœ… æ ¹æ®å†³ç­–æ ‘ç¡®å®šåç»­ä¼˜åŒ–æ–¹å‘

å…³é”®äº§å‡ºï¼š
  ã€Šè‹±æ–‡èšç±»ç³»ç»ŸåŸºçº¿æŠ¥å‘Š-2025-12.mdã€‹

å†³ç­–ç‚¹ï¼š
  æ ¹æ®4ä¸ªå®éªŒçš„åˆ¤å®šç»“æœï¼Œå†³å®šPhase 1å’ŒPhase 2çš„å…·ä½“ä»»åŠ¡
```

### Week 2-4: Phase 1 åŸºç¡€è®¾æ–½ï¼ˆ2-3å‘¨ï¼‰

```
Week 2-3: Phase 1.1 éœ€æ±‚å¡ç‰‡ç»“æ„ç»Ÿä¸€ï¼ˆå¿…åšï¼‰
  âœ… æ•°æ®æ¨¡å‹æ‰©å±•
  âœ… LLMæå–äº”ç±»è¯
  âœ… æ‰¹é‡å¤„ç†ç°æœ‰éœ€æ±‚
  âœ… UIå±•ç¤ºä¸ç­›é€‰

Week 3-4: Phase 1.2 èšç±»è´¨é‡è¯„åˆ†ï¼ˆæ¡ä»¶æ‰§è¡Œï¼‰
  å‰ç½®æ¡ä»¶ï¼šExperiment Aåˆ¤å®šä¸ºâŒæˆ–ğŸŸ¡
  âœ… è¯„åˆ†ç®—æ³•å®ç°
  âœ… UIæ¨èç°‡åŒºåŸŸ
  âœ… æ•ˆæœéªŒè¯

  å¦‚æœPhase 0åˆ¤å®šä¸éœ€è¦ï¼Œè·³è¿‡æ­¤æ­¥
```

### Week 5-12: Phase 2 æŒ‰éœ€ä¼˜åŒ–ï¼ˆ6-8å‘¨ï¼‰

```
æ ¹æ®Phase 0ç»“æœï¼Œé€‰æ‹©1-2ä¸ªä¼˜åŒ–æ–¹å‘å®æ–½ï¼š

é€‰é¡¹1ï¼šè¯çº§è§„èŒƒåŒ–å»é‡ï¼ˆ2-3å‘¨ï¼‰
  å‰ç½®æ¡ä»¶ï¼šå†—ä½™ç‡ > 20%
  Week 5-6: ç®—æ³•å®ç°ä¸æµ‹è¯•
  Week 7: æ‰¹é‡å¤„ç†ä¸éªŒè¯

é€‰é¡¹2ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆ4-5å‘¨ï¼‰
  å‰ç½®æ¡ä»¶ï¼šTokenè¦†ç›–ç‡ < 60%
  Week 5-6: ç§å­é€‰æ‹©ä¸è¿­ä»£æå–
  Week 7-8: å˜é‡åˆ†ç±»ä¸å…¥åº“
  Week 9: æ•ˆæœéªŒè¯ä¸è°ƒæ•´

é€‰é¡¹3ï¼šæœç´¢æ„å›¾åˆ†ç±»ï¼ˆ1-2å‘¨ï¼‰
  å‰ç½®æ¡ä»¶ï¼šå¯»æ‰¾ç±» > 70%
  Week 5: åˆ†ç±»å™¨å®ç°ä¸éªŒè¯
  Week 6: æ‰¹é‡åˆ†ç±»ä¸UIå®ç°

å®é™…æ‰§è¡Œï¼š
  - å¦‚æœåªéœ€è¦1ä¸ªä¼˜åŒ–ï¼š5-7å‘¨
  - å¦‚æœéœ€è¦2ä¸ªä¼˜åŒ–ï¼š8-10å‘¨
  - å¦‚æœéƒ½éœ€è¦ï¼š10-12å‘¨
```

### Week 12-13: Phase 3 ç»“æœè§†å›¾ä¸å¤ç›˜ï¼ˆ1å‘¨ï¼‰

```
Week 12: RunæŠ¥å‘Šå®ç°
  âœ… RunReporterå®ç°
  âœ… UIé›†æˆ
  âœ… æµ‹è¯•å®Œæ•´æµç¨‹

Week 13: å¤ç›˜ä¸æ€»ç»“
  âœ… å¯¹æ¯”ä¼˜åŒ–å‰åæ•ˆæœ
  âœ… è®°å½•ç»éªŒæ•™è®­
  âœ… è§„åˆ’ä¸‹ä¸€é˜¶æ®µæ–¹å‘
```

### âš ï¸ æ—¶é—´ä¿®æ­£è¯´æ˜

**GPTä¼°ç®—ï¼š6å‘¨**
**ç°å®ä¼°ç®—ï¼š10-13å‘¨**

**å·®å¼‚åŸå› **ï¼š

1. **ç®—æ³•è°ƒè¯•æ—¶é—´**ï¼ˆ+2å‘¨ï¼‰
   - è¯çº§è§„èŒƒåŒ–éœ€è¦å¤šæ¬¡è°ƒå‚
   - æ¨¡æ¿-å˜é‡è¿­ä»£éœ€è¦éªŒè¯è´¨é‡
   - æ¯æ¬¡è°ƒæ•´åéœ€è¦é‡æ–°è¿è¡Œ

2. **LLMæ¥å£è°ƒç”¨å»¶è¿Ÿ**ï¼ˆ+1å‘¨ï¼‰
   - æ‰¹é‡è°ƒç”¨LLMåˆ†ç±»æ•°åƒä¸ªå˜é‡
   - éœ€è¦æ§åˆ¶è°ƒç”¨é€Ÿç‡ï¼ˆé¿å…è¶…é™ï¼‰
   - å¯èƒ½æœ‰å¤±è´¥é‡è¯•

3. **äººå·¥éªŒè¯ç¯èŠ‚**ï¼ˆ+1å‘¨ï¼‰
   - æ¯ä¸ªé˜¶æ®µéƒ½éœ€è¦æŠ½æ ·éªŒè¯
   - éªŒè¯200-500æ¡æ ·æœ¬éœ€è¦æ—¶é—´
   - å¯èƒ½éœ€è¦è°ƒæ•´åé‡æ–°éªŒè¯

4. **æ•°æ®åº“è¿ç§»æˆæœ¬**ï¼ˆ+0.5å‘¨ï¼‰
   - æ–°å¢å¤šå¼ è¡¨å’Œå­—æ®µ
   - æ•°æ®è¿ç§»è„šæœ¬ç¼–å†™ä¸æµ‹è¯•
   - å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜

5. **Phase 0åŸºçº¿æµ‹é‡**ï¼ˆ+1å‘¨ï¼‰
   - GPTæœªå……åˆ†è€ƒè™‘æµ‹é‡æ—¶é—´
   - 4ä¸ªå®éªŒéœ€è¦é€ä¸€å®Œæˆ
   - äººå·¥æ ‡æ³¨1000+æ¡æ ·æœ¬éœ€è¦æ—¶é—´

**ç»“è®º**ï¼š10-13å‘¨æ˜¯æ›´ç°å®çš„ä¼°ç®—

---

## âš ï¸ é£é™©ä¸ç¼“è§£

### é£é™©1ï¼šè¯å½¢è¿˜åŸè¿‡åº¦å½’å¹¶

**é—®é¢˜æè¿°**ï¼š
```
è¯å½¢è¿˜åŸå¯èƒ½å¯¼è‡´è¯­ä¹‰æ··æ·†ï¼š
  stem("compressor") = "compress"
  stem("compression") = "compress"
  â†’ "image compressor"ï¼ˆå·¥å…·ï¼‰å’Œ"image compression"ï¼ˆæŠ€æœ¯ï¼‰è¢«è¯¯åˆå¹¶
```

**ç¼“è§£æªæ–½**ï¼š
```
1. ä½¿ç”¨å¯é…ç½®çš„è¿˜åŸç­–ç•¥
   - é…ç½®é€‰é¡¹ï¼šenable_lemmatization (True/False)
   - é»˜è®¤ï¼šFalseï¼ˆä¸è¿˜åŸï¼Œæ›´ä¿å®ˆï¼‰
   - ä»…åœ¨å†—ä½™ç‡>25%æ—¶è€ƒè™‘å¼€å¯

2. äººå·¥æŠ½æ ·éªŒè¯
   - æ¯æ¬¡è§„èŒƒåŒ–åæŠ½æ ·200æ¡
   - æ£€æŸ¥false positiveç‡
   - å¦‚æœ>5%ï¼Œå…³é—­è¯å½¢è¿˜åŸé‡æ–°è·‘

3. ä¿ç•™åŸå§‹æ•°æ®
   - canonical_idä»…ç”¨äºå»é‡ç»Ÿè®¡
   - åŸå§‹çŸ­è¯­ä¿ç•™åœ¨phrasesè¡¨
   - å¯ä»¥éšæ—¶å›æ»š
```

### é£é™©2ï¼šæ¨¡æ¿æå–å™ªå£°å¤š

**é—®é¢˜æè¿°**ï¼š
```
ä½é¢‘æ¨¡æ¿å¯èƒ½æ— æ„ä¹‰ï¼š
  "[X] best for" (é¢‘æ¬¡=3)
  "online [X] free" (é¢‘æ¬¡=4)
  â†’ è¿™äº›æ¨¡æ¿ç‰¹å¼‚æ€§å¤ªå¼ºï¼Œæå–çš„å˜é‡è´¨é‡ä½
```

**ç¼“è§£æªæ–½**ï¼š
```
1. è®¾ç½®åˆç†çš„é¢‘æ¬¡é˜ˆå€¼
   - min_template_freq = 5ï¼ˆæœ€å°‘5æ¬¡ï¼‰
   - æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´ï¼ˆ10ä¸‡æ•°æ®â†’5æ¬¡ï¼Œ50ä¸‡æ•°æ®â†’10æ¬¡ï¼‰

2. äººå·¥éªŒè¯é«˜é¢‘æ¨¡æ¿
   - æå–top 100æ¨¡æ¿äººå·¥å®¡æ ¸
   - åˆ é™¤æ— æ„ä¹‰æ¨¡æ¿ï¼ˆå¦‚ï¼š"[X] the"ï¼‰
   - æ·»åŠ åˆ°æ¨¡æ¿é»‘åå•

3. å˜é‡è´¨é‡è¿‡æ»¤ï¼ˆå›è¨€çš„å…³é”®ç»éªŒï¼‰
   - å˜é‡å¿…é¡»é€‚é… â‰¥3ä¸ªæ¨¡æ¿
   - è¿™èƒ½æœ‰æ•ˆè¿‡æ»¤å™ªå£°
```

### é£é™©3ï¼šLLMåˆ†ç±»ä¸ç¨³å®š

**é—®é¢˜æè¿°**ï¼š
```
åŒä¸€å˜é‡å¤šæ¬¡åˆ†ç±»å¯èƒ½ä¸åŒï¼š
  ç¬¬1æ¬¡ï¼šclassify("python") â†’ "channel"
  ç¬¬2æ¬¡ï¼šclassify("python") â†’ "object"
  â†’ åˆ†ç±»ä¸ä¸€è‡´å¯¼è‡´æ··ä¹±
```

**ç¼“è§£æªæ–½**ï¼š
```
1. ä½¿ç”¨ç¡®å®šæ€§é‡‡æ ·
   - temperature = 0ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰
   - ç›¸åŒè¾“å…¥æ€»æ˜¯å¾—åˆ°ç›¸åŒç»“æœ

2. æ‰¹é‡åˆ†ç±»è€Œéé€ä¸ª
   - ä¸€æ¬¡æ€§åˆ†ç±»50ä¸ªå˜é‡
   - ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§
   - å‡å°‘è°ƒç”¨æ¬¡æ•°

3. è§„åˆ™+LLMåŒé‡éªŒè¯
   - å…ˆç”¨è§„åˆ™åŒ¹é…æ˜æ˜¾çš„ç±»åˆ«
     å¦‚ï¼š"compress"åŒ…å«åŠ¨è¯åç¼€ â†’ function
   - ä»…å¯¹ä¸ç¡®å®šçš„ç”¨LLM
   - å‡å°‘LLMå‡ºé”™æ¦‚ç‡

4. äººå·¥æŠ½æ ·ä¿®æ­£
   - æŠ½æ ·100ä¸ªLLMåˆ†ç±»ç»“æœ
   - äººå·¥ä¿®æ­£é”™è¯¯
   - æ ¹æ®é”™è¯¯æ¨¡å¼è°ƒæ•´prompt
   - é‡æ–°æ‰¹é‡åˆ†ç±»
```

### é£é™©4ï¼šèšç±»ç»“æœä¸ç¨³å®š

**é—®é¢˜æè¿°**ï¼š
```
HDBSCANæœ‰éšæœºæ€§ï¼Œä¸¤æ¬¡è¿è¡Œå¯èƒ½ç»“æœä¸åŒï¼š
  Run 1: 85ä¸ªç°‡
  Run 2: 92ä¸ªç°‡
  â†’ éš¾ä»¥å¯¹æ¯”ä¼˜åŒ–å‰åæ•ˆæœ
```

**ç¼“è§£æªæ–½**ï¼š
```
1. å›ºå®šéšæœºç§å­
   - np.random.seed(42)
   - ä¿è¯èšç±»ç»“æœå¯å¤ç°

2. ä½¿ç”¨ç›¸åŒæ•°æ®é›†å¯¹æ¯”
   - ä¿å­˜ä¸€ä»½"åŸºå‡†æ•°æ®é›†"
   - ä¼˜åŒ–å‰åéƒ½åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•
   - ç¡®ä¿å¯¹æ¯”å…¬å¹³

3. å¤šæ¬¡è¿è¡Œå–å¹³å‡
   - æ¯ä¸ªé…ç½®è¿è¡Œ3æ¬¡
   - å–å¹³å‡æŒ‡æ ‡
   - å‡å°‘éšæœºæ€§å½±å“
```

### é£é™©5ï¼šæ•°æ®åº“è¿ç§»å¤±è´¥

**é—®é¢˜æè¿°**ï¼š
```
æ–°å¢å­—æ®µå’Œè¡¨å¯èƒ½ä¸ç°æœ‰æ•°æ®å†²çªï¼š
  - å­—æ®µç±»å‹ä¸å…¼å®¹
  - å¤–é”®çº¦æŸå¤±è´¥
  - æ•°æ®ä¸¢å¤±
```

**ç¼“è§£æªæ–½**ï¼š
```
1. è¿ç§»å‰å¤‡ä»½
   - å®Œæ•´å¤‡ä»½æ•°æ®åº“
   - ä¿ç•™å¤‡ä»½è‡³å°‘7å¤©

2. è¿ç§»è„šæœ¬æµ‹è¯•
   - å…ˆåœ¨æµ‹è¯•ç¯å¢ƒè¿è¡Œ
   - éªŒè¯æ•°æ®å®Œæ•´æ€§
   - ç¡®è®¤æ— è¯¯ååœ¨ç”Ÿäº§ç¯å¢ƒæ‰§è¡Œ

3. åˆ†æ­¥è¿ç§»
   - å…ˆæ·»åŠ å¯ç©ºå­—æ®µ
   - å¡«å……æ•°æ®
   - å†æ·»åŠ çº¦æŸ
   - é€æ­¥æ¨è¿›ï¼Œé™ä½é£é™©

4. å›æ»šæ–¹æ¡ˆ
   - ç¼–å†™å›æ»šSQLè„šæœ¬
   - æµ‹è¯•å›æ»šæµç¨‹
   - ç¡®ä¿éšæ—¶å¯ä»¥å›é€€
```

---

## âœ… æˆåŠŸæ ‡å‡†

### Phase 0 å®Œæˆæ ‡å‡†

```
âœ… 4ä¸ªå®éªŒæŠ¥å‘Šå®Œæ•´ï¼š
  - Experiment A: èšç±»å®¡æ ¸ï¼ˆæ—¶é—´ã€é—æ¼ç‡ã€æ„Ÿå—ï¼‰
  - Experiment B: Tokenè¦†ç›–ç‡ï¼ˆç™¾åˆ†æ¯”ã€æœªè¦†ç›–æ ·æœ¬ï¼‰
  - Experiment C: æ•°æ®å†—ä½™ç‡ï¼ˆç™¾åˆ†æ¯”ã€å†—ä½™æ¨¡å¼ï¼‰
  - Experiment D: æ„å›¾åˆ†å¸ƒï¼ˆå„ç±»å æ¯”ï¼‰

âœ… å†³ç­–æ ‘æ¸…æ™°ï¼š
  - æ˜ç¡®æ ‡æ³¨å“ªäº›ä¼˜åŒ–éœ€è¦åšï¼ˆâœ…ï¼‰
  - æ˜ç¡®æ ‡æ³¨å“ªäº›ä¼˜åŒ–æš‚ä¸åšï¼ˆâŒï¼‰
  - ç»™å‡ºä¼˜å…ˆçº§æ’åº

âœ… æ—¶é—´è§„åˆ’åˆç†ï¼š
  - æ ¹æ®å†³ç­–æ ‘åˆ¶å®šåç»­3ä¸ªæœˆè®¡åˆ’
  - æ—¶é—´ä¼°ç®—è€ƒè™‘é£é™©å› ç´ 
```

### Phase 1 å®Œæˆæ ‡å‡†

```
âœ… Phase 1.1ï¼ˆå¿…åšï¼‰ï¼š
  - æ‰€æœ‰éœ€æ±‚å¡ç‰‡åŒ…å«äº”ç±»è¯ç»“æ„
  - LLMæå–å‡†ç¡®ç‡ > 80%
  - UIèƒ½æŒ‰äº”ç±»è¯ç­›é€‰éœ€æ±‚
  - èƒ½å¿«é€Ÿå®šä½"é«˜ä»·å€¼+å¯è¡Œ+ç‰¹å®šé¢†åŸŸ"ç»„åˆ

âœ… Phase 1.2ï¼ˆæ¡ä»¶æ‰§è¡Œï¼‰ï¼š
  - èšç±»è´¨é‡è¯„åˆ†ç®—æ³•åˆç†ï¼ˆäººå·¥éªŒè¯TOP 15ç¡®å®é«˜è´¨é‡ï¼‰
  - TOP 15æ¨èç°‡å‘½ä¸­ç‡ > 70%
  - å®¡æ ¸æ—¶é—´ä¸‹é™ 30-50%
  - é—æ¼ç‡ä¸‹é™
```

### Phase 2 å®Œæˆæ ‡å‡†ï¼ˆæ ¹æ®å®é™…å®æ–½é¡¹åˆ¤æ–­ï¼‰

```
âœ… Phase 2.1ï¼ˆå¦‚é€‚ç”¨ï¼‰è¯çº§è§„èŒƒåŒ–å»é‡ï¼š
  - é¢å¤–å»é‡ 10-20%
  - è¯¯åˆå¹¶ç‡ < 5%ï¼ˆäººå·¥æŠ½æ ·éªŒè¯ï¼‰
  - èšç±»ç°‡æ›´æ¸…æ™°ï¼ˆç°‡å†…çŸ­è¯­ä¸€è‡´æ€§æå‡ï¼‰
  - æ²¡æœ‰ç ´åé«˜è´¨é‡éœ€æ±‚çš„ç‹¬ç‰¹æ€§

âœ… Phase 2.2ï¼ˆå¦‚é€‚ç”¨ï¼‰æ¨¡æ¿-å˜é‡è¿­ä»£ï¼š
  - Tokenä»26ä¸ªæ‰©å±•åˆ°200-500ä¸ª
  - Tokenè¦†ç›–ç‡æå‡åˆ°70%+
  - æå–å˜é‡è´¨é‡é«˜ï¼ˆå‡†ç¡®ç‡>80%ï¼‰
  - æ¯ä¸ªå˜é‡é€‚é…â‰¥3ä¸ªæ¨¡æ¿
  - ç°‡çš„Topç‰¹å¾è¯æ›´ä¸°å¯Œå‡†ç¡®

âœ… Phase 2.3ï¼ˆå¦‚é€‚ç”¨ï¼‰æœç´¢æ„å›¾åˆ†ç±»ï¼š
  - æ„å›¾åˆ†ç±»å‡†ç¡®ç‡ > 75%
  - ç°‡çš„ä¸»å¯¼æ„å›¾è¯†åˆ«å‡†ç¡®ç‡ > 80%
  - æ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µæ¸…æ™°æ˜¾ç¤ºåˆ†å¸ƒ
  - èƒ½å¿«é€Ÿç­›é€‰ç‰¹å®šæ„å›¾+ç±»å‹éœ€æ±‚
```

### Phase 3 å®Œæˆæ ‡å‡†

```
âœ… Run Overviewè‡ªåŠ¨ç”Ÿæˆï¼š
  - æ¯æ¬¡è¿è¡Œåè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
  - æŠ¥å‘ŠåŒ…å«4ä¸ªæ ¸å¿ƒsection
  - æŠ¥å‘Šæ ¼å¼æ¸…æ™°æ˜“è¯»

âœ… å¯å¤ç›˜åˆ†æï¼š
  - UIèƒ½æŸ¥çœ‹å†å²RunæŠ¥å‘Š
  - èƒ½ä¸‹è½½æŠ¥å‘Š
  - èƒ½å¯¹æ¯”ä¸åŒRunçš„æ•ˆæœ
  - æ–¹ä¾¿æ€»ç»“ç»éªŒå’Œè¶‹åŠ¿
```

### æ•´ä½“æˆåŠŸæ ‡å‡†

```
âœ… è¯æ®é©±åŠ¨å†³ç­–ï¼š
  - æ‰€æœ‰ä¼˜åŒ–åŸºäºPhase 0æµ‹é‡ç»“æœ
  - ä¸åšæ— è¯æ®æ”¯æŒçš„ä¼˜åŒ–
  - æ¯ä¸ªä¼˜åŒ–éƒ½æœ‰æ•ˆæœéªŒè¯

âœ… ä¿æŒMVPä¼˜åŠ¿ï¼š
  - ç°æœ‰HDBSCANèšç±»ä»æ­£å¸¸å·¥ä½œ
  - LLMè‡ªåŠ¨åŒ–ç¨‹åº¦æ²¡æœ‰é™ä½
  - UIä¿æŒå‹å¥½æ˜“ç”¨
  - æ²¡æœ‰å¼•å…¥ç ´åæ€§å˜æ›´

âœ… å®é™…ä»·å€¼æå‡ï¼š
  - èšç±»å®¡æ ¸æ•ˆç‡æå‡ï¼ˆæ—¶é—´ä¸‹é™æˆ–é—æ¼ç‡ä¸‹é™ï¼‰
  - éœ€æ±‚åˆ†æèƒ½åŠ›æå‡ï¼ˆtokenæ›´ä¸°å¯Œã€åˆ†ç±»æ›´æ¸…æ™°ï¼‰
  - æ•°æ®è´¨é‡æå‡ï¼ˆå†—ä½™ä¸‹é™ã€è¦†ç›–ç‡ä¸Šå‡ï¼‰
  - å†³ç­–æ”¯æŒæå‡ï¼ˆæ„å›¾çŸ©é˜µã€RunæŠ¥å‘Šï¼‰

âœ… å¯æŒç»­å‘å±•ï¼š
  - ä»£ç æ¨¡å—åŒ–ã€å¯ç»´æŠ¤
  - æ–‡æ¡£å®Œæ•´ã€å¯è¿½æº¯
  - å¯ä»¥ç»§ç»­è¿­ä»£ä¼˜åŒ–
  - ä¸ä¼šé™·å…¥æŠ€æœ¯å€ºåŠ¡
```

---

## ğŸ“š å‚è€ƒä¾æ®

### 1. å›è¨€ç³»ç»Ÿæºç åˆ†æï¼ˆè¯æ®åŸºç¡€ï¼‰

```
æ–‡æ¡£ï¼šdocs/å›è¨€ç³»ç»Ÿæºç åˆ†æ-è‹±æ–‡é€‚é…è¯„ä¼°.md

æ ¸å¿ƒå€Ÿé‰´ï¼š
âœ… Phase 0åŸºçº¿æµ‹é‡æ€æƒ³
âœ… æ¨¡æ¿-å˜é‡è¿­ä»£ç®—æ³•
âœ… ç‰¹å¾ç‰‡æ®µæ˜ å°„åŸç†ï¼ˆé™ç»´æ€æƒ³ï¼‰
âœ… éœ€æ±‚ç±»åˆ«åˆ†ææ¡†æ¶
âœ… ä»å™ªå£°ç‚¹æå–ç§å­å˜é‡ï¼ˆå…³é”®åˆ›æ–°ï¼‰

ä¸é€‚é…ç‚¹ï¼š
âŒ å­—ç¬¦çº§æ’åºï¼ˆæ”¹ä¸ºè¯çº§ï¼‰
âŒ æ‹¼éŸ³æ’åºï¼ˆè‹±æ–‡æ— å¯¹åº”ç‰©ï¼‰
âŒ è¶…å¤§è§„æ¨¡å¤„ç†ï¼ˆå½“å‰è§„æ¨¡ä¸éœ€è¦ï¼‰
```

### 2. GPTåé¦ˆï¼ˆPhase 0æ¡†æ¶ï¼‰

```
æ–‡æ¡£ï¼šdocs/gptå›å¤.md

æ ¸å¿ƒé‡‡çº³ï¼š
âœ… Phase 0å››ä¸ªå®éªŒè®¾è®¡
âœ… èšç±»è´¨é‡è¯„åˆ†æ€è·¯
âœ… éœ€æ±‚å¡ç‰‡äº”ç±»è¯ç»“æ„
âœ… æ„å›¾Ã—äº§å“ç±»å‹çŸ©é˜µè§†å›¾
âœ… Run Overviewæ¦‚å¿µ

å…³é”®ä¿®æ­£ï¼š
âŒ åœç”¨è¯ç­–ç•¥ï¼ˆä¿å®ˆåŒ–ï¼‰
âŒ ç§å­å˜é‡æ¥æºï¼ˆå¢åŠ å™ªå£°ç‚¹æå–ï¼‰
âŒ æ—¶é—´ä¼°ç®—ï¼ˆç°å®åŒ–ï¼‰
```

### 3. æ‰¹åˆ¤æ€§åˆ†æï¼ˆæŠ€æœ¯ä¿®æ­£ï¼‰

```
å¯¹GPTå»ºè®®çš„ä¿®æ­£ï¼š

ä¿®æ­£1ï¼šè§„èŒƒåŒ–ç®—æ³•
  GPT: å»é™¤for/at/inç­‰ä»‹è¯
  ä¿®æ­£: ä»…å»é™¤a/an/theå† è¯
  åŸå› : ä»‹è¯æ‰¿è½½è¯­ä¹‰ï¼ˆfor students â‰  for businessï¼‰

ä¿®æ­£2ï¼šç§å­å˜é‡æ¥æº
  GPT: ç°æœ‰tokens + æ‰‹å·¥å®šä¹‰
  ä¿®æ­£: å™ªå£°ç‚¹ + ç°æœ‰tokens + æ‰‹å·¥å®šä¹‰
  åŸå› : æ•°æ®é©±åŠ¨ï¼Œå‘ç°éšè—ç‰¹å¾ï¼ˆå›è¨€ç»éªŒï¼‰

ä¿®æ­£3ï¼šæ—¶é—´ä¼°ç®—
  GPT: 6å‘¨
  ä¿®æ­£: 10-13å‘¨
  åŸå› : è€ƒè™‘è°ƒè¯•ã€LLMå»¶è¿Ÿã€äººå·¥éªŒè¯ã€è¿ç§»æˆæœ¬

ä¿®æ­£4ï¼šPhase 1.2æ‰§è¡Œæ¡ä»¶
  GPT: ä¸€å®šè¦åš
  ä¿®æ­£: æ¡ä»¶æ‰§è¡Œï¼ˆæ ¹æ®Phase 0å†³å®šï¼‰
  åŸå› : å¦‚æœå®¡æ ¸ä¸è´¹åŠ›ï¼Œä¸éœ€è¦è¯„åˆ†è¾…åŠ©
```

---

## ğŸ”„ å…³é”®å·®å¼‚å¯¹ç…§

| ç»´åº¦ | GPTå»ºè®® | æœ€ç»ˆè®¡åˆ’ | ä¿®æ­£åŸå›  |
|------|---------|----------|----------|
| **è§„èŒƒåŒ–åœç”¨è¯** | å»é™¤for/at/inç­‰ä»‹è¯ | ä»…å»é™¤a/an/theå† è¯ | ä¿ç•™è¯­ä¹‰å®Œæ•´æ€§ï¼Œä»‹è¯è¡¨ç¤ºåœºæ™¯/ç¾¤ä½“ |
| **ç§å­å˜é‡æ¥æº** | ç°æœ‰tokens + æ‰‹å·¥å®šä¹‰ | å™ªå£°ç‚¹ + ç°æœ‰tokens + æ‰‹å·¥ | å¢åŠ æ•°æ®é©±åŠ¨æ¥æºï¼ˆå›è¨€æ ¸å¿ƒç»éªŒï¼‰ |
| **è¯å½¢è¿˜åŸç­–ç•¥** | é»˜è®¤å¼€å¯ | é»˜è®¤å…³é—­ï¼ˆå¯é€‰é…ç½®ï¼‰ | é¿å…è¿‡åº¦å½’å¹¶ï¼ˆcompress vs compressorï¼‰ |
| **è¯çº§æ’åºç­–ç•¥** | é»˜è®¤å¼€å¯ | é»˜è®¤å…³é—­ï¼ˆå¯é€‰é…ç½®ï¼‰ | æ›´ä¿å®ˆï¼Œé¿å…ç ´åè¯­ä¹‰ |
| **Phase 1.2ä¼˜å…ˆçº§** | å¿…åš | æ¡ä»¶æ‰§è¡Œï¼ˆæ ¹æ®Exp Aï¼‰ | è¯æ®é©±åŠ¨ï¼Œä¸ç›²ç›®ä¼˜åŒ– |
| **Phase 2.1ä¼˜å…ˆçº§** | è¾ƒé«˜ | æ¡ä»¶æ‰§è¡Œï¼ˆæ ¹æ®Exp Cï¼‰ | å†—ä½™<20%æ—¶ä¸åš |
| **Phase 2.2ä¼˜å…ˆçº§** | è¾ƒé«˜ | æ¡ä»¶æ‰§è¡Œï¼ˆæ ¹æ®Exp Bï¼‰ | è¦†ç›–ç‡>80%æ—¶ä¸åš |
| **æ€»æ—¶é—´ä¼°ç®—** | 6å‘¨ | 10-13å‘¨ | ç°å®å·¥ç¨‹æ—¶é—´ï¼ˆè°ƒè¯•+LLM+éªŒè¯+è¿ç§»ï¼‰ |
| **é£é™©ç®¡ç†** | æœªæåŠ | è¯¦ç»†åˆ—å‡º5å¤§é£é™©+ç¼“è§£ | é™ä½å®æ–½é£é™© |
| **æˆåŠŸæ ‡å‡†** | ç®€ç•¥ | è¯¦ç»†åˆ†é˜¶æ®µæ ‡å‡† | å¯é‡åŒ–ã€å¯éªŒè¯ |

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¼€å§‹ï¼ˆWeek 1ï¼‰

```bash
# 1. åˆ›å»ºPhase 0æµ‹é‡è„šæœ¬
mkdir -p scripts/phase0
touch scripts/phase0/experiment_a_clustering_cost.py
touch scripts/phase0/experiment_b_token_coverage.py
touch scripts/phase0/experiment_c_redundancy_rate.py
touch scripts/phase0/experiment_d_intent_distribution.py

# 2. å‡†å¤‡åŸºçº¿æ•°æ®
python scripts/prepare_baseline_data.py

# 3. è¿è¡Œ4ä¸ªå®éªŒ
python scripts/phase0/run_all_experiments.py

# 4. ç”ŸæˆåŸºçº¿æŠ¥å‘Š
python scripts/phase0/generate_baseline_report.py

# 5. æ ¹æ®æŠ¥å‘Šç¡®å®šåç»­è®¡åˆ’
# â†’ åœ¨å›¢é˜Ÿä¼šè®®ä¸­è®¨è®º
# â†’ æ ‡æ³¨å“ªäº›ä¼˜åŒ–éœ€è¦åš
# â†’ åˆ¶å®šè¯¦ç»†æ—¶é—´è¡¨
```

### å…³é”®åŸåˆ™ï¼ˆè´¯ç©¿æ•´ä¸ªè¿‡ç¨‹ï¼‰

```
1. âœ… è¯æ®ä¼˜å…ˆï¼šPhase 0å¿…é¡»æœ€å…ˆå®Œæˆ
2. âœ… æŒ‰éœ€ä¼˜åŒ–ï¼šæ ¹æ®æµ‹é‡ç»“æœé€‰æ‹©1-2ä¸ªæ–¹å‘
3. âœ… è¯­è¨€é€‚é…ï¼šè‹±æ–‡è¯çº§å¤„ç†ï¼Œéå­—ç¬¦çº§
4. âœ… ä¿æŒä¼˜åŠ¿ï¼šä¸ç ´åHDBSCAN+LLM+UIæ¶æ„
5. âœ… è¿­ä»£éªŒè¯ï¼šæ¯é˜¶æ®µå®ŒæˆåéªŒè¯æ•ˆæœ
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0 æœ€ç»ˆç‰ˆ
**åˆ›å»ºæ—¥æœŸ**: 2025-12-23
**ç»´æŠ¤è€…**: é¡¹ç›®å›¢é˜Ÿ
**çŠ¶æ€**: å¾…æ‰§è¡Œ

---

> ğŸ’¡ **æ ¸å¿ƒç†å¿µ**
>
> "ä¸ä¸ºä¼˜åŒ–è€Œä¼˜åŒ–ï¼Œåªä¸ºè§£å†³çœŸå®é—®é¢˜è€Œä¼˜åŒ–"
>
> "è¯æ®è¯´è¯ï¼Œæ•°æ®å†³ç­–ï¼Œä¿å®ˆå®æ–½ï¼Œè¿­ä»£éªŒè¯"
>
> "å€Ÿé‰´å›è¨€æ€æƒ³ï¼Œé€‚é…è‹±æ–‡ç‰¹ç‚¹ï¼Œä¿æŒMVPä¼˜åŠ¿"

---

## é™„å½•ï¼šå¿«é€Ÿå†³ç­–æ ‘

```
Phase 0 åŸºçº¿æµ‹é‡
    â†“
Experiment A: èšç±»å®¡æ ¸æˆæœ¬
    â”œâ”€ æ—¶é—´<60min ä¸” é—æ¼<10% â†’ Phase 1.2 æš‚ä¸åš âœ…
    â”œâ”€ æ—¶é—´60-120min æˆ– é—æ¼10-30% â†’ Phase 1.2 å¯é€‰ ğŸŸ¡
    â””â”€ æ—¶é—´>120min æˆ– é—æ¼>30% â†’ Phase 1.2 å¿…åš âŒ

Experiment B: Tokenè¦†ç›–ç‡
    â”œâ”€ è¦†ç›–ç‡â‰¥80% â†’ Phase 2.2 æš‚ä¸åš âœ…
    â”œâ”€ è¦†ç›–ç‡60-80% â†’ Phase 2.2 å¯é€‰ ğŸŸ¡
    â””â”€ è¦†ç›–ç‡<60% â†’ Phase 2.2 å¿…åš âŒ

Experiment C: æ•°æ®å†—ä½™ç‡
    â”œâ”€ å†—ä½™ç‡<10% â†’ Phase 2.1 æš‚ä¸åš âœ…
    â”œâ”€ å†—ä½™ç‡10-20% â†’ Phase 2.1 å¯é€‰ ğŸŸ¡
    â””â”€ å†—ä½™ç‡>20% â†’ Phase 2.1 å¿…åš âŒ

Experiment D: æœç´¢æ„å›¾åˆ†å¸ƒ
    â”œâ”€ å¯»æ‰¾ç±»>70% â†’ Phase 2.3 å€¼å¾—åš âœ…
    â”œâ”€ åˆ†å¸ƒè¾ƒå‡åŒ€ â†’ Phase 2.3 éœ€è°ƒæ•´ ğŸŸ¡
    â””â”€ Otherå ä¸»å¯¼ â†’ Phase 2.3 é‡æ–°è®¾è®¡ âŒ

Phase 1.1: éœ€æ±‚å¡ç‰‡ç»“æ„ç»Ÿä¸€
    â†’ æ— æ¡ä»¶æ‰§è¡Œ âœ…âœ…âœ…ï¼ˆåŸºç¡€è®¾æ–½ï¼‰

æœ€ç»ˆé€‰æ‹©ï¼š
    æ ¹æ®å†³ç­–æ ‘ç»“æœï¼Œé€‰æ‹©1-2ä¸ªPhase 2ä¼˜åŒ–æ–¹å‘
    æ—¶é—´çº¿ï¼š10-13å‘¨
```

---

**ç¥å¼€å‘é¡ºåˆ©ï¼**
