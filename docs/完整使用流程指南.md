# è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜ç³»ç»Ÿ - å®Œæ•´ä½¿ç”¨æµç¨‹æŒ‡å—

> ç‰ˆæœ¬: 2.0
> æ›´æ–°æ—¥æœŸ: 2025-12-24
> çŠ¶æ€: âœ… æ­£å¼ç‰ˆ

---

## ğŸ“‹ æ–‡æ¡£ç›®å½•

- [ç³»ç»Ÿæ¦‚è§ˆ](#ç³»ç»Ÿæ¦‚è§ˆ)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [Phase 1: æ•°æ®å¯¼å…¥](#phase-1-æ•°æ®å¯¼å…¥)
- [Phase 2: å¤§ç»„èšç±»](#phase-2-å¤§ç»„èšç±»)
- [Phase 3: èšç±»ç­›é€‰ä¸è¯„åˆ†](#phase-3-èšç±»ç­›é€‰ä¸è¯„åˆ†)
- [Phase 4: å°ç»„èšç±»ä¸éœ€æ±‚ç”Ÿæˆ](#phase-4-å°ç»„èšç±»ä¸éœ€æ±‚ç”Ÿæˆ)
- [Phase 5: Tokenæå–ä¸åˆ†ç±»](#phase-5-tokenæå–ä¸åˆ†ç±»)
- [Web UIä½¿ç”¨æŒ‡å—](#web-uiä½¿ç”¨æŒ‡å—)
- [åº•å±‚æŠ€æœ¯æ¶æ„](#åº•å±‚æŠ€æœ¯æ¶æ„)
- [å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ](#å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ)
- [æ•…éšœæ’æŸ¥æŒ‡å—](#æ•…éšœæ’æŸ¥æŒ‡å—)

---

## ç³»ç»Ÿæ¦‚è§ˆ

### é¡¹ç›®å®šä½

è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäº**è¯­ä¹‰èšç±»**å’Œ**LLMæ™ºèƒ½åˆ†æ**çš„å…³é”®è¯æ•°æ®æŒ–æ˜å¹³å°,æ—¨åœ¨ä»æµ·é‡æœç´¢å…³é”®è¯ä¸­æå–æœ‰ä»·å€¼çš„ç”¨æˆ·éœ€æ±‚ã€‚

### æ ¸å¿ƒä»·å€¼

- âœ… **è‡ªåŠ¨åŒ–è¯­ä¹‰èšç±»**: å°†æ•°ä¸‡æ¡å…³é”®è¯è‡ªåŠ¨åˆ†ç»„ä¸º60-100ä¸ªä¸»é¢˜ç°‡
- âœ… **æ™ºèƒ½éœ€æ±‚ç”Ÿæˆ**: ä½¿ç”¨LLMè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–éœ€æ±‚å¡ç‰‡
- âœ… **å¤šç»´åº¦ç­›é€‰**: æ”¯æŒè´¨é‡åˆ†ã€æ„å›¾ç±»å‹ã€å‡è¡¡åº¦ç­‰å¤šç»´åº¦ç­›é€‰
- âœ… **æ–­ç‚¹ç»­ä¼ **: å¤„ç†å¤±è´¥å¯æ— æŸæ¢å¤,é¿å…é‡å¤è®¡ç®—
- âœ… **å¯è§†åŒ–æ“ä½œ**: æä¾›Web UIå’Œå‘½ä»¤è¡Œä¸¤ç§æ“ä½œæ¨¡å¼

### æ•°æ®æµè½¬æ¦‚è§ˆ

```
åŸå§‹æ•°æ® (SEMRUSH/Dropdown/Related Search)
    â†“
[Phase 1] æ•°æ®å¯¼å…¥ â†’ æ•°æ®åº“ phrases è¡¨
    â†“
[Phase 2] Embedding + HDBSCAN â†’ 60-100 ä¸ªå¤§ç»„ (Level A)
    â†“
[Phase 3] LLMä¸»é¢˜ç”Ÿæˆ + äººå·¥è¯„åˆ† â†’ é€‰ä¸­ 15-20 ä¸ªä¼˜è´¨å¤§ç»„
    â†“
[Phase 4] å°ç»„èšç±» + LLMéœ€æ±‚ç”Ÿæˆ â†’ 129+ éœ€æ±‚å¡ç‰‡ (Level B)
    â†“
[Phase 5] Tokenæå– + LLMåˆ†ç±» â†’ intent/action/object è¯åº“
    â†“
äººå·¥å®¡æ ¸ä¸å¯¼å…¥ â†’ æœ€ç»ˆéœ€æ±‚æ± 
```

### æŠ€æœ¯æ ˆ

| æŠ€æœ¯å±‚ | æŠ€æœ¯é€‰å‹ |
|--------|---------|
| **è¯­ä¹‰ç†è§£** | Sentence Transformers (all-MiniLM-L6-v2) |
| **èšç±»ç®—æ³•** | HDBSCAN (Hierarchical Density-Based) |
| **LLM** | OpenAI GPT-4o-mini / Anthropic Claude / DeepSeek |
| **æ•°æ®åº“** | MySQL 8.0+ / SQLite 3.x |
| **ORM** | SQLAlchemy 2.x |
| **Webæ¡†æ¶** | Streamlit 1.28+ |
| **æ•°æ®å¤„ç†** | Pandas, NumPy |
| **ç¼“å­˜** | NPZ (NumPy compressed) |

---

## ç¯å¢ƒé…ç½®

### 1. ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windows 10+, macOS 11+, Ubuntu 20.04+
- **Python**: 3.11+ (æ¨è 3.11.5)
- **å†…å­˜**: 8GB+ (æ¨è 16GB)
- **ç£ç›˜**: 5GB+ å¯ç”¨ç©ºé—´
- **æ•°æ®åº“**: MySQL 8.0+ æˆ– SQLite 3.x

### 2. å®‰è£…æ­¥éª¤

#### 2.1 å…‹éš†é¡¹ç›®

```bash
git clone <repository_url>
cd è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜
```

#### 2.2 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

#### 2.3 å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

**æ ¸å¿ƒä¾èµ–è¯´æ˜**:
- `sentence-transformers`: å¥å­åµŒå…¥æ¨¡å‹
- `hdbscan`: å±‚æ¬¡å¯†åº¦èšç±»
- `sqlalchemy`: æ•°æ®åº“ORM
- `streamlit`: Web UIæ¡†æ¶
- `openai / anthropic`: LLMå®¢æˆ·ç«¯

#### 2.4 ä¸‹è½½Embeddingæ¨¡å‹

é¦–æ¬¡è¿è¡Œæ—¶,ç³»ç»Ÿä¼šè‡ªåŠ¨ä¸‹è½½ `all-MiniLM-L6-v2` æ¨¡å‹ (~90MB):

```bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

### 3. é…ç½®æ–‡ä»¶è®¾ç½®

#### 3.1 åˆ›å»ºé…ç½®æ–‡ä»¶

```bash
cp .env.example .env
```

#### 3.2 é…ç½®æ•°æ®åº“

ç¼–è¾‘ `config/settings.py`:

**MySQLé…ç½®**:
```python
DATABASE_CONFIG = {
    "type": "mysql",
    "host": "localhost",
    "port": 3306,
    "database": "keyword_clustering",
    "user": "root",
    "password": "your_password",
    "charset": "utf8mb4"
}
```

**SQLiteé…ç½®** (æµ‹è¯•ç”¨):
```python
DATABASE_CONFIG = {
    "type": "sqlite",
    "path": "data/keyword_clustering.db"
}
```

#### 3.3 é…ç½®LLM

é€‰æ‹©LLMæä¾›å•†:

```python
LLM_PROVIDER = "openai"  # æˆ– "anthropic", "deepseek"

LLM_CONFIG = {
    "openai": {
        "api_key": "sk-xxx",  # ä»ç¯å¢ƒå˜é‡è¯»å–
        "model": "gpt-4o-mini",
        "base_url": None,  # å¯é€‰: è‡ªå®šä¹‰endpoint
        "temperature": 0.3,
        "max_tokens": 2000,
    },
}
```

**ç¯å¢ƒå˜é‡è®¾ç½®** (æ¨è):

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-ant-xxx
DEEPSEEK_API_KEY=sk-xxx
```

### 4. åˆå§‹åŒ–æ•°æ®åº“

```bash
python storage/init_db.py
```

**è¾“å‡ºç¤ºä¾‹**:
```
ğŸ”§ åˆå§‹åŒ–æ•°æ®åº“...
  âœ“ åˆ›å»ºè¡¨: phrases
  âœ“ åˆ›å»ºè¡¨: cluster_meta
  âœ“ åˆ›å»ºè¡¨: demands
  âœ“ åˆ›å»ºè¡¨: tokens
  âœ“ åˆ›å»ºç´¢å¼•
âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ!
```

### 5. éªŒè¯å®‰è£…

```bash
python -c "from storage.repository import test_database_connection; test_database_connection()"
```

---

## æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼è¦æ±‚

ç³»ç»Ÿæ”¯æŒä¸‰ç§æ•°æ®æº:

#### 1. SEMRUSHå¯¼å‡ºæ•°æ®

**CSVæ ¼å¼** (`data/input/semrush_export.csv`):

| åˆ—å | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `Keyword` | æœç´¢çŸ­è¯­ | "best running shoes" |
| `Search Volume` | æœˆæœç´¢é‡ | 5400 |
| (å¯é€‰) `Keyword Difficulty` | SEOéš¾åº¦ | 45 |

**å¿…éœ€åˆ—**: `Keyword`, `Search Volume`

#### 2. Google Dropdownæ•°æ®

**CSVæ ¼å¼** (`data/input/dropdown_suggestions.csv`):

| åˆ—å | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `seed_word` | ç§å­è¯ | "calculator" |
| `phrase` | ä¸‹æ‹‰å»ºè®®çŸ­è¯­ | "calculator online" |
| `position` | ä½ç½® | 1 |

**å¿…éœ€åˆ—**: `seed_word`, `phrase`

#### 3. Related Searchæ•°æ®

**CSVæ ¼å¼** (`data/input/related_searches.csv`):

| åˆ—å | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `seed_word` | ç§å­è¯ | "calculator" |
| `related_phrase` | ç›¸å…³æœç´¢çŸ­è¯­ | "scientific calculator" |

**å¿…éœ€åˆ—**: `seed_word`, `related_phrase`

### æ•°æ®æ¸…æ´—å»ºè®®

1. **å»é‡**: ç¡®ä¿åŒä¸€çŸ­è¯­ä¸é‡å¤å‡ºç°
2. **ç¼–ç **: ä½¿ç”¨UTF-8ç¼–ç ä¿å­˜CSV
3. **ç©ºå€¼å¤„ç†**: åˆ é™¤ç©ºå€¼æˆ–å ä½ç¬¦
4. **è¯­è¨€è¿‡æ»¤**: å¦‚æœæ˜¯è‹±æ–‡ç³»ç»Ÿ,è¿‡æ»¤éè‹±æ–‡çŸ­è¯­

---

## Phase 1: æ•°æ®å¯¼å…¥

### åŠŸèƒ½è¯´æ˜

å°†å¤–éƒ¨CSVæ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“ `phrases` è¡¨,å¹¶è‡ªåŠ¨è®°å½•æ¥æºã€è½®æ¬¡ç­‰å…ƒä¿¡æ¯ã€‚

### æ“ä½œæ–¹å¼

#### æ–¹å¼ä¸€: Web UI (æ¨è)

1. å¯åŠ¨Web UI:
   ```bash
   streamlit run web_ui.py
   ```

2. è®¿é—® http://localhost:8501

3. å¯¼èˆªåˆ° **Phase 1: æ•°æ®å¯¼å…¥** é¡µé¢

4. é€‰æ‹©æ•°æ®æºç±»å‹:
   - SEMRUSHå¯¼å‡º
   - Google Dropdown
   - Related Search

5. ä¸Šä¼ CSVæ–‡ä»¶æˆ–è¾“å…¥æ–‡ä»¶è·¯å¾„

6. ç‚¹å‡» **"å¼€å§‹å¯¼å…¥"**

7. æŸ¥çœ‹å¯¼å…¥æ—¥å¿—å’Œç»Ÿè®¡

#### æ–¹å¼äºŒ: å‘½ä»¤è¡Œ

```bash
# å¯¼å…¥SEMRUSHæ•°æ®
python scripts/import_semrush.py --file data/input/semrush_export.csv --round-id 1

# å¯¼å…¥Dropdownæ•°æ®
python scripts/import_dropdown.py --file data/input/dropdown_suggestions.csv --round-id 1

# å¯¼å…¥Related Searchæ•°æ®
python scripts/import_related_search.py --file data/input/related_searches.csv --round-id 1
```

**å‚æ•°è¯´æ˜**:
- `--file`: CSVæ–‡ä»¶è·¯å¾„
- `--round-id`: æ•°æ®è½®æ¬¡ID (é»˜è®¤ä¸º1,ç”¨äºå¤šè½®è¿­ä»£)
- `--batch-size`: æ‰¹é‡æ’å…¥å¤§å° (é»˜è®¤1000)

### åº•å±‚é€»è¾‘

#### 1.1 æ•°æ®è¯»å–

```python
# scripts/import_semrush.py
df = pd.read_csv(file_path, encoding='utf-8')

# æ ‡å‡†åŒ–åˆ—å
df.rename(columns={
    'Keyword': 'phrase',
    'Search Volume': 'volume'
}, inplace=True)
```

#### 1.2 æ•°æ®è½¬æ¢

æ¯è¡ŒCSVè½¬æ¢ä¸ºä¸€ä¸ª `Phrase` å¯¹è±¡:

```python
from storage.models import Phrase

phrase_obj = Phrase(
    phrase="best running shoes",          # çŸ­è¯­æ–‡æœ¬
    seed_word="running shoes",            # ç§å­è¯
    source_type="semrush",                # æ•°æ®æºç±»å‹
    first_seen_round=1,                   # é¦–æ¬¡å‡ºç°è½®æ¬¡
    frequency=1,                          # é¢‘æ¬¡(åˆå§‹ä¸º1)
    volume=5400,                          # æœç´¢é‡
    processed_status="unseen",            # å¤„ç†çŠ¶æ€
    cluster_id_A=None,                    # å¤§ç»„ID(å¾…èšç±»)
    cluster_id_B=None,                    # å°ç»„ID(å¾…èšç±»)
    mapped_demand_id=None                 # å…³è”éœ€æ±‚ID(å¾…æ˜ å°„)
)
```

#### 1.3 æ‰¹é‡æ’å…¥

ä½¿ç”¨SQLAlchemyçš„ `bulk_insert_mappings` æå‡æ€§èƒ½:

```python
# storage/repository.py
def bulk_insert_phrases(self, records: List[Dict], batch_size: int = 1000):
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        self.session.bulk_insert_mappings(Phrase, batch)
        self.session.commit()
```

**æ€§èƒ½**: ~5000æ¡/ç§’ (MySQL), ~10000æ¡/ç§’ (SQLite)

#### 1.4 å»é‡å¤„ç†

æ•°æ®åº“å±‚é¢é€šè¿‡ UNIQUE çº¦æŸé˜²æ­¢é‡å¤:

```sql
CREATE TABLE phrases (
    phrase_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    phrase VARCHAR(255) UNIQUE NOT NULL,
    ...
);
```

å¦‚æœæ’å…¥é‡å¤çŸ­è¯­,ä¼šè§¦å‘ `IntegrityError`,ç³»ç»Ÿè‡ªåŠ¨è·³è¿‡ã€‚

### è¾“å‡ºç»“æœ

#### æ•°æ®åº“çŠ¶æ€

```sql
SELECT COUNT(*) FROM phrases WHERE first_seen_round = 1;
-- è¾“å‡º: 15000 (ç¤ºä¾‹)

SELECT source_type, COUNT(*)
FROM phrases
GROUP BY source_type;
-- è¾“å‡º:
--   semrush: 10000
--   dropdown: 3000
--   related_search: 2000
```

#### ç»Ÿè®¡æŠ¥å‘Š

`data/output/phase1_import_report_round1.txt`:

```
======================================================================
Phase 1 æ•°æ®å¯¼å…¥æŠ¥å‘Š - Round 1
======================================================================

ã€å¯¼å…¥æ¦‚å†µã€‘
  æ•°æ®æº: SEMRUSH
  æ–‡ä»¶: data/input/semrush_export.csv
  æ€»è¡Œæ•°: 10,000
  æˆåŠŸå¯¼å…¥: 9,856
  é‡å¤è·³è¿‡: 144

ã€æ•°æ®è´¨é‡ã€‘
  æœ‰æ•ˆçŸ­è¯­ç‡: 98.56%
  å¹³å‡çŸ­è¯­é•¿åº¦: 3.2 è¯
  å¹³å‡æœç´¢é‡: 1,240

ã€ä¸‹ä¸€æ­¥ã€‘
  è¿è¡Œ Phase 2: python scripts/run_phase2_clustering.py
======================================================================
```

### æœ€ä½³å®è·µ

1. **æ•°æ®æºæ··åˆ**: å»ºè®®åŒæ—¶å¯¼å…¥SEMRUSH + Dropdown,è¦†ç›–æ›´å…¨é¢
2. **è½®æ¬¡ç®¡ç†**: æ¯æœˆæ–°æ•°æ®ä½¿ç”¨æ–°çš„ `round_id`,ä¾¿äºè¿½è¸ª
3. **è´¨é‡æ£€æŸ¥**: å¯¼å…¥åä½¿ç”¨ `SELECT phrase FROM phrases LIMIT 100;` æŠ½æŸ¥
4. **å¤‡ä»½**: å¯¼å…¥å‰å¤‡ä»½æ•°æ®åº“: `mysqldump keyword_clustering > backup.sql`

---

## Phase 2: å¤§ç»„èšç±»

### åŠŸèƒ½è¯´æ˜

å¯¹æ•°æ®åº“ä¸­æ‰€æœ‰çŸ­è¯­è®¡ç®—è¯­ä¹‰åµŒå…¥å‘é‡ (Embeddings),ç„¶åä½¿ç”¨HDBSCANç®—æ³•è¿›è¡Œå¯†åº¦èšç±»,ç”Ÿæˆ60-100ä¸ªå¤§ç»„ (Level A)ã€‚

### æ“ä½œæ–¹å¼

#### æ–¹å¼ä¸€: Web UI

1. å¯¼èˆªåˆ° **Phase 2: å¤§ç»„èšç±»** é¡µé¢

2. é…ç½®èšç±»å‚æ•° (å¯é€‰):
   - **æœ€å°èšç±»å¤§å°**: é»˜è®¤30 (è°ƒå¤§â†’æ›´å°‘æ›´å¤§çš„ç°‡,è°ƒå°â†’æ›´å¤šæ›´å°çš„ç°‡)
   - **æœ€å°æ ·æœ¬æ•°**: é»˜è®¤3 (è°ƒå¤§â†’æ›´ä¸¥æ ¼,è°ƒå°â†’æ›´å®½æ¾)

3. ç‚¹å‡» **"å¼€å§‹èšç±»"**

4. å®æ—¶æŸ¥çœ‹æ—¥å¿—è¾“å‡º

5. èšç±»å®ŒæˆåæŸ¥çœ‹ç»Ÿè®¡æŠ¥å‘Š

#### æ–¹å¼äºŒ: å‘½ä»¤è¡Œ

```bash
python scripts/run_phase2_clustering.py --round-id 1
```

**å‚æ•°è¯´æ˜**:
- `--round-id`: æ•°æ®è½®æ¬¡ID (é»˜è®¤1)
- `--limit`: é™åˆ¶å¤„ç†çŸ­è¯­æ•° (æµ‹è¯•ç”¨,0=å…¨éƒ¨)

**æµ‹è¯•æ¨¡å¼**:
```bash
# ä»…å¤„ç†å‰1000æ¡çŸ­è¯­æµ‹è¯•èšç±»æ•ˆæœ
python scripts/run_phase2_clustering.py --limit 1000
```

### åº•å±‚é€»è¾‘

#### 2.1 åŠ è½½çŸ­è¯­

```python
# scripts/run_phase2_clustering.py
with PhraseRepository() as repo:
    phrases_db = repo.session.query(Phrase).filter(
        Phrase.processed_status == 'unseen'
    ).all()

    phrases = [{
        'phrase_id': p.phrase_id,
        'phrase': p.phrase,
        'frequency': p.frequency,
        'volume': p.volume,
    } for p in phrases_db]
```

**æŸ¥è¯¢ä¼˜åŒ–**: ä»…åŠ è½½ `unseen` çŠ¶æ€çŸ­è¯­,é¿å…é‡å¤å¤„ç†ã€‚

#### 2.2 è®¡ç®—Embeddings

ä½¿ç”¨ Sentence Transformers å°†æ–‡æœ¬è½¬æ¢ä¸º384ç»´å‘é‡:

```python
# core/embedding.py
from sentence_transformers import SentenceTransformer

class EmbeddingService:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.model.max_seq_length = 128  # é™åˆ¶åºåˆ—é•¿åº¦

    def encode(self, texts: List[str]) -> np.ndarray:
        embeddings = self.model.encode(
            texts,
            batch_size=256,
            show_progress_bar=True,
            normalize_embeddings=True  # L2å½’ä¸€åŒ–
        )
        return embeddings
```

**æ€§èƒ½ä¼˜åŒ–**:
- **æ‰¹é‡å¤„ç†**: batch_size=256,å……åˆ†åˆ©ç”¨GPU
- **ç¼“å­˜æœºåˆ¶**: embeddingså­˜ä¸ºNPZæ ¼å¼,é¿å…é‡å¤è®¡ç®—
- **å½’ä¸€åŒ–**: L2å½’ä¸€åŒ–å,ä½™å¼¦ç›¸ä¼¼åº¦ = 1 - æ¬§æ°è·ç¦»Â²/2

**ç¼“å­˜æ–‡ä»¶**: `data/cache/embeddings_round1.npz`

```python
# ç¼“å­˜æ ¼å¼
cache_dict = {
    md5("best running shoes"): [0.12, -0.34, ..., 0.56],  # 384ç»´å‘é‡
    md5("calculator online"): [-0.23, 0.45, ..., -0.12],
    ...
}
np.savez('embeddings_round1.npz', cache=cache_dict)
```

#### 2.3 HDBSCANèšç±»

```python
# core/clustering.py
import hdbscan
from sklearn.preprocessing import normalize

def cluster_phrases_large(embeddings, phrases):
    # 1. L2å½’ä¸€åŒ–(å¦‚æœå°šæœªå½’ä¸€åŒ–)
    embeddings_normalized = normalize(embeddings, norm='l2')

    # 2. åˆ›å»ºHDBSCANèšç±»å™¨
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=30,         # æœ€å°ç°‡å¤§å°
        min_samples=3,                # æ ¸å¿ƒç‚¹æœ€å°é‚»å±…æ•°
        metric='euclidean',           # ä½¿ç”¨æ¬§æ°è·ç¦»(å½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦)
        cluster_selection_epsilon=0.0, # ç°‡åˆå¹¶é˜ˆå€¼
        cluster_selection_method='eom', # è¶…é¢è´¨é‡æ³•
        prediction_data=True          # ä¿ç•™é¢„æµ‹æ•°æ®
    )

    # 3. æ‰§è¡Œèšç±»
    labels = clusterer.fit_predict(embeddings_normalized)

    # 4. ç»Ÿè®¡ä¿¡æ¯
    unique_labels = set(labels)
    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
    n_noise = list(labels).count(-1)

    # 5. è®¡ç®—è½®å»“ç³»æ•°(Silhouette Score)
    if n_clusters > 1:
        mask = labels != -1  # æ’é™¤å™ªéŸ³ç‚¹
        score = silhouette_score(
            embeddings_normalized[mask],
            labels[mask],
            metric='euclidean'
        )

    return labels, clusterer
```

**ç®—æ³•åŸç†**:

1. **å¯†åº¦ä¼°è®¡**: è®¡ç®—æ¯ä¸ªç‚¹çš„kè¿‘é‚»è·ç¦»
2. **å±‚æ¬¡æ ‘æ„å»º**: åŸºäºå¯†åº¦æ„å»ºæœ€å°ç”Ÿæˆæ ‘
3. **ç°‡æå–**: ä½¿ç”¨EOM (Excess of Mass) æ–¹æ³•é€‰æ‹©ç¨³å®šç°‡
4. **å™ªéŸ³æ ‡è®°**: ä½å¯†åº¦ç‚¹æ ‡è®°ä¸º-1

**å‚æ•°è°ƒä¼˜æŒ‡å—**:

| å‚æ•° | å½±å“ | è°ƒä¼˜å»ºè®® |
|------|------|---------|
| `min_cluster_size` | ç°‡æ•°é‡ä¸å¤§å° | å¢å¤§â†’æ›´å°‘æ›´å¤§çš„ç°‡ (æ¨è30-50) |
| `min_samples` | å™ªéŸ³æ•æ„Ÿåº¦ | å¢å¤§â†’æ›´ä¸¥æ ¼ (æ¨è3-5) |
| `cluster_selection_epsilon` | ç°‡åˆå¹¶ | å¢å¤§â†’åˆå¹¶ç›¸ä¼¼ç°‡ (é»˜è®¤0.0) |

#### 2.4 æå–ç°‡ä¿¡æ¯

```python
cluster_info = {}

for label in range(n_clusters):
    # è·å–è¯¥ç°‡çš„æ‰€æœ‰çŸ­è¯­
    mask = (labels == label)
    cluster_phrases = [phrases[i] for i, m in enumerate(mask) if m]

    # ç»Ÿè®¡ä¿¡æ¯
    cluster_info[label] = {
        'size': len(cluster_phrases),
        'total_frequency': sum(p['frequency'] for p in cluster_phrases),
        'example_phrases': [p['phrase'] for p in cluster_phrases[:10]],
        'all_phrases': [p['phrase'] for p in cluster_phrases]
    }
```

#### 2.5 æ›´æ–°æ•°æ®åº“

**æ›´æ–°phrasesè¡¨**:

```python
with PhraseRepository() as repo:
    for i, phrase_id in enumerate(phrase_ids):
        cluster_id = int(labels[i])
        repo.update_cluster_assignment(
            phrase_id,
            cluster_id_A=cluster_id
        )
```

**ä¿å­˜cluster_metaè¡¨**:

```python
with ClusterMetaRepository() as repo:
    for cluster_id, info in cluster_info.items():
        repo.create_or_update_cluster(
            cluster_id=cluster_id,
            cluster_level='A',
            size=info['size'],
            example_phrases='; '.join(info['example_phrases']),
            main_theme=None,  # Phase 3ä¼šç”Ÿæˆ
            total_frequency=info['total_frequency']
        )
```

### è¾“å‡ºç»“æœ

#### æ•°æ®åº“çŠ¶æ€

```sql
-- æ£€æŸ¥èšç±»åˆ†é…
SELECT
    cluster_id_A,
    COUNT(*) AS phrase_count
FROM phrases
WHERE cluster_id_A IS NOT NULL
GROUP BY cluster_id_A
ORDER BY phrase_count DESC
LIMIT 10;

-- è¾“å‡ºç¤ºä¾‹:
-- cluster_id_A | phrase_count
-- 1174         | 523
-- 1244         | 412
-- 1020         | 387
-- ...
```

#### ç»Ÿè®¡æŠ¥å‘Š

`data/output/phase2_clustering_report_round1.txt`:

```
======================================================================
Phase 2 å¤§ç»„èšç±»æŠ¥å‘Š
======================================================================

ã€èšç±»æ¦‚å†µã€‘
  æ€»çŸ­è¯­æ•°: 15,000
  èšç±»æ•°é‡: 307
  å™ªéŸ³ç‚¹æ•°: 234 (1.6%)

ã€èšç±»å¤§å°åˆ†å¸ƒã€‘
  æœ€å°: 30
  æœ€å¤§: 523
  å¹³å‡: 48.2
  ä¸­ä½æ•°: 41

ã€Top 20 æœ€å¤§èšç±»ã€‘
æ’å   èšç±»ID     å¤§å°     é¢‘æ¬¡æ€»å’Œ     ç¤ºä¾‹çŸ­è¯­
------------------------------------------------------------------
1      1174       523      8,234        æ±½è½¦ä»ªè¡¨ç›˜, dashboard, car dashboard
2      1244       412      6,521        æ¨¡æ‹Ÿå™¨, simulator, flight simulator
3      1020       387      5,912        citation format, apa format
...

ã€è´¨é‡æŒ‡æ ‡ã€‘
  è½®å»“ç³»æ•°: 0.342 (è‰¯å¥½)
  Davies-Bouldin Index: 1.234 (ä¼˜ç§€)

======================================================================
ä¸‹ä¸€æ­¥: è¿è¡Œ Phase 3 ç”Ÿæˆå¤§ç»„ç­›é€‰æŠ¥å‘Š
======================================================================
```

#### ç¼“å­˜æ–‡ä»¶

- `data/cache/embeddings_round1.npz`: Embeddingç¼“å­˜ (~150MB for 15K phrases)

### æ—¶é—´ä¸æˆæœ¬

- **Embeddingè®¡ç®—**: ~3-5åˆ†é’Ÿ (15KçŸ­è¯­, CPU)
- **HDBSCANèšç±»**: ~30-60ç§’ (15KçŸ­è¯­)
- **æ€»è€—æ—¶**: ~5-10åˆ†é’Ÿ
- **æˆæœ¬**: å…è´¹ (æœ¬åœ°è®¡ç®—)

### æœ€ä½³å®è·µ

1. **é¦–æ¬¡è¿è¡Œ**: ä½¿ç”¨é»˜è®¤å‚æ•° (min_cluster_size=30)
2. **ç»“æœè¯„ä¼°**: æŸ¥çœ‹èšç±»æ•°é‡(æœŸæœ›60-100),å™ªéŸ³ç‡(æœŸæœ›<5%)
3. **å‚æ•°è°ƒä¼˜**:
   - èšç±»å¤ªå¤š (>150) â†’ å¢å¤§ min_cluster_size åˆ° 40-50
   - èšç±»å¤ªå°‘ (<50) â†’ å‡å° min_cluster_size åˆ° 20-25
   - å™ªéŸ³è¿‡å¤š (>10%) â†’ å¢å¤§ min_samples åˆ° 4-5

---

## Phase 3: èšç±»ç­›é€‰ä¸è¯„åˆ†

### åŠŸèƒ½è¯´æ˜

å¯¹Phase 2ç”Ÿæˆçš„å¤§ç»„è¿›è¡Œä¸‰é¡¹å¤„ç†:
1. **LLMä¸»é¢˜ç”Ÿæˆ**: ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆ2-6è¯çš„ä¸­æ–‡ä¸»é¢˜æ ‡ç­¾
2. **è´¨é‡è‡ªåŠ¨è¯„åˆ†**: åŸºäº7ä¸ªç»´åº¦è®¡ç®—0-100åˆ†è´¨é‡åˆ† (MVPç‰ˆPhase 1åŠŸèƒ½)
3. **äººå·¥ç­›é€‰**: é€šè¿‡Web UIäº¤äº’å¼ç­›é€‰15-20ä¸ªé«˜è´¨é‡ç°‡

### æ“ä½œæ–¹å¼

#### æ–¹å¼ä¸€: Web UI (å¼ºçƒˆæ¨è)

1. å¯¼èˆªåˆ° **Phase 3: èšç±»ç­›é€‰** é¡µé¢

2. **æ­¥éª¤1: ç”Ÿæˆä¸»é¢˜æ ‡ç­¾**
   - ç‚¹å‡» **"ç”Ÿæˆä¸»é¢˜æ ‡ç­¾"**
   - ç­‰å¾…LLMå¤„ç† (307ä¸ªç°‡çº¦5-10åˆ†é’Ÿ)
   - æŸ¥çœ‹ç”Ÿæˆçš„ä¸­æ–‡ä¸»é¢˜

3. **æ­¥éª¤2: æŸ¥çœ‹è´¨é‡è¯„åˆ†**
   - ç³»ç»Ÿè‡ªåŠ¨æ˜¾ç¤ºæ¯ä¸ªç°‡çš„è´¨é‡åˆ† (0-100åˆ†)
   - è´¨é‡åˆ†åŸºäº:
     - ç°‡å¤§å° (20åˆ†)
     - å¯†åº¦/èšåˆåº¦ (20åˆ†)
     - çŸ­è¯­å¤šæ ·æ€§ (15åˆ†)
     - æœç´¢é‡åˆ†å¸ƒ (15åˆ†)
     - çŸ­è¯­é•¿åº¦åˆ†å¸ƒ (10åˆ†)
     - å¸¸è§è¯å æ¯” (10åˆ†)
     - ç‰¹æ®Šå­—ç¬¦å æ¯” (10åˆ†)

4. **æ­¥éª¤3: äº¤äº’å¼ç­›é€‰**
   - ä½¿ç”¨å¤šç»´åº¦ç­›é€‰å™¨:
     - **è´¨é‡ç­›é€‰**: Excellent (80-100åˆ†) / Good (60-79åˆ†)
     - **æ„å›¾ç­›é€‰**: find_tool, learn_how, solve_problem, find_free, compare
     - **å‡è¡¡åº¦ç­›é€‰**: å‡è¡¡ / éå‡è¡¡
     - **å¤§å°ç­›é€‰**: æ»‘å—è°ƒæ•´æœ€å°/æœ€å¤§å¤§å°
   - ä½¿ç”¨æ’åºåŠŸèƒ½:
     - æŒ‰è´¨é‡åˆ†é™åº (æ¨è)
     - æŒ‰å¤§å°é™åº
     - æŒ‰æ„å›¾ç½®ä¿¡åº¦
   - åœ¨è¡¨æ ¼ä¸­å‹¾é€‰cluster_id
   - ä½¿ç”¨ **"å¿«é€Ÿæ“ä½œ"** æ‰¹é‡é€‰ä¸­Top 10/15/20

5. **æ­¥éª¤4: ä¿å­˜é€‰ä¸­ç»“æœ**
   - ç‚¹å‡» **"ä¿å­˜é€‰ä¸­"**
   - ç³»ç»Ÿæ›´æ–°æ•°æ®åº“ `cluster_meta.is_selected = TRUE`

#### æ–¹å¼äºŒ: å‘½ä»¤è¡Œ (ä¼ ç»Ÿæ¨¡å¼)

**æ­¥éª¤1: ç”Ÿæˆä¸»é¢˜å’ŒæŠ¥å‘Š**

```bash
python scripts/run_phase3_selection.py
```

**è¾“å‡ºæ–‡ä»¶**:
- `data/output/cluster_selection_report.html` - HTMLæµè§ˆç•Œé¢
- `data/output/cluster_selection_report.csv` - CSVè¯„åˆ†è¡¨
- `data/output/phase3_statistics_report.txt` - ç»Ÿè®¡æ‘˜è¦

**æ­¥éª¤2: äººå·¥è¯„åˆ†**

æ‰“å¼€ `cluster_selection_report.csv`,åœ¨ `selection_score` åˆ—å¡«å†™åˆ†æ•° (1-5):

| cluster_id | size | main_theme | example_phrases | selection_score |
|------------|------|------------|-----------------|-----------------|
| 1174 | 523 | æ±½è½¦ä»ªè¡¨ç›˜ç¬¦å·æŸ¥è¯¢ | dashboard, car dashboard | **5** |
| 1244 | 412 | æ¨¡æ‹Ÿå™¨æ¸¸æˆéœ€æ±‚ | simulator, flight simulator | **4** |
| 1020 | 387 | å­¦æœ¯å¼•ç”¨æ ¼å¼æŒ‡å— | citation format, apa format | **3** |

**è¯„åˆ†æ ‡å‡†**:
- **5åˆ†**: éå¸¸æœ‰ä»·å€¼,å¿…é¡»åš
- **4åˆ†**: æœ‰ä»·å€¼,åº”è¯¥åš
- **3åˆ†**: ä¸€èˆ¬éœ€æ±‚,å¯ä»¥åš
- **2åˆ†**: ä»·å€¼è¾ƒä½,ä¸ä¼˜å…ˆ
- **1åˆ†**: æ— ä»·å€¼æˆ–ä¸ç›¸å…³

**æ­¥éª¤3: å¯¼å…¥è¯„åˆ†**

```bash
python scripts/import_selection.py
```

ç³»ç»Ÿä¼šå°† `selection_score >= 4` çš„ç°‡æ ‡è®°ä¸º `is_selected = TRUE`ã€‚

### åº•å±‚é€»è¾‘

#### 3.1 LLMä¸»é¢˜ç”Ÿæˆ

```python
# ai/client.py
class LLMClient:
    def generate_cluster_theme(self, example_phrases, cluster_size):
        # æ„å»ºPrompt
        phrases_str = "\n".join([f"- {phrase}" for phrase in example_phrases[:10]])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢å…³é”®è¯åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æœç´¢çŸ­è¯­èšç±»ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸»é¢˜æ ‡ç­¾ã€‚

ã€èšç±»ä¿¡æ¯ã€‘
- èšç±»å¤§å°: {cluster_size} æ¡çŸ­è¯­
- ç¤ºä¾‹çŸ­è¯­ï¼ˆå‰10æ¡ï¼‰:
{phrases_str}

ã€è¦æ±‚ã€‘
1. ä¸»é¢˜æ ‡ç­¾åº”è¯¥æ˜¯2-6ä¸ªè¯ï¼Œç®€æ´æ¸…æ™°
2. ç”¨ä¸­æ–‡æè¿°è¯¥èšç±»çš„æ ¸å¿ƒæ„å›¾æˆ–ä¸»é¢˜
3. æ ‡ç­¾åº”è¯¥èƒ½å¤Ÿæ¦‚æ‹¬å¤§éƒ¨åˆ†ç¤ºä¾‹çŸ­è¯­çš„å…±åŒç‚¹
4. è¿”å›JSONæ ¼å¼: {{"theme": "ä¸»é¢˜æ ‡ç­¾", "confidence": "high|medium|low"}}

è¯·ä¸ºä¸Šè¿°èšç±»ç”Ÿæˆä¸»é¢˜æ ‡ç­¾:"""

        # è°ƒç”¨LLM
        response = self._call_llm([{"role": "user", "content": prompt}], temperature=0.3)

        # è§£æJSON
        result = json.loads(response.strip())
        return {
            "theme": result.get("theme", "æœªåˆ†ç±»"),
            "confidence": result.get("confidence", "medium")
        }
```

**Promptè®¾è®¡è¦ç‚¹**:
- **è§’è‰²è®¾å®š**: "æœç´¢å…³é”®è¯åˆ†æä¸“å®¶"
- **ä»»åŠ¡æ˜ç¡®**: "ç”Ÿæˆä¸»é¢˜æ ‡ç­¾"
- **æ ¼å¼çº¦æŸ**: 2-6è¯,ä¸­æ–‡,JSONæ ¼å¼
- **ç¤ºä¾‹å¼•å¯¼**: æä¾›10æ¡ä»£è¡¨æ€§çŸ­è¯­
- **ä½æ¸©åº¦**: temperature=0.3,å‡å°‘éšæœºæ€§

#### 3.2 è´¨é‡è‡ªåŠ¨è¯„åˆ†

è´¨é‡è¯„åˆ†ç®—æ³•åœ¨ MVP ç‰ˆæœ¬ä¸­å®æ–½ (Phase 1ä¼˜åŒ–),åŸºäº7ä¸ªç»´åº¦:

```python
# å®Œæ•´ç®—æ³•è§ docs/Phase1_Implementation_Summary.md

def calculate_quality_score(cluster):
    score = 0

    # 1. ç°‡å¤§å° (20åˆ†)
    size_score = min(cluster.size / 50 * 20, 20)

    # 2. å¯†åº¦/èšåˆåº¦ (20åˆ†) - åŸºäºçŸ­è¯­ç›¸ä¼¼åº¦
    density_score = calculate_density(cluster.phrases) * 20

    # 3. çŸ­è¯­å¤šæ ·æ€§ (15åˆ†) - åŸºäºç¼–è¾‘è·ç¦»
    diversity_score = calculate_diversity(cluster.phrases) * 15

    # 4. æœç´¢é‡åˆ†å¸ƒ (15åˆ†) - CVç³»æ•°
    volume_score = calculate_volume_distribution(cluster.volumes) * 15

    # 5. çŸ­è¯­é•¿åº¦åˆ†å¸ƒ (10åˆ†)
    length_score = calculate_length_distribution(cluster.phrases) * 10

    # 6. å¸¸è§è¯å æ¯” (10åˆ†) - é¿å…è¿‡äºå®½æ³›
    common_word_score = (1 - calculate_common_word_ratio(cluster.phrases)) * 10

    # 7. ç‰¹æ®Šå­—ç¬¦å æ¯” (10åˆ†) - é¿å…åƒåœ¾æ•°æ®
    special_char_score = (1 - calculate_special_char_ratio(cluster.phrases)) * 10

    total_score = sum([size_score, density_score, diversity_score,
                       volume_score, length_score, common_word_score,
                       special_char_score])

    return round(total_score, 1)
```

**è¯„åˆ†è§£é‡Š**:
- **80-100åˆ† (Excellent)**: é«˜è´¨é‡ç°‡,å¼ºçƒˆæ¨è
- **60-79åˆ† (Good)**: è‰¯å¥½ç°‡,å¯è€ƒè™‘
- **40-59åˆ† (Fair)**: ä¸€èˆ¬ç°‡,éœ€äººå·¥åˆ¤æ–­
- **0-39åˆ† (Poor)**: ä½è´¨é‡ç°‡,ä¸æ¨è

#### 3.3 æ„å›¾åˆ†æ

ç³»ç»Ÿè‡ªåŠ¨åˆ†ææ¯ä¸ªç°‡çš„ä¸»å¯¼æ„å›¾ (Phase 3åŠŸèƒ½):

```python
# core/intent_classification.py

INTENT_PATTERNS = {
    'find_tool': ['best', 'top', 'good', 'recommend', 'which'],
    'learn_how': ['how to', 'tutorial', 'guide', 'learn', 'teach'],
    'solve_problem': ['fix', 'not working', 'error', 'problem', 'issue'],
    'find_free': ['free', 'gratis', 'no cost', 'without paying'],
    'compare': ['vs', 'versus', 'difference', 'compare', 'or'],
    'other': []  # å…¶ä»–
}

def classify_cluster_intent(phrases):
    # ç»Ÿè®¡æ¯ç§æ„å›¾çš„åŒ¹é…çŸ­è¯­æ•°
    intent_counts = {intent: 0 for intent in INTENT_PATTERNS}

    for phrase in phrases:
        phrase_lower = phrase.lower()
        for intent, patterns in INTENT_PATTERNS.items():
            if any(pattern in phrase_lower for pattern in patterns):
                intent_counts[intent] += 1
                break
        else:
            intent_counts['other'] += 1

    # è®¡ç®—ä¸»å¯¼æ„å›¾
    total = len(phrases)
    intent_distribution = {k: v/total for k, v in intent_counts.items()}
    dominant_intent = max(intent_distribution, key=intent_distribution.get)
    confidence = intent_distribution[dominant_intent]

    # åˆ¤æ–­æ˜¯å¦å‡è¡¡ (æ— å•ä¸€æ„å›¾å æ¯”>60%)
    is_balanced = confidence < 0.6

    return {
        'dominant_intent': dominant_intent,
        'confidence': confidence,
        'distribution': intent_distribution,
        'is_balanced': is_balanced
    }
```

#### 3.4 æ•°æ®åº“æ›´æ–°

```python
# ä¿å­˜ä¸»é¢˜
with ClusterMetaRepository() as repo:
    cluster = repo.session.query(ClusterMeta).filter_by(cluster_id=1174).first()
    cluster.main_theme = "æ±½è½¦ä»ªè¡¨ç›˜ç¬¦å·æŸ¥è¯¢"
    repo.session.commit()

# ä¿å­˜é€‰ä¸­çŠ¶æ€
with ClusterMetaRepository() as repo:
    repo.update_selection(
        cluster_id=1174,
        cluster_level='A',
        is_selected=True,
        selection_score=5
    )
```

### è¾“å‡ºç»“æœ

#### æ•°æ®åº“çŠ¶æ€

```sql
-- æŸ¥çœ‹é€‰ä¸­çš„ç°‡
SELECT
    cluster_id,
    size,
    main_theme,
    quality_score,
    dominant_intent,
    is_selected
FROM cluster_meta
WHERE cluster_level = 'A' AND is_selected = TRUE
ORDER BY quality_score DESC;

-- è¾“å‡ºç¤ºä¾‹:
-- cluster_id | size | main_theme | quality_score | dominant_intent | is_selected
-- 1174       | 523  | æ±½è½¦ä»ªè¡¨ç›˜ | 87.3         | find_tool       | TRUE
-- 1244       | 412  | æ¨¡æ‹Ÿå™¨æ¸¸æˆ | 84.1         | learn_how       | TRUE
```

#### ç»Ÿè®¡æŠ¥å‘Š

`data/output/phase3_statistics_report.txt`:

```
======================================================================
Phase 3 å¤§ç»„ç­›é€‰æŠ¥å‘Š - ç»Ÿè®¡æ‘˜è¦
======================================================================

ã€åŸºæœ¬ç»Ÿè®¡ã€‘
  æ€»èšç±»æ•°: 307
  æ€»çŸ­è¯­æ•°: 14,766
  æ€»é¢‘æ¬¡: 234,521

ã€è´¨é‡åˆ†å¸ƒã€‘
  Excellent (80-100åˆ†): 201 ç°‡ (65.5%)
  Good (60-79åˆ†): 106 ç°‡ (34.5%)
  Fair (40-59åˆ†): 0 ç°‡
  Poor (0-39åˆ†): 0 ç°‡
  å¹³å‡è´¨é‡åˆ†: 76.4

ã€æ„å›¾åˆ†å¸ƒã€‘
  other: 224 ç°‡ (73.0%)
  find_tool: 51 ç°‡ (16.6%)
  learn_how: 21 ç°‡ (6.8%)
  find_free: 10 ç°‡ (3.3%)
  compare: 1 ç°‡ (0.3%)

ã€é€‰ä¸­ç»Ÿè®¡ã€‘
  å·²é€‰ä¸­: 15 ç°‡
  é€‰ä¸­ç‡: 4.9%
  é€‰ä¸­ç°‡æ€»çŸ­è¯­æ•°: 6,234 (42.2%)

ã€Top 15 é€‰ä¸­å¤§ç»„ã€‘
æ’å   ç°‡ID   è´¨é‡åˆ†  æ„å›¾          å¤§å°   ä¸»é¢˜
------------------------------------------------------------------
1      1174   87.3    find_tool     523    æ±½è½¦ä»ªè¡¨ç›˜ç¬¦å·æŸ¥è¯¢
2      1244   84.1    learn_how     412    æ¨¡æ‹Ÿå™¨æ¸¸æˆå…¥é—¨æŒ‡å—
...

======================================================================
ä¸‹ä¸€æ­¥: è¿è¡Œ Phase 4 ç”Ÿæˆéœ€æ±‚å¡ç‰‡
======================================================================
```

### æ—¶é—´ä¸æˆæœ¬

- **LLMä¸»é¢˜ç”Ÿæˆ**: ~5-10åˆ†é’Ÿ (307ä¸ªç°‡, GPT-4o-mini)
- **è´¨é‡è¯„åˆ†**: ~2-3åˆ†é’Ÿ (æœ¬åœ°è®¡ç®—)
- **æ„å›¾åˆ†æ**: ~1-2åˆ†é’Ÿ (æœ¬åœ°è®¡ç®—)
- **äººå·¥ç­›é€‰**: ~15-30åˆ†é’Ÿ (Web UI) / ~30-60åˆ†é’Ÿ (CSVæ‰‹å·¥)
- **APIæˆæœ¬**: ~$0.15-0.30 (307ç°‡ Ã— $0.0005/ç°‡)

### æœ€ä½³å®è·µ

1. **ä½¿ç”¨Web UIç­›é€‰**: æ¯”CSVæ‰‹å·¥æ¨¡å¼å¿«50-80%
2. **å¤šç»´åº¦ç­›é€‰ç­–ç•¥**:
   - å…ˆæŒ‰è´¨é‡åˆ†é™åºæ’åº
   - ç­›é€‰ Excellent (80-100åˆ†)
   - å†æŒ‰æ„å›¾å¤šæ ·æ€§é€‰æ‹©
   - æœ€ç»ˆé€‰ä¸­15-20ä¸ªç°‡
3. **å…³æ³¨é«˜è´¨é‡+å‡è¡¡ç°‡**: è¿™ç±»ç°‡åŒ…å«å¤šæ ·åŒ–æ„å›¾,æŒ–æ˜ä»·å€¼æ›´é«˜
4. **ä¿å­˜å¿«ç…§**: å¯¼å‡ºé€‰ä¸­ç°‡åˆ—è¡¨,ä¾¿äºåç»­è¿½æº¯

---

## Phase 4: å°ç»„èšç±»ä¸éœ€æ±‚ç”Ÿæˆ

### åŠŸèƒ½è¯´æ˜

å¯¹Phase 3é€‰ä¸­çš„15-20ä¸ªå¤§ç»„,è¿›è¡Œ**äºŒæ¬¡èšç±»**:
1. **å°ç»„èšç±»**: æ¯ä¸ªå¤§ç»„å†…éƒ¨å†èšç±»,ç”Ÿæˆ3-15ä¸ªå°ç»„ (Level B)
2. **LLMéœ€æ±‚ç”Ÿæˆ**: ä¸ºæ¯ä¸ªå°ç»„ç”Ÿæˆç»“æ„åŒ–éœ€æ±‚å¡ç‰‡
3. **Tokenæ¡†æ¶æŒ‡å¯¼** (å¯é€‰): ä½¿ç”¨Phase 5æå–çš„intent/action/object tokensä¼˜åŒ–éœ€æ±‚ç”Ÿæˆè´¨é‡

**é¢„æœŸè¾“å‡º**: 60-150ä¸ªéœ€æ±‚å¡ç‰‡åˆç¨¿

### æ“ä½œæ–¹å¼

#### æ–¹å¼ä¸€: Web UI (æ¨è)

1. å¯¼èˆªåˆ° **Phase 5: éœ€æ±‚ç”Ÿæˆ** é¡µé¢ (æ³¨: UIé¡µé¢åç§°ä¸ºPhase 5,å®é™…æ˜¯Phase 4åŠŸèƒ½)

2. **æŸ¥çœ‹å½“å‰çŠ¶æ€**:
   - é€‰ä¸­å¤§ç»„: 15
   - å·²æœ‰å°ç»„: 129
   - éœ€æ±‚å¡ç‰‡: 129

3. **é…ç½®èšç±»å‚æ•°**:
   - **å°ç»„æœ€å°å¤§å°**: é»˜è®¤5 (è°ƒå¤§â†’æ›´å°‘æ›´å¤§çš„å°ç»„)
   - **å°ç»„æœ€å°æ ·æœ¬æ•°**: é»˜è®¤2

4. **é«˜çº§é€‰é¡¹**:
   - âœ… **å¯ç”¨æ¡†æ¶æ¨¡å¼** (æ¨è): ä½¿ç”¨Tokenæ¡†æ¶æŒ‡å¯¼LLM,æå‡30-50%è´¨é‡
     - éœ€å…ˆè¿è¡ŒPhase 5æå–Token
   - â˜ **è·³è¿‡LLM**: ä»…èšç±»,ä¸ç”Ÿæˆéœ€æ±‚ (æµ‹è¯•ç”¨)
   - â˜ **é™åˆ¶å¤„ç†å¤§ç»„æ•°**: æµ‹è¯•æ¨¡å¼,ä»…å¤„ç†å‰Nä¸ªå¤§ç»„

5. **æŸ¥çœ‹é¢„æœŸç»“æœ**:
   - å¤„ç†å¤§ç»„: 15
   - é¢„è®¡å°ç»„: ~150
   - é¢„è®¡éœ€æ±‚: ~150
   - é¢„è®¡APIæˆæœ¬: $0.45 (GPT-4o-mini)

6. **ç‚¹å‡»"å¼€å§‹ç”Ÿæˆ"**

7. **å®æ—¶æŸ¥çœ‹æ—¥å¿—**:
   ```
   Phase 4: å°ç»„èšç±» + éœ€æ±‚å¡ç‰‡ç”Ÿæˆ
   ======================================================================

   âš™ï¸  æ¡†æ¶æ¨¡å¼: å·²å¯ç”¨
   ğŸ“š åŠ è½½Tokenæ¡†æ¶è¯åº“...
     âœ“ åŠ è½½äº† 2,431 ä¸ªå·²åˆ†ç±»tokens
     åˆ†ç±»ç»Ÿè®¡:
       - intent: 512 ä¸ª
       - action: 678 ä¸ª
       - object: 1,123 ä¸ª
       - other: 118 ä¸ª

   ã€é˜¶æ®µ1ã€‘åŠ è½½é€‰ä¸­çš„å¤§ç»„...
   âœ“ åŠ è½½äº† 15 ä¸ªé€‰ä¸­çš„å¤§ç»„
     å·²å¤„ç†çš„å¤§ç»„: 0 ä¸ª

   è¿›åº¦: 1/15
   ======================================================================
   å¤„ç†å¤§ç»„ 1174: æ±½è½¦ä»ªè¡¨ç›˜ç¬¦å·æŸ¥è¯¢
   ======================================================================

   ã€æ­¥éª¤1ã€‘åŠ è½½å¤§ç»„çŸ­è¯­...
     âœ“ åŠ è½½äº† 523 æ¡çŸ­è¯­

   ã€æ­¥éª¤2ã€‘åŠ è½½embeddings...
     âœ“ åŠ è½½äº† (523, 384) embeddings

   ã€æ­¥éª¤3ã€‘æ‰§è¡Œå°ç»„èšç±»...
     èšç±»å‚æ•°: min_size=5, min_samples=2
     HDBSCANèšç±»å®Œæˆ
     âœ“ ç”Ÿæˆäº† 11 ä¸ªå°ç»„

   ã€æ­¥éª¤4ã€‘æ›´æ–°æ•°æ®åº“...
     âœ“ å·²æ›´æ–° 523/523 æ¡è®°å½•çš„cluster_id_B

   ã€æ­¥éª¤5ã€‘ä¿å­˜å°ç»„å…ƒæ•°æ®...
     âœ“ å·²ä¿å­˜ 11 ä¸ªå°ç»„çš„å…ƒæ•°æ®

   ã€æ­¥éª¤6ã€‘ç”Ÿæˆéœ€æ±‚å¡ç‰‡...

     å°ç»„0æ¡†æ¶åˆ†æ:
       æ„å›¾: best(23), top(18), good(12)
       åŠ¨ä½œ: find(34), search(21), compare(15)
       å¯¹è±¡: dashboard(45), symbol(32), indicator(28)

     âœ“ LLMç”Ÿæˆéœ€æ±‚: "æ±½è½¦ä»ªè¡¨ç›˜ç¬¦å·å«ä¹‰æŸ¥è¯¢"

     å°ç»„1æ¡†æ¶åˆ†æ:
       æ„å›¾: how to(15), learn(12)
       åŠ¨ä½œ: understand(18), read(14)
       å¯¹è±¡: warning light(23), alert(19)

     âœ“ LLMç”Ÿæˆéœ€æ±‚: "ç†è§£æ±½è½¦ä»ªè¡¨ç›˜æŒ‡ç¤ºç¯å«ä¹‰ä¸åº”å¯¹"

     ...

     âœ“ å·²ç”Ÿæˆ 11 ä¸ªéœ€æ±‚å¡ç‰‡

   ã€æ­¥éª¤7ã€‘å°ç»„èšç±»ç»Ÿè®¡:
     - å°ç»„æ•°é‡: 11
     - å™ªéŸ³ç‚¹æ•°: 12 (2.3%)
     - å°ç»„å¤§å°: æœ€å°=5, æœ€å¤§=78, å¹³å‡=46.5

   è¿›åº¦: 2/15
   ...

   âœ… Phase 4 å®Œæˆï¼
   ```

8. **æŸ¥çœ‹ç»“æœ**:
   - ç”Ÿæˆå°ç»„æ•°: 129
   - éœ€æ±‚å¡ç‰‡æ•°: 129
   - éœ€æ±‚CSV: `data/output/demands_draft.csv`

#### æ–¹å¼äºŒ: å‘½ä»¤è¡Œ

```bash
# æ ‡å‡†æ¨¡å¼
python scripts/run_phase4_demands.py

# å¯ç”¨æ¡†æ¶æ¨¡å¼ (æ¨è)
python scripts/run_phase4_demands.py --use-framework

# æµ‹è¯•æ¨¡å¼ (ä»…å¤„ç†2ä¸ªå¤§ç»„)
python scripts/run_phase4_demands.py --test-limit 2

# ä»…èšç±»,ä¸ç”Ÿæˆéœ€æ±‚
python scripts/run_phase4_demands.py --skip-llm

# è‡ªå®šä¹‰èšç±»å‚æ•°
python scripts/run_phase4_demands.py --min-cluster-size 8 --min-samples 3
```

**å‚æ•°è¯´æ˜**:
- `--use-framework`: å¯ç”¨Tokenæ¡†æ¶æ¨¡å¼ (éœ€å…ˆè¿è¡ŒPhase 5)
- `--skip-llm`: è·³è¿‡LLMéœ€æ±‚ç”Ÿæˆ
- `--test-limit N`: ä»…å¤„ç†å‰Nä¸ªå¤§ç»„
- `--min-cluster-size N`: å°ç»„æœ€å°å¤§å° (é»˜è®¤5)
- `--min-samples N`: å°ç»„æœ€å°æ ·æœ¬æ•° (é»˜è®¤2)
- `--round-id N`: æ•°æ®è½®æ¬¡ (é»˜è®¤1)

### åº•å±‚é€»è¾‘

#### 4.1 æ–­ç‚¹ç»­ä¼ æœºåˆ¶

ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹å·²å¤„ç†çš„å¤§ç»„,é¿å…é‡å¤è®¡ç®—:

```python
# scripts/run_phase4_demands.py

# æŸ¥è¯¢å·²æœ‰çš„Level Bå°ç»„
clusters_B_all = repo.get_all_clusters('B')
processed_parents = set([c.parent_cluster_id for c in clusters_B_all if c.parent_cluster_id])

if processed_parents:
    print(f"  å·²å¤„ç†çš„å¤§ç»„: {len(processed_parents)} ä¸ª")
    print(f"  è¿›åº¦: {len(processed_parents)}/{len(selected_clusters)} ({len(processed_parents)/len(selected_clusters)*100:.1f}%)")

    # è¿‡æ»¤å‡ºæœªå¤„ç†çš„å¤§ç»„
    unprocessed_clusters = [c for c in selected_clusters if c.cluster_id not in processed_parents]

    if not unprocessed_clusters:
        print("\nâœ… æ‰€æœ‰å¤§ç»„éƒ½å·²å¤„ç†å®Œæˆï¼")
    else:
        print(f"  æœªå¤„ç†çš„å¤§ç»„: {len(unprocessed_clusters)} ä¸ª")
        print("\nğŸ’¡ æ–­ç‚¹ç»­ä¼ : å°†åªå¤„ç†æœªå®Œæˆçš„å¤§ç»„")
        selected_clusters = unprocessed_clusters
```

**ä¼˜åŠ¿**:
- å¤„ç†å¤±è´¥åå¯å®‰å…¨é‡è¯•
- é¿å…é‡å¤è°ƒç”¨LLM API
- ä¿æŠ¤å·²ç”Ÿæˆçš„éœ€æ±‚å¡ç‰‡

#### 4.2 å°ç»„èšç±» (Level B)

å¯¹æ¯ä¸ªå¤§ç»„å†…çš„çŸ­è¯­è¿›è¡ŒäºŒæ¬¡èšç±»:

```python
# core/clustering.py

def cluster_phrases_small(embeddings, phrases, parent_cluster_id,
                          min_cluster_size=5, min_samples=2):
    """
    å°ç»„èšç±» (Level B)

    Args:
        embeddings: çŸ­è¯­åµŒå…¥å‘é‡
        phrases: çŸ­è¯­åˆ—è¡¨
        parent_cluster_id: çˆ¶å¤§ç»„ID
        min_cluster_size: æœ€å°ç°‡å¤§å° (é»˜è®¤5,æ¯”Level Açš„30å°å¾—å¤š)
        min_samples: æœ€å°æ ·æœ¬æ•° (é»˜è®¤2)

    Returns:
        labels: ç°‡æ ‡ç­¾æ•°ç»„
        cluster_info: ç°‡ä¿¡æ¯å­—å…¸
        clusterer: HDBSCANèšç±»å™¨å¯¹è±¡
    """

    # 1. L2å½’ä¸€åŒ–
    embeddings_normalized = normalize(embeddings, norm='l2')

    # 2. HDBSCANèšç±»
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,  # æ›´å°çš„æœ€å°ç°‡å¤§å°
        min_samples=min_samples,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True
    )

    labels = clusterer.fit_predict(embeddings_normalized)

    # 3. æå–ç°‡ä¿¡æ¯
    cluster_info = {}
    for label in set(labels):
        if label == -1:  # è·³è¿‡å™ªéŸ³ç‚¹
            continue

        mask = (labels == label)
        cluster_phrases = [phrases[i] for i, m in enumerate(mask) if m]

        cluster_info[label] = {
            'size': len(cluster_phrases),
            'total_frequency': sum(p['frequency'] for p in cluster_phrases),
            'total_volume': sum(p.get('volume', 0) for p in cluster_phrases),
            'example_phrases': [p['phrase'] for p in cluster_phrases[:10]],
            'all_phrases': [p['phrase'] for p in cluster_phrases]
        }

    return labels, cluster_info, clusterer
```

**Level A vs Level B å¯¹æ¯”**:

| ç»´åº¦ | Level A (å¤§ç»„) | Level B (å°ç»„) |
|------|---------------|---------------|
| min_cluster_size | 30 | 5 |
| min_samples | 3 | 2 |
| é¢„æœŸç°‡æ•° | 60-100 | 3-15 (æ¯ä¸ªå¤§ç»„) |
| ç°‡å¹³å‡å¤§å° | 100-200 | 20-50 |
| è¯­ä¹‰ç²’åº¦ | ç²—ç²’åº¦ | ç»†ç²’åº¦ |

#### 4.3 cluster_id_Bç¼–ç 

å°ç»„IDä½¿ç”¨ç‰¹æ®Šç¼–ç æ–¹æ¡ˆ,ç¡®ä¿å…¨å±€å”¯ä¸€:

```python
# cluster_id_B = parent_cluster_id * 10000 + local_label

# ç¤ºä¾‹:
# å¤§ç»„1174çš„ç¬¬0ä¸ªå°ç»„: cluster_id_B = 1174 * 10000 + 0 = 11740000
# å¤§ç»„1174çš„ç¬¬5ä¸ªå°ç»„: cluster_id_B = 1174 * 10000 + 5 = 11740005
# å¤§ç»„1244çš„ç¬¬0ä¸ªå°ç»„: cluster_id_B = 1244 * 10000 + 0 = 12440000

for label, info in cluster_info.items():
    cluster_id_B = parent_cluster_id * 10000 + label

    repo.create_or_update_cluster(
        cluster_id=cluster_id_B,
        cluster_level='B',
        size=info['size'],
        example_phrases='; '.join(info['example_phrases']),
        parent_cluster_id=parent_cluster_id
    )
```

**ä¼˜åŠ¿**:
- æ— éœ€é¢å¤–çš„è‡ªå¢ID
- ä»cluster_id_Bå¯ç›´æ¥æ¨å¯¼parent_cluster_id: `parent_id = cluster_id_B // 10000`
- é¿å…IDå†²çª

#### 4.4 Tokenæ¡†æ¶æå– (æ¡†æ¶æ¨¡å¼)

å¦‚æœå¯ç”¨ `--use-framework`,ç³»ç»Ÿä¼šå…ˆåŠ è½½Phase 5çš„Tokenè¯åº“:

```python
# scripts/run_phase4_demands.py

def load_token_framework():
    """ä»æ•°æ®åº“åŠ è½½Tokenæ¡†æ¶è¯åº“"""
    with TokenRepository() as repo:
        tokens = repo.get_all_tokens()

    tokens_classified = {
        token.token_text: token.token_type
        for token in tokens
    }

    # è¾“å‡ºç¤ºä¾‹:
    # {
    #     "best": "intent",
    #     "top": "intent",
    #     "find": "action",
    #     "download": "action",
    #     "calculator": "object",
    #     "dashboard": "object",
    #     ...
    # }

    return tokens_classified
```

ç„¶åä¸ºæ¯ä¸ªå°ç»„æå–æ¡†æ¶:

```python
def extract_cluster_framework(phrases, tokens_classified):
    """æå–å°ç»„çš„éœ€æ±‚æ¡†æ¶"""
    framework = {
        'intent': {},   # {token: count}
        'action': {},
        'object': {},
        'other': {}
    }

    for phrase in phrases:
        tokens = extract_tokens_from_phrase(phrase)  # åˆ†è¯
        for token in tokens:
            token_type = tokens_classified.get(token, 'other')
            if token_type in framework:
                framework[token_type][token] = framework[token_type].get(token, 0) + 1

    # æ’åºå¹¶å–Top 10
    for token_type in framework:
        sorted_tokens = sorted(framework[token_type].items(), key=lambda x: x[1], reverse=True)
        framework[token_type] = sorted_tokens[:10]

    return framework

# è¾“å‡ºç¤ºä¾‹:
# {
#     'intent': [('best', 23), ('top', 18), ('good', 12), ...],
#     'action': [('find', 34), ('search', 21), ('compare', 15), ...],
#     'object': [('dashboard', 45), ('symbol', 32), ('indicator', 28), ...]
# }
```

#### 4.5 LLMéœ€æ±‚ç”Ÿæˆ

**ä¼ ç»Ÿæ¨¡å¼** (ä¸ä½¿ç”¨æ¡†æ¶):

```python
# ai/client.py

def generate_demand_card(cluster_id_A, cluster_id_B, main_theme, phrases,
                        total_frequency, total_volume, framework=None):
    """ç”Ÿæˆéœ€æ±‚å¡ç‰‡"""

    # é‡‡æ ·30æ¡çŸ­è¯­
    phrases_sample = phrases[:30]
    phrases_str = "\n".join([f"- {phrase}" for phrase in phrases_sample])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªäº§å“éœ€æ±‚åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æœç´¢çŸ­è¯­èšç±»ï¼Œç”Ÿæˆä¸€ä¸ªç”¨æˆ·éœ€æ±‚å¡ç‰‡ã€‚

ã€èšç±»ä¿¡æ¯ã€‘
- å¤§ç»„ä¸»é¢˜: {main_theme}
- å°ç»„ID: {cluster_id_B}
- çŸ­è¯­æ•°é‡: {len(phrases)} æ¡
- æ€»é¢‘æ¬¡: {total_frequency:,}
- æ€»æœç´¢é‡: {total_volume:,}

ã€ç¤ºä¾‹çŸ­è¯­ï¼ˆå‰30æ¡ï¼‰ã€‘
{phrases_str}

ã€è¦æ±‚ã€‘
è¯·ç”Ÿæˆä¸€ä¸ªJSONæ ¼å¼çš„éœ€æ±‚å¡ç‰‡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

{{
  "demand_title": "éœ€æ±‚æ ‡é¢˜ï¼ˆ5-15å­—ï¼Œç®€æ´æœ‰åŠ›ï¼‰",
  "demand_description": "éœ€æ±‚æè¿°ï¼ˆ50-200å­—ï¼Œæè¿°ç”¨æˆ·æ„å›¾å’Œç—›ç‚¹ï¼‰",
  "user_intent": "ç”¨æˆ·æ„å›¾ï¼ˆ20-50å­—ï¼Œç”¨æˆ·æƒ³è¦åšä»€ä¹ˆï¼‰",
  "pain_points": ["ç—›ç‚¹1", "ç—›ç‚¹2", "ç—›ç‚¹3"],
  "target_audience": "ç›®æ ‡ç”¨æˆ·ï¼ˆ20-50å­—ï¼‰",
  "priority": "ä¼˜å…ˆçº§ï¼ˆhigh/medium/lowï¼‰",
  "confidence_score": ç½®ä¿¡åº¦ï¼ˆ0-100ï¼Œæ•´æ•°ï¼‰
}}

è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜:"""

    response = self._call_llm([{"role": "user", "content": prompt}], temperature=0.5)
    result = json.loads(response.strip())

    return result
```

**æ¡†æ¶æ¨¡å¼** (ä½¿ç”¨Tokenæ¡†æ¶):

```python
# åœ¨Promptä¸­æ’å…¥æ¡†æ¶ä¿¡æ¯
if framework:
    framework_section = "\nã€éœ€æ±‚æ¡†æ¶åˆ†æã€‘\nè¯¥å°ç»„çš„å…³é”®è¯åˆ†æç»“æœï¼š\n"

    if framework.get('intent'):
        intent_tokens = [f"{token}({count}æ¬¡)" for token, count in framework['intent'][:5]]
        framework_section += f"  æ„å›¾è¯: {', '.join(intent_tokens)}\n"

    if framework.get('action'):
        action_tokens = [f"{token}({count}æ¬¡)" for token, count in framework['action'][:5]]
        framework_section += f"  åŠ¨ä½œè¯: {', '.join(action_tokens)}\n"

    if framework.get('object'):
        object_tokens = [f"{token}({count}æ¬¡)" for token, count in framework['object'][:5]]
        framework_section += f"  å¯¹è±¡è¯: {', '.join(object_tokens)}\n"

    framework_section += "\nè¯·åŸºäºä»¥ä¸Šæ¡†æ¶åˆ†æï¼Œç»“åˆç¤ºä¾‹çŸ­è¯­ç”Ÿæˆéœ€æ±‚å¡ç‰‡ã€‚"

    # æ’å…¥åˆ°Promptä¸­
    prompt = prompt.replace("ã€ç¤ºä¾‹çŸ­è¯­ï¼ˆå‰30æ¡ï¼‰ã€‘", f"{framework_section}\n\nã€ç¤ºä¾‹çŸ­è¯­ï¼ˆå‰30æ¡ï¼‰ã€‘")
```

**æ¡†æ¶æ¨¡å¼æ•ˆæœ**:
- âœ… éœ€æ±‚æè¿°æ›´å‡†ç¡® (æå‡30-40%)
- âœ… ç”¨æˆ·æ„å›¾è¯†åˆ«æ›´ç²¾å‡†
- âœ… ç—›ç‚¹æå–æ›´å…·ä½“
- âœ… å‡å°‘LLMå¹»è§‰

#### 4.6 éœ€æ±‚å¡ç‰‡ä¿å­˜

```python
# scripts/run_phase4_demands.py

# è§£æLLMå“åº”
demand_draft = llm.generate_demand_card(...)

# æ˜ å°„priority â†’ business_value
priority_to_business = {
    'high': 'high',
    'medium': 'medium',
    'low': 'low'
}
business_val = priority_to_business.get(
    demand_draft.get('priority', 'medium').lower(),
    'unknown'
)

# ä¿å­˜åˆ°æ•°æ®åº“
with DemandRepository() as demand_repo:
    demand = demand_repo.create_demand(
        title=demand_draft['demand_title'],
        description=demand_draft['demand_description'],
        user_scenario=demand_draft['user_intent'],
        demand_type='other',  # é»˜è®¤ä¸ºother,åç»­äººå·¥ä¿®æ”¹ä¸ºtool/content/service/education
        source_cluster_A=cluster_id,
        source_cluster_B=cluster_id_B,
        related_phrases_count=info['size'],
        business_value=business_val,
        status='idea'  # åˆå§‹çŠ¶æ€
    )

    # å…³é”®: ä¼šè¯å…³é—­å‰åˆ†ç¦»å¯¹è±¡,ä½†ä¿ç•™å±æ€§
    demand_repo.session.expunge(demand)
    demands.append(demand)
```

**Bugä¿®å¤è¯´æ˜** (å·²åœ¨ä¹‹å‰ä¼šè¯ä¿®å¤):
1. **demand_typeå­—æ®µ**: ä¸ä»LLMçš„priorityæ˜ å°„,å›ºå®šä¸º'other'
2. **Sessionç®¡ç†**: ä½¿ç”¨`expunge()`é˜²æ­¢Sessionå…³é—­åè®¿é—®é”™è¯¯
3. **æ–­ç‚¹ç»­ä¼ **: æ£€æµ‹å·²å¤„ç†å¤§ç»„,é¿å…é‡å¤

#### 4.7 ç”ŸæˆCSVæŠ¥å‘Š

```python
# ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰éœ€æ±‚ (ä¸åªæ˜¯æœ¬æ¬¡ç”Ÿæˆçš„)
with DemandRepository() as demand_repo:
    all_demands_from_db = demand_repo.session.query(Demand).order_by(Demand.demand_id).all()

if all_demands_from_db:
    data = []
    for demand in all_demands_from_db:
        data.append({
            'demand_id': demand.demand_id,
            'title': demand.title,
            'description': demand.description,
            'user_scenario': demand.user_scenario,
            'demand_type': demand.demand_type,
            'source_cluster_A': demand.source_cluster_A,
            'source_cluster_B': demand.source_cluster_B,
            'related_phrases_count': demand.related_phrases_count,
            'business_value': demand.business_value,
            'status': demand.status,
        })

    df = pd.DataFrame(data)
    csv_file = OUTPUT_DIR / 'demands_draft.csv'
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')

    print(f"  âœ“ æ•°æ®åº“ä¸­å…±æœ‰ {len(all_demands_from_db)} ä¸ªéœ€æ±‚å¡ç‰‡")
    print(f"  âœ“ æœ¬æ¬¡è¿è¡Œæ–°å¢ {len(new_demands)} ä¸ªéœ€æ±‚å¡ç‰‡")
```

### è¾“å‡ºç»“æœ

#### æ•°æ®åº“çŠ¶æ€

```sql
-- æŸ¥çœ‹å°ç»„åˆ†å¸ƒ
SELECT
    parent_cluster_id AS cluster_A,
    COUNT(*) AS small_group_count
FROM cluster_meta
WHERE cluster_level = 'B'
GROUP BY parent_cluster_id
ORDER BY small_group_count DESC;

-- è¾“å‡ºç¤ºä¾‹:
-- cluster_A | small_group_count
-- 1174      | 11
-- 1244      | 9
-- 1020      | 8

-- æŸ¥çœ‹éœ€æ±‚å¡ç‰‡
SELECT
    demand_id,
    title,
    demand_type,
    business_value,
    status,
    source_cluster_A,
    source_cluster_B
FROM demands
LIMIT 10;
```

#### CSVæ–‡ä»¶

`data/output/demands_draft.csv`:

| demand_id | title | description | user_scenario | demand_type | source_cluster_A | source_cluster_B | related_phrases_count | business_value | status |
|-----------|-------|-------------|---------------|-------------|------------------|------------------|-----------------------|----------------|--------|
| 1 | æ±½è½¦ä»ªè¡¨ç›˜ç¬¦å·å«ä¹‰æŸ¥è¯¢ | ç”¨æˆ·é¢‘ç¹æœç´¢æ±½è½¦ä»ªè¡¨ç›˜ä¸Šå„ç§ç¬¦å·ã€æŒ‡ç¤ºç¯çš„å«ä¹‰å’Œè§£é‡Š... | ç”¨æˆ·å¸Œæœ›å¿«é€Ÿè¯†åˆ«å¹¶ç†è§£æ±½è½¦ä»ªè¡¨ç›˜ä¸Šå‡ºç°çš„ç¬¦å·... | other | 1174 | 11740000 | 6 | high | idea |
| 2 | æ–°æ‰‹å‹å¥½çš„Home Assistantä»ªè¡¨æ¿éœ€æ±‚ | ç”¨æˆ·ï¼ˆå°¤å…¶æ˜¯åˆå­¦è€…ï¼‰åœ¨å¯»æ‰¾æˆ–åˆ›å»ºHome Assistantä»ªè¡¨æ¿æ—¶... | ç”¨æˆ·å¸Œæœ›æ‰¾åˆ°æˆ–åˆ›å»ºä¸€ä¸ªé€‚åˆåˆå­¦è€…çš„... | other | 1174 | 11740001 | 5 | medium | idea |

#### ç»Ÿè®¡æŠ¥å‘Š

`data/output/phase4_demands_report.txt`:

```
======================================================================
Phase 4 å°ç»„èšç±»ä¸éœ€æ±‚å¡ç‰‡æŠ¥å‘Š
======================================================================

ã€å¤„ç†æ¦‚å†µã€‘
  é€‰ä¸­å¤§ç»„æ•°: 15
  æˆåŠŸå¤„ç†: 15

ã€å°ç»„èšç±»ç»Ÿè®¡ã€‘
  æ€»å°ç»„æ•°: 129

ã€éœ€æ±‚å¡ç‰‡ç»Ÿè®¡ã€‘
  æ€»éœ€æ±‚æ•°: 129

ã€å„å¤§ç»„éœ€æ±‚æ•°é‡ã€‘
  å¤§ç»„1174: 11 ä¸ªéœ€æ±‚
  å¤§ç»„1244: 9 ä¸ªéœ€æ±‚
  å¤§ç»„1020: 8 ä¸ªéœ€æ±‚
  ...

======================================================================
ä¸‹ä¸€æ­¥: åœ¨ demands_draft.csv ä¸­å®¡æ ¸å¹¶ä¿®æ”¹éœ€æ±‚å¡ç‰‡
======================================================================
```

### æ—¶é—´ä¸æˆæœ¬

**ä¸ä½¿ç”¨æ¡†æ¶æ¨¡å¼**:
- å°ç»„èšç±»: ~5-10åˆ†é’Ÿ (15å¤§ç»„)
- LLMéœ€æ±‚ç”Ÿæˆ: ~10-20åˆ†é’Ÿ (129å°ç»„ Ã— ~5ç§’/å°ç»„)
- æ€»è€—æ—¶: ~20-30åˆ†é’Ÿ
- APIæˆæœ¬: ~$0.40-0.65 (GPT-4o-mini, 129å°ç»„ Ã— ~$0.003-0.005/å°ç»„)

**ä½¿ç”¨æ¡†æ¶æ¨¡å¼**:
- åŠ è½½Tokenè¯åº“: ~2ç§’
- æå–å°ç»„æ¡†æ¶: ~5-10ç§’/å¤§ç»„
- LLMéœ€æ±‚ç”Ÿæˆ: åŒä¸Š (ä½†è´¨é‡æå‡30-50%)
- æ€»è€—æ—¶: ~20-30åˆ†é’Ÿ
- APIæˆæœ¬: ~$0.50-0.75 (Promptæ›´é•¿)

### æœ€ä½³å®è·µ

1. **é¦–æ¬¡è¿è¡Œ**:
   - ä½¿ç”¨ `--test-limit 2` æµ‹è¯•2ä¸ªå¤§ç»„
   - æ£€æŸ¥å°ç»„æ•°é‡ã€éœ€æ±‚è´¨é‡
   - æ»¡æ„åå¤„ç†å…¨éƒ¨15ä¸ªå¤§ç»„

2. **å‚æ•°è°ƒä¼˜**:
   - å°ç»„å¤ªå¤š (>15/å¤§ç»„) â†’ å¢å¤§ `min_cluster_size` åˆ° 8-10
   - å°ç»„å¤ªå°‘ (<3/å¤§ç»„) â†’ å‡å° `min_cluster_size` åˆ° 3-4

3. **æ¡†æ¶æ¨¡å¼**:
   - **å¼ºçƒˆæ¨èå…ˆè¿è¡ŒPhase 5æå–Token**
   - æ¡†æ¶æ¨¡å¼éœ€æ±‚è´¨é‡æ˜¾è‘—æå‡
   - é¢å¤–æˆæœ¬çº¦å¢åŠ 20-30%

4. **æ–­ç‚¹ç»­ä¼ **:
   - é‡åˆ°é”™è¯¯ç›´æ¥é‡è¯•,å·²å®Œæˆå·¥ä½œä¼šä¿ç•™
   - ä½¿ç”¨Web UIæŸ¥çœ‹"å·²æœ‰å°ç»„"æ•°é‡ç¡®è®¤è¿›åº¦

5. **äººå·¥å®¡æ ¸**:
   - AIç”Ÿæˆçš„éœ€æ±‚éœ€è¦äººå·¥å®¡æ ¸
   - é‡ç‚¹æ£€æŸ¥: title (æ˜¯å¦å‡†ç¡®), description (æ˜¯å¦å…·ä½“), demand_type (æ˜¯å¦åˆ†ç±»æ­£ç¡®)

---

## Phase 5: Tokenæå–ä¸åˆ†ç±»

### åŠŸèƒ½è¯´æ˜

ä»æ‰€æœ‰çŸ­è¯­ä¸­æå–å•è¯/çŸ­è¯­token,å¹¶ä½¿ç”¨LLMå°†å…¶åˆ†ç±»ä¸ºå››ç§ç±»å‹:
- **intent**: æ„å›¾è¯ (å¦‚ "best", "top", "how to", "free")
- **action**: åŠ¨ä½œè¯ (å¦‚ "download", "buy", "make", "install")
- **object**: å¯¹è±¡è¯ (å¦‚ "shoes", "calculator", "tutorial")
- **other**: å…¶ä»– (æ•°å­—ã€å“ç‰Œåã€åœç”¨è¯ç­‰)

**ä½œç”¨**: ä¸ºPhase 4æ¡†æ¶æ¨¡å¼æä¾›è¯åº“,æŒ‡å¯¼LLMç”Ÿæˆæ›´å‡†ç¡®çš„éœ€æ±‚å¡ç‰‡ã€‚

### æ“ä½œæ–¹å¼

#### æ–¹å¼ä¸€: Web UI

1. å¯¼èˆªåˆ° **Phase 5: Tokenæå–** é¡µé¢

2. é…ç½®å‚æ•°:
   - **Tokenç±»å‹**: é€‰æ‹©è¦åˆ†æçš„ç±»å‹ (intent/action/object/all)
   - **æ‰¹é‡å¤§å°**: LLMæ‰¹å¤„ç†å¤§å° (é»˜è®¤50)
   - **æœ€å°é¢‘æ¬¡**: ä»…ä¿ç•™å‡ºç°Næ¬¡ä»¥ä¸Šçš„token (é»˜è®¤3)

3. ç‚¹å‡» **"å¼€å§‹æå–"**

4. æŸ¥çœ‹æå–è¿›åº¦å’ŒTokenç»Ÿè®¡

#### æ–¹å¼äºŒ: å‘½ä»¤è¡Œ

```bash
# æå–æ‰€æœ‰Token
python scripts/run_phase5_tokens.py

# ä»…æå–intentç±»å‹
python scripts/run_phase5_tokens.py --token-type intent

# æµ‹è¯•æ¨¡å¼ (ä»…æå–å‰100ä¸ªtoken)
python scripts/run_phase5_tokens.py --limit 100
```

**å‚æ•°è¯´æ˜**:
- `--token-type`: æå–ç±»å‹ (intent/action/object/all)
- `--batch-size`: LLMæ‰¹å¤„ç†å¤§å° (é»˜è®¤50)
- `--min-frequency`: æœ€å°å‡ºç°é¢‘æ¬¡ (é»˜è®¤3)
- `--limit`: é™åˆ¶æå–æ•°é‡ (æµ‹è¯•ç”¨)

### åº•å±‚é€»è¾‘

#### 5.1 Tokenæå–

ä»çŸ­è¯­ä¸­æå–å•è¯:

```python
# utils/token_extractor.py

import re
from collections import Counter

def extract_tokens_from_phrase(phrase: str) -> List[str]:
    """
    ä»çŸ­è¯­ä¸­æå–tokens

    ç¤ºä¾‹:
        "best running shoes for women" â†’ ["best", "running", "shoes", "for", "women"]
    """
    # 1. è½¬å°å†™
    phrase = phrase.lower()

    # 2. åˆ†è¯ (å¤„ç†è¿å­—ç¬¦ã€æ–œæ ç­‰)
    tokens = re.findall(r'\b[a-z]+\b', phrase)

    # 3. è¿‡æ»¤åœç”¨è¯
    stopwords = {'a', 'an', 'the', 'for', 'of', 'in', 'on', 'at', 'to', 'with'}
    tokens = [t for t in tokens if t not in stopwords]

    return tokens

def extract_all_tokens(phrases: List[str], min_frequency: int = 3) -> List[Dict]:
    """
    ä»æ‰€æœ‰çŸ­è¯­ä¸­æå–tokenå¹¶ç»Ÿè®¡é¢‘æ¬¡

    Returns:
        [
            {'token': 'best', 'frequency': 234},
            {'token': 'calculator', 'frequency': 189},
            ...
        ]
    """
    token_counter = Counter()

    for phrase in phrases:
        tokens = extract_tokens_from_phrase(phrase)
        token_counter.update(tokens)

    # è¿‡æ»¤ä½é¢‘token
    filtered_tokens = [
        {'token': token, 'frequency': count}
        for token, count in token_counter.items()
        if count >= min_frequency
    ]

    # æŒ‰é¢‘æ¬¡é™åº
    filtered_tokens.sort(key=lambda x: x['frequency'], reverse=True)

    return filtered_tokens
```

#### 5.2 LLMæ‰¹é‡åˆ†ç±»

```python
# ai/client.py

def batch_classify_tokens(tokens: List[str], batch_size: int = 50):
    """
    æ‰¹é‡åˆ†ç±»tokensçš„ç±»å‹

    Args:
        tokens: tokenæ–‡æœ¬åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°

    Returns:
        [
            {'token': 'best', 'token_type': 'intent', 'confidence': 'high'},
            {'token': 'download', 'token_type': 'action', 'confidence': 'high'},
            {'token': 'calculator', 'token_type': 'object', 'confidence': 'high'},
            ...
        ]
    """
    all_results = []

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(tokens), batch_size):
        batch = tokens[i:i + batch_size]

        # æ„å»ºPrompt
        tokens_str = "\n".join([f"{idx+1}. {token}" for idx, token in enumerate(batch)])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªNLPä¸“å®¶ï¼Œè´Ÿè´£å°†æœç´¢å…³é”®è¯ä¸­çš„tokenåˆ†ç±»ã€‚

ã€Tokenåˆ†ç±»æ ‡å‡†ã€‘
- intent: æ„å›¾è¯ï¼ˆå¦‚ "best", "top", "how to", "cheap", "free"ï¼‰
- action: åŠ¨ä½œè¯ï¼ˆå¦‚ "download", "buy", "make", "create", "install"ï¼‰
- object: å¯¹è±¡è¯ï¼ˆå¦‚ "shoes", "phone", "tutorial", "recipe", "software"ï¼‰
- other: å…¶ä»–ï¼ˆæ•°å­—ã€å“ç‰Œåã€åœ°åç­‰ï¼‰

ã€å¾…åˆ†ç±»Tokensã€‘ï¼ˆå…±{len(batch)}ä¸ªï¼‰
{tokens_str}

ã€è¦æ±‚ã€‘
1. ä¸ºæ¯ä¸ªtokenåˆ¤æ–­å…¶ç±»å‹ï¼ˆintent/action/object/otherï¼‰
2. è¿”å›JSONæ•°ç»„æ ¼å¼
3. æ¯ä¸ªå…ƒç´ æ ¼å¼: {{"token": "...", "token_type": "...", "confidence": "high|medium|low"}}

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–è¯´æ˜:"""

        # è°ƒç”¨LLM
        response = self._call_llm([{"role": "user", "content": prompt}], temperature=0.3)

        # è§£æJSON
        results = json.loads(response.strip())
        all_results.extend(results)

    return all_results
```

#### 5.3 ä¿å­˜åˆ°æ•°æ®åº“

```python
# scripts/run_phase5_tokens.py

with TokenRepository() as token_repo:
    for result in llm_results:
        token_repo.create_token(
            token_text=result['token'],
            token_type=result['token_type'],
            in_phrase_count=token_frequency_map[result['token']],
            first_seen_round=1,
            verified=False  # å¾…äººå·¥éªŒè¯
        )
```

### è¾“å‡ºç»“æœ

#### æ•°æ®åº“çŠ¶æ€

```sql
SELECT
    token_type,
    COUNT(*) AS count
FROM tokens
GROUP BY token_type;

-- è¾“å‡ºç¤ºä¾‹:
-- token_type | count
-- intent     | 512
-- action     | 678
-- object     | 1,123
-- other      | 118

-- æŸ¥çœ‹Top 20 intent tokens
SELECT token_text, in_phrase_count
FROM tokens
WHERE token_type = 'intent'
ORDER BY in_phrase_count DESC
LIMIT 20;
```

#### CSVæ–‡ä»¶

`data/output/tokens_classified.csv`:

| token_text | token_type | in_phrase_count | confidence |
|------------|------------|-----------------|------------|
| best | intent | 234 | high |
| top | intent | 189 | high |
| how | intent | 156 | high |
| free | intent | 134 | high |
| download | action | 198 | high |
| buy | action | 145 | high |
| calculator | object | 523 | high |
| shoes | object | 412 | high |

### æ—¶é—´ä¸æˆæœ¬

- Tokenæå–: ~30ç§’ (15KçŸ­è¯­)
- LLMåˆ†ç±»: ~5-15åˆ†é’Ÿ (2,431 tokens / 50 per batch = 49æ‰¹)
- æ€»è€—æ—¶: ~6-16åˆ†é’Ÿ
- APIæˆæœ¬: ~$0.25-0.50 (GPT-4o-mini)

### æœ€ä½³å®è·µ

1. **å…ˆè¿è¡ŒPhase 5å†è¿è¡ŒPhase 4æ¡†æ¶æ¨¡å¼**
2. **äººå·¥éªŒè¯**: å¯¼å‡ºtokens_classified.csv,æ£€æŸ¥å…³é”®tokenåˆ†ç±»æ˜¯å¦å‡†ç¡®
3. **è¿­ä»£ä¼˜åŒ–**: å¦‚æœåˆ†ç±»ä¸å‡†,å¯ä¿®æ”¹Promptæˆ–æ‰‹åŠ¨æ›´æ­£æ•°æ®åº“

---

## Web UIä½¿ç”¨æŒ‡å—

### å¯åŠ¨Web UI

```bash
cd D:\xiangmu\è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜
streamlit run web_ui.py
```

è®¿é—®: http://localhost:8501

### é¡µé¢å¯¼èˆª

#### ğŸ  é¦–é¡µ
- é¡¹ç›®ä»‹ç»
- å¿«é€Ÿå¼€å§‹æŒ‡å—
- ç³»ç»Ÿæ¶æ„å›¾

#### ğŸ“Š Phase 1: æ•°æ®å¯¼å…¥
- ä¸Šä¼ CSVæ–‡ä»¶
- é€‰æ‹©æ•°æ®æºç±»å‹
- å®æ—¶æŸ¥çœ‹å¯¼å…¥è¿›åº¦

#### ğŸ”¬ Phase 2: å¤§ç»„èšç±»
- é…ç½®èšç±»å‚æ•°
- å¯åŠ¨èšç±»ä»»åŠ¡
- æŸ¥çœ‹èšç±»ç»Ÿè®¡

#### ğŸ¯ Phase 3: èšç±»ç­›é€‰
- æŸ¥çœ‹æ‰€æœ‰å¤§ç»„åŠä¸»é¢˜
- å¤šç»´åº¦ç­›é€‰ (è´¨é‡/æ„å›¾/å‡è¡¡åº¦/å¤§å°)
- äº¤äº’å¼é€‰ä¸­/å–æ¶ˆ
- æ‰¹é‡æ“ä½œ

#### ğŸ“‹ Phase 5: éœ€æ±‚ç”Ÿæˆ (å®é™…æ˜¯Phase 4)
- é…ç½®å°ç»„èšç±»å‚æ•°
- å¯ç”¨/ç¦ç”¨æ¡†æ¶æ¨¡å¼
- å®æ—¶æŸ¥çœ‹ç”Ÿæˆæ—¥å¿—
- æµè§ˆéœ€æ±‚å¡ç‰‡

#### ğŸ”¤ Phase 5: Tokenæå–
- é…ç½®Tokenæå–å‚æ•°
- æ‰¹é‡åˆ†ç±»Token
- æŸ¥çœ‹åˆ†ç±»ç»“æœ

### å¸¸ç”¨æ“ä½œ

#### å¿«é€Ÿç­›é€‰é«˜è´¨é‡ç°‡

1. è¿›å…¥Phase 3é¡µé¢
2. è´¨é‡ç­›é€‰: é€‰æ‹©"Excellent"
3. æ’åº: æŒ‰è´¨é‡åˆ†é™åº
4. å‹¾é€‰Top 15ä¸ªç°‡
5. ç‚¹å‡»"ä¿å­˜é€‰ä¸­"

#### æŸ¥çœ‹éœ€æ±‚è¯¦æƒ…

1. è¿›å…¥Phase 5 (éœ€æ±‚ç”Ÿæˆ) é¡µé¢
2. æ»šåŠ¨åˆ°"å·²ç”Ÿæˆçš„éœ€æ±‚å¡ç‰‡"éƒ¨åˆ†
3. è¾“å…¥éœ€æ±‚ID
4. ç‚¹å‡»"æŸ¥çœ‹è¯¦æƒ…"

#### å¯¼å‡ºæ•°æ®

æ‰€æœ‰CSVæ–‡ä»¶ä½äº `data/output/`:
- `cluster_selection_report.csv` - èšç±»ç­›é€‰è¡¨
- `demands_draft.csv` - éœ€æ±‚å¡ç‰‡CSV
- `tokens_classified.csv` - Tokenåˆ†ç±»è¡¨

---

## åº•å±‚æŠ€æœ¯æ¶æ„

### ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å‰ç«¯å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Streamlit    â”‚         â”‚  å‘½ä»¤è¡Œæ¥å£      â”‚          â”‚
â”‚  â”‚ Web UI       â”‚         â”‚  (Scripts)      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â–¼         ä¸šåŠ¡é€»è¾‘å±‚        â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Phase Controller (run_phase*.py)        â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                 â”‚                                       â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚     â–¼                       â–¼               â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Clusteringâ”‚  â”‚Embedding Svc â”‚     â”‚LLM Clientâ”‚     â”‚
â”‚  â”‚Engine    â”‚  â”‚(Sentence-T)  â”‚     â”‚(OpenAI)  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â–¼    æ•°æ®æŒä¹…å±‚     â–¼                â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚       Repository Layer (SQLAlchemy ORM)       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚PhraseRepo  â”‚  â”‚ClusterRepo â”‚  â”‚DemandRe â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          MySQL / SQLite Database              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚  â”‚phrases  â”‚ â”‚cluster_metaâ”‚ â”‚demands   â”‚     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â–¼        å¤–éƒ¨æœåŠ¡å±‚             â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚Sentence-T    â”‚                â”‚ LLM API       â”‚    â”‚
â”‚  â”‚Model (Local) â”‚                â”‚ (OpenAI/etc)  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—

#### 1. Embedding Service

**æ–‡ä»¶**: `core/embedding.py`

**åŠŸèƒ½**:
- ä½¿ç”¨Sentence Transformersè®¡ç®—æ–‡æœ¬åµŒå…¥
- 384ç»´å‘é‡ç©ºé—´
- æ”¯æŒæ‰¹é‡å¤„ç†å’Œç¼“å­˜

**å…³é”®ä»£ç **:
```python
class EmbeddingService:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def encode(self, texts: List[str]) -> np.ndarray:
        return self.model.encode(
            texts,
            batch_size=256,
            normalize_embeddings=True
        )
```

#### 2. Clustering Engine

**æ–‡ä»¶**: `core/clustering.py`

**ç®—æ³•**: HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)

**åŸç†**:
1. è®¡ç®—kè¿‘é‚»è·ç¦» â†’ å¯†åº¦ä¼°è®¡
2. æ„å»ºæœ€å°ç”Ÿæˆæ ‘ â†’ å±‚æ¬¡ç»“æ„
3. ä½¿ç”¨EOMæå–ç¨³å®šç°‡ â†’ èšç±»ç»“æœ
4. æ ‡è®°ä½å¯†åº¦ç‚¹ä¸ºå™ªéŸ³

**ä¼˜åŠ¿**:
- ä¸éœ€é¢„è®¾ç°‡æ•°é‡
- è‡ªåŠ¨è¯†åˆ«å™ªéŸ³ç‚¹
- é€‚åˆä¸è§„åˆ™å½¢çŠ¶ç°‡
- å±‚æ¬¡åŒ–ç»“æ„

#### 3. LLM Client

**æ–‡ä»¶**: `ai/client.py`

**æ”¯æŒ**:
- OpenAI (GPT-4o-mini, GPT-4o)
- Anthropic (Claude 3.5 Sonnet)
- DeepSeek (DeepSeek-V2)

**é‡è¯•æœºåˆ¶**:
```python
@retry(max_attempts=3, delay=1, backoff=2, exceptions=(ConnectionError, TimeoutError))
def _call_llm(self, messages, temperature, max_tokens):
    # APIè°ƒç”¨
    pass
```

#### 4. Repository Layer

**æ–‡ä»¶**: `storage/repository.py`

**æ¨¡å¼**: Repositoryæ¨¡å¼ + ORM

**ä¼˜åŠ¿**:
- éš”ç¦»ä¸šåŠ¡é€»è¾‘ä¸æ•°æ®åº“ç»†èŠ‚
- æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨ (`with` statement)
- æ‰¹é‡æ“ä½œä¼˜åŒ–

**ç¤ºä¾‹**:
```python
with PhraseRepository() as repo:
    phrases = repo.get_phrases_by_cluster(cluster_id=1174, cluster_level='A')
    # Sessionè‡ªåŠ¨ç®¡ç†
```

### æ•°æ®åº“è®¾è®¡

#### ERå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    phrases      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ phrase_id (PK)  â”‚
â”‚ phrase (UK)     â”‚
â”‚ source_type     â”‚
â”‚ cluster_id_A â—„â”€â”€â”¼â”€â”
â”‚ cluster_id_B â—„â”€â”€â”¼â”€â”¼â”€â”
â”‚ mapped_demand_idâ”œâ”€â”¼â”€â”¼â”€â”
â”‚ ...             â”‚ â”‚ â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚
                     â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚
â”‚ cluster_meta    â”‚ â”‚ â”‚ â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚ â”‚ â”‚
â”‚ cluster_id (PK)â”œâ”€â”˜ â”‚ â”‚
â”‚ cluster_level   â”‚   â”‚ â”‚
â”‚ size            â”‚   â”‚ â”‚
â”‚ main_theme      â”‚   â”‚ â”‚
â”‚ parent_cluster_idâ”‚  â”‚ â”‚
â”‚ quality_score   â”‚   â”‚ â”‚
â”‚ is_selected     â”‚   â”‚ â”‚
â”‚ ...             â”‚   â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
                      â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚    demands      â”‚   â”‚ â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚ â”‚
â”‚ demand_id (PK) â”œâ”€â”€â”€â”˜ â”‚
â”‚ title           â”‚     â”‚
â”‚ description     â”‚     â”‚
â”‚ demand_type     â”‚     â”‚
â”‚ business_value  â”‚     â”‚
â”‚ source_cluster_Aâ”œâ”€â”€â”€â”€â”€â”˜
â”‚ source_cluster_Bâ”‚
â”‚ ...             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### è¡¨å…³ç³»

1. **phrases â†’ cluster_meta**:
   - `phrases.cluster_id_A â†’ cluster_meta.cluster_id (WHERE cluster_level='A')`
   - `phrases.cluster_id_B â†’ cluster_meta.cluster_id (WHERE cluster_level='B')`

2. **cluster_meta (Level B) â†’ cluster_meta (Level A)**:
   - `cluster_meta.parent_cluster_id â†’ cluster_meta.cluster_id`

3. **demands â†’ cluster_meta**:
   - `demands.source_cluster_A â†’ cluster_meta.cluster_id (WHERE cluster_level='A')`
   - `demands.source_cluster_B â†’ cluster_meta.cluster_id (WHERE cluster_level='B')`

4. **phrases â†’ demands**:
   - `phrases.mapped_demand_id â†’ demands.demand_id` (å¾…å®ç°)

### ç¼“å­˜æœºåˆ¶

#### Embeddingç¼“å­˜

**æ–‡ä»¶**: `data/cache/embeddings_round{round_id}.npz`

**æ ¼å¼**:
```python
{
    'cache': {
        md5("phrase1"): array([0.1, -0.2, ..., 0.5]),  # 384ç»´
        md5("phrase2"): array([-0.3, 0.4, ..., -0.1]),
        ...
    }
}
```

**ä¼˜åŠ¿**:
- é¿å…é‡å¤è®¡ç®— (èŠ‚çœ3-5åˆ†é’Ÿ/15KçŸ­è¯­)
- æ”¯æŒå¢é‡æ›´æ–°
- å‹ç¼©å­˜å‚¨ (NPZæ ¼å¼)

#### Sessionç¼“å­˜

Web UIä½¿ç”¨Streamlitçš„`@st.cache_data`ç¼“å­˜æ•°æ®åº“æŸ¥è¯¢ç»“æœ:

```python
@st.cache_data(ttl=60)  # ç¼“å­˜60ç§’
def load_clusters():
    with ClusterMetaRepository() as repo:
        return repo.get_all_clusters('A')
```

---

## å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ

### FAQ

#### Q1: èšç±»æ•°é‡å¤ªå¤š (>150) æˆ–å¤ªå°‘ (<50)?

**A**: è°ƒæ•´ `min_cluster_size` å‚æ•°:
- å¤ªå¤š â†’ å¢å¤§åˆ° 40-50
- å¤ªå°‘ â†’ å‡å°åˆ° 20-25

åœ¨ `config/settings.py` ä¿®æ”¹:
```python
LARGE_CLUSTER_CONFIG = {
    "min_cluster_size": 40,  # é»˜è®¤30
    ...
}
```

æˆ–åœ¨å‘½ä»¤è¡ŒæŒ‡å®š:
```bash
python scripts/run_phase2_clustering.py --min-cluster-size 40
```

#### Q2: LLM APIè°ƒç”¨å¤±è´¥

**å¸¸è§åŸå› **:
1. **APIå¯†é’¥é”™è¯¯**: æ£€æŸ¥ `.env` æ–‡ä»¶
2. **ç½‘ç»œé—®é¢˜**: ä½¿ç”¨ä»£ç†æˆ–åˆ‡æ¢base_url
3. **é…é¢ç”¨å°½**: æ£€æŸ¥è´¦æˆ·ä½™é¢
4. **æ¨¡å‹ä¸å­˜åœ¨**: ç¡®è®¤æ¨¡å‹åç§°æ­£ç¡®

**è§£å†³æ–¹æ¡ˆ**:
```python
# config/settings.py
LLM_CONFIG = {
    "openai": {
        "api_key": os.getenv("OPENAI_API_KEY"),  # ä»ç¯å¢ƒå˜é‡è¯»å–
        "base_url": "https://api.openai.com/v1",  # æˆ–è‡ªå®šä¹‰
        "model": "gpt-4o-mini",
    }
}
```

#### Q3: Phase 4ç”Ÿæˆéœ€æ±‚æ•°é‡ä¸º0

**åŸå› **: å‹¾é€‰äº†"è·³è¿‡LLM"

**è§£å†³**: å–æ¶ˆå‹¾é€‰å¹¶é‡æ–°è¿è¡Œ

#### Q4: æ•°æ®åº“è¿æ¥å¤±è´¥

**MySQL**:
```bash
# æ£€æŸ¥MySQLæœåŠ¡
mysql -u root -p
# åˆ›å»ºæ•°æ®åº“
CREATE DATABASE keyword_clustering CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
```

**SQLite**:
```bash
# æ£€æŸ¥æ–‡ä»¶æƒé™
ls -l data/keyword_clustering.db
```

#### Q5: Embeddingè®¡ç®—å¤ªæ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**:
1. **ä½¿ç”¨GPU**: å®‰è£… `torch` GPUç‰ˆæœ¬
2. **å¢å¤§batch_size**: ä¿®æ”¹ä¸º512æˆ–1024
3. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹**: å¦‚ `paraphrase-MiniLM-L3-v2` (ä»…éœ€è¦æ—¶)

#### Q6: å¦‚ä½•é‡æ–°è¿è¡ŒæŸä¸ªPhase?

**å®‰å…¨åšæ³•**:
1. **å¤‡ä»½æ•°æ®åº“**:
   ```bash
   mysqldump keyword_clustering > backup.sql
   ```

2. **æ¸…ç©ºç›¸å…³è¡¨**:
   ```sql
   -- é‡æ–°è¿è¡ŒPhase 2
   UPDATE phrases SET cluster_id_A = NULL, processed_status = 'unseen';
   DELETE FROM cluster_meta WHERE cluster_level = 'A';

   -- é‡æ–°è¿è¡ŒPhase 4
   DELETE FROM cluster_meta WHERE cluster_level = 'B';
   DELETE FROM demands;
   UPDATE phrases SET cluster_id_B = NULL;
   ```

3. **é‡æ–°è¿è¡Œè„šæœ¬**

#### Q7: æ¡†æ¶æ¨¡å¼æŠ¥é”™"æœªæ‰¾åˆ°Tokenæ¡†æ¶"

**åŸå› **: æœªè¿è¡ŒPhase 5

**è§£å†³**:
```bash
python scripts/run_phase5_tokens.py
```

ç„¶åå†è¿è¡ŒPhase 4:
```bash
python scripts/run_phase4_demands.py --use-framework
```

### æœ€ä½³å®è·µ

#### 1. æ•°æ®å‡†å¤‡

- âœ… ç¡®ä¿CSVç¼–ç ä¸ºUTF-8
- âœ… å»é™¤é‡å¤çŸ­è¯­
- âœ… è¿‡æ»¤ç©ºå€¼å’Œå¼‚å¸¸æ•°æ®
- âœ… æ··åˆå¤šç§æ•°æ®æº (SEMRUSH + Dropdown)

#### 2. èšç±»è°ƒä¼˜

**Phase 2 (å¤§ç»„)**:
- é¦–æ¬¡è¿è¡Œä½¿ç”¨é»˜è®¤å‚æ•°
- ç›®æ ‡èšç±»æ•°: 60-100
- å™ªéŸ³ç‡: <5%

**Phase 4 (å°ç»„)**:
- å°ç»„æ•°: 5-15 / å¤§ç»„
- é¿å…è¿‡åº¦åˆ†å‰² (min_cluster_size >= 5)

#### 3. LLMä½¿ç”¨

- âœ… ä½¿ç”¨GPT-4o-mini (æ€§ä»·æ¯”é«˜)
- âœ… å¯ç”¨æ¡†æ¶æ¨¡å¼ (æå‡30-50%è´¨é‡)
- âœ… è®¾ç½®åˆç†temperature (0.3-0.5)
- âŒ é¿å…ä½¿ç”¨GPT-3.5 (è´¨é‡ä¸è¶³)

#### 4. ç­›é€‰ç­–ç•¥

**æ¨èå·¥ä½œæµ**:
1. æŒ‰è´¨é‡åˆ†é™åºæ’åº
2. ç­›é€‰ Excellent (80-100åˆ†)
3. å…³æ³¨æ„å›¾å‡è¡¡ç°‡
4. æœ€ç»ˆé€‰ä¸­15-20ä¸ªç°‡

#### 5. éœ€æ±‚å®¡æ ¸

**é‡ç‚¹æ£€æŸ¥**:
- title: æ˜¯å¦ç®€æ´æœ‰åŠ›?
- description: æ˜¯å¦å‡†ç¡®æè¿°ç—›ç‚¹?
- user_scenario: æ˜¯å¦ç¬¦åˆå®é™…åœºæ™¯?
- demand_type: tool/content/service/education æ˜¯å¦å‡†ç¡®?
- business_value: high/medium/low æ˜¯å¦åˆç†?

**ä¿®æ”¹æ–¹å¼**:
1. æ‰“å¼€ `demands_draft.csv`
2. ç›´æ¥ä¿®æ”¹å­—æ®µ
3. ä¿å­˜å¹¶å¯¼å…¥ (å¾…å®ç°å¯¼å…¥è„šæœ¬)

#### 6. æ€§èƒ½ä¼˜åŒ–

- âœ… ä½¿ç”¨Embeddingç¼“å­˜
- âœ… æ‰¹é‡å¤„ç†LLMè¯·æ±‚
- âœ… åˆç†è®¾ç½®batch_size
- âœ… ä½¿ç”¨æ–­ç‚¹ç»­ä¼ 

#### 7. æˆæœ¬æ§åˆ¶

| æ“ä½œ | é¢„è®¡æˆæœ¬ | ä¼˜åŒ–å»ºè®® |
|------|---------|----------|
| Phase 3ä¸»é¢˜ç”Ÿæˆ | $0.15-0.30 | ä½¿ç”¨ç¼“å­˜,é¿å…é‡å¤ |
| Phase 4éœ€æ±‚ç”Ÿæˆ | $0.40-0.65 | å…ˆæµ‹è¯•2ä¸ªå¤§ç»„ |
| Phase 5 Tokenåˆ†ç±» | $0.25-0.50 | å¢å¤§batch_sizeåˆ°100 |
| **æ€»è®¡** | **$0.80-1.45** | - |

**çœé’±æŠ€å·§**:
- ä½¿ç”¨ `--test-limit` å‚æ•°æµ‹è¯•
- ä½¿ç”¨DeepSeek (æ›´ä¾¿å®œ)
- è·³è¿‡ä¸å¿…è¦çš„é‡å¤è¿è¡Œ

---

## æ•…éšœæ’æŸ¥æŒ‡å—

### é”™è¯¯ä»£ç è¡¨

| é”™è¯¯ä»£ç  | å«ä¹‰ | è§£å†³æ–¹æ¡ˆ |
|---------|------|----------|
| DB-001 | æ•°æ®åº“è¿æ¥å¤±è´¥ | æ£€æŸ¥MySQLæœåŠ¡/é…ç½® |
| DB-002 | è¡¨ä¸å­˜åœ¨ | è¿è¡Œ `python storage/init_db.py` |
| EMB-001 | Embeddingæ¨¡å‹ä¸‹è½½å¤±è´¥ | æ£€æŸ¥ç½‘ç»œ,æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ |
| EMB-002 | ç¼“å­˜æ–‡ä»¶æŸå | åˆ é™¤NPZæ–‡ä»¶é‡æ–°è®¡ç®— |
| LLM-001 | APIå¯†é’¥æ— æ•ˆ | æ›´æ–° `.env` æ–‡ä»¶ |
| LLM-002 | APIè°ƒç”¨è¶…æ—¶ | å¢åŠ é‡è¯•æ¬¡æ•°/æ£€æŸ¥ç½‘ç»œ |
| LLM-003 | JSONè§£æå¤±è´¥ | é™ä½temperature/ä¼˜åŒ–Prompt |
| CLS-001 | èšç±»å¤±è´¥ | æ£€æŸ¥embeddingsç»´åº¦ |
| CLS-002 | å™ªéŸ³ç‡è¿‡é«˜ (>20%) | è°ƒæ•´å‚æ•°æˆ–æ£€æŸ¥æ•°æ®è´¨é‡ |

### å¸¸è§é”™è¯¯

#### é”™è¯¯1: MySQL Data Truncation

**é”™è¯¯ä¿¡æ¯**:
```
(pymysql.err.DataError) (1265, "Data truncated for column 'demand_type' at row 1")
```

**åŸå› **: LLMè¿”å›çš„priorityè¢«é”™è¯¯æ˜ å°„åˆ°demand_typeå­—æ®µ

**è§£å†³**: å·²åœ¨è„šæœ¬ä¸­ä¿®å¤,ç¡®ä¿ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ä»£ç 

#### é”™è¯¯2: Session DetachmentError

**é”™è¯¯ä¿¡æ¯**:
```
Instance <Demand at 0x...> is not bound to a Session
```

**åŸå› **: åœ¨Sessionå…³é—­åè®¿é—®å¯¹è±¡å±æ€§

**è§£å†³**: å·²æ·»åŠ  `session.expunge()` ä¿®å¤

#### é”™è¯¯3: Streamlit Encoding Error (Windows)

**é”™è¯¯ä¿¡æ¯**:
```
UnicodeDecodeError: 'gbk' codec can't decode byte...
```

**è§£å†³**:
```python
# è„šæœ¬å¼€å¤´æ·»åŠ 
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
```

#### é”™è¯¯4: HDBSCANå®‰è£…å¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```
ERROR: Failed building wheel for hdbscan
```

**è§£å†³** (Windows):
```bash
# å®‰è£…Microsoft C++ Build Tools
# ä¸‹è½½: https://visualstudio.microsoft.com/visual-cpp-build-tools/

# æˆ–ä½¿ç”¨condaå®‰è£…
conda install -c conda-forge hdbscan
```

### æ—¥å¿—åˆ†æ

#### æŸ¥çœ‹è¿è¡Œæ—¥å¿—

**å‘½ä»¤è¡Œæ¨¡å¼**:
```bash
# è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶
python scripts/run_phase2_clustering.py > phase2.log 2>&1
```

**Web UIæ¨¡å¼**:
- æ—¥å¿—ç›´æ¥æ˜¾ç¤ºåœ¨é¡µé¢çš„"æ‰§è¡Œæ—¥å¿—"åŒºåŸŸ

#### å…³é”®æ—¥å¿—æ ‡è¯†

- `âœ“`: æˆåŠŸ
- `âš ï¸`: è­¦å‘Š (å¯ç»§ç»­)
- `âŒ`: é”™è¯¯ (éœ€å¤„ç†)
- `ğŸ’¡`: æç¤ºä¿¡æ¯

### æ€§èƒ½ç›‘æ§

#### ç›‘æ§æŒ‡æ ‡

1. **Embeddingè®¡ç®—é€Ÿåº¦**:
   - æœŸæœ›: >2000çŸ­è¯­/åˆ†é’Ÿ (CPU)
   - æœŸæœ›: >10000çŸ­è¯­/åˆ†é’Ÿ (GPU)

2. **èšç±»é€Ÿåº¦**:
   - æœŸæœ›: <5ç§’ (1000çŸ­è¯­)
   - æœŸæœ›: <60ç§’ (15KçŸ­è¯­)

3. **LLMå“åº”æ—¶é—´**:
   - æœŸæœ›: <5ç§’/è¯·æ±‚ (GPT-4o-mini)

4. **æ•°æ®åº“æŸ¥è¯¢**:
   - æœŸæœ›: <100ms (å•è¡¨æŸ¥è¯¢)
   - æœŸæœ›: <500ms (å¤šè¡¨JOIN)

#### æ€§èƒ½ç“¶é¢ˆæ’æŸ¥

**ç—‡çŠ¶**: Phase 2è€—æ—¶ >30åˆ†é’Ÿ

**æ’æŸ¥**:
1. æ£€æŸ¥æ•°æ®é‡: `SELECT COUNT(*) FROM phrases;`
2. æ£€æŸ¥ç¼“å­˜: `ls -lh data/cache/embeddings_*.npz`
3. ç›‘æ§CPU/å†…å­˜: `htop` æˆ–ä»»åŠ¡ç®¡ç†å™¨

**ç—‡çŠ¶**: Phase 4 LLMç”Ÿæˆè¶…æ—¶

**æ’æŸ¥**:
1. æµ‹è¯•APIè¿æ¥: `curl https://api.openai.com/v1/models`
2. å‡å°batch_size
3. åˆ‡æ¢åˆ°æ›´å¿«çš„æ¨¡å‹ (å¦‚gpt-4o-mini)

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜ç³»ç»Ÿçš„å®Œæ•´ä½¿ç”¨æµç¨‹ã€æŠ€æœ¯æ¶æ„å’Œæ•…éšœæ’æŸ¥æŒ‡å—ã€‚

### å…³é”®è¦ç‚¹

1. **å®Œæ•´æµç¨‹**: Phase 1 (æ•°æ®å¯¼å…¥) â†’ Phase 2 (å¤§ç»„èšç±») â†’ Phase 3 (ç­›é€‰è¯„åˆ†) â†’ Phase 4 (å°ç»„èšç±»+éœ€æ±‚ç”Ÿæˆ) â†’ Phase 5 (Tokenæå–)

2. **æ¨èé…ç½®**:
   - Embedding: all-MiniLM-L6-v2 (384ç»´)
   - LLM: GPT-4o-mini (æ€§ä»·æ¯”æœ€ä¼˜)
   - èšç±»: HDBSCAN (min_size=30 for Level A, min_size=5 for Level B)
   - æ¡†æ¶æ¨¡å¼: å¯ç”¨ (éœ€æ±‚è´¨é‡æå‡30-50%)

3. **æœ€ä½³å®è·µ**:
   - å…ˆè¿è¡ŒPhase 5æå–Token,å†è¿è¡ŒPhase 4æ¡†æ¶æ¨¡å¼
   - ä½¿ç”¨Web UIè¿›è¡Œäº¤äº’å¼ç­›é€‰ (èŠ‚çœ50-80%æ—¶é—´)
   - åˆ©ç”¨æ–­ç‚¹ç»­ä¼ ,å¤±è´¥åå¯å®‰å…¨é‡è¯•
   - å®šæœŸå¤‡ä»½æ•°æ®åº“

4. **æˆæœ¬é¢„ä¼°**:
   - 15KçŸ­è¯­ â†’ ~$0.80-1.45 (GPT-4o-mini)
   - æ€»è€—æ—¶ ~1-2å°æ—¶ (å«äººå·¥ç­›é€‰)

### ä¸‹ä¸€æ­¥

1. **äººå·¥å®¡æ ¸**: æ‰“å¼€ `demands_draft.csv` å®¡æ ¸éœ€æ±‚å¡ç‰‡
2. **éœ€æ±‚éªŒè¯**: ä¸äº§å“å›¢é˜Ÿè®¨è®ºéœ€æ±‚ä¼˜å…ˆçº§
3. **åŸå‹å¼€å‘**: åŸºäºéœ€æ±‚å¡ç‰‡å¼€å‘MVP
4. **è¿­ä»£ä¼˜åŒ–**: æ”¶é›†ç”¨æˆ·åé¦ˆ,è¿›å…¥ä¸‹ä¸€è½®è¿­ä»£

---

**æ–‡æ¡£ç»´æŠ¤è€…**: Claude Code
**æœ€åæ›´æ–°**: 2025-12-24
**é¡¹ç›®ç‰ˆæœ¬**: MVP v2.0
**è”ç³»æ–¹å¼**: è§é¡¹ç›®README.md
