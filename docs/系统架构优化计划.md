# 词根聚类需求挖掘系统 - 架构优化计划

**创建时间**: 2026-01-05
**状态**: 待实施
**目标**: 消除重复操作，统一数据流，优化系统架构

---

## 📋 一、核心问题分析

### 1.1 重复的分词操作（Triple Segmentation）

**问题描述**：
同样的125K+短语数据被分词了3次，造成资源浪费和数据不一致风险。

| 位置 | 文件 | 功能 | 分词方法 |
|------|------|------|---------|
| **Phase 0 Tab 1** | `ui/pages/phase0_expansion.py` | 关键词扩展 | `segment_keywords_with_seed_tracking()` |
| **Phase 2E Tab 0** | `ui/pages/phase2e_junyan.py` | 君言方法-种子词获取 | `segment_keywords()` |
| **Phase 4** | `ui/pages/phase4_tokens.py` | Token提取 | 内部分词逻辑 |

**影响**：
- ❌ 浪费计算资源（每次分词耗时数分钟）
- ❌ 数据可能不一致（不同时间点分词，停用词可能不同）
- ❌ 用户体验差（需要多次等待）

---

### 1.2 分词定义不清晰

**核心问题**：
"分词"应该包括什么？

**正确的定义**（与用户达成共识）：
```
分词结果 = N-gram提取（n=1到6）+ 频次过滤（≥2次）
```

包括：
- **1-gram（单词）**: running, shoes, best
- **2-gram（双词短语）**: running shoes, best running
- **3-gram（三词短语）**: best running shoes
- **4/5/6-gram（更长短语）**: how to make money online

**规则**：
- ✅ 只要在125K长尾词里重复出现≥2次，都应该保存
- ✅ 不管是单词还是短语，统一存储到`word_segments`表
- ✅ 通过`word_count`字段区分（1=单词，2/3/4/5/6=短语）

**当前状态**：
- ✅ **底层代码已支持**（`keyword_segmentation.py` lines 53-153）
- ✅ **数据库表已支持**（`word_segments.word_count`字段）
- ⚠️ **UI可能未开启N-gram提取**（需要确认`extract_ngrams`默认值）

---

### 1.3 Phase功能重叠

**Phase 2D vs Phase 2E**：

| 维度 | Phase 2D（模板发现） | Phase 2E（君言方法） |
|------|-------------------|------------------|
| **模板来源** | 完全自动（N-gram频次） | 人工挑选种子词 |
| **质量控制** | 交叉验证（变量≥2模板） | 首字/末字分析 + 交叉验证 |
| **迭代性** | 一次性 | 支持循环迭代 |
| **适用场景** | 全自动发现 | 半自动，需领域知识 |

**结论**：
- 君言方法**功能完全覆盖**Phase 2D
- 君言方法**质量更高**（人工挑选种子词）
- 君言方法**更适合用户工作流**（支持循环迭代）

**决定**：
- ❌ **删除Phase 2D**（UI页面 + 导航）
- ✅ **保留底层代码**（`core/template_discovery.py`作为参考）

---

## 🎯 二、优化方案

### 2.1 统一分词管理（核心方案）

#### **目标**：
- ✅ **分词只执行一次**（Phase 0）
- ✅ **所有后续Phase从`word_segments`表读取**
- ✅ **支持增量分词**（新数据导入后自动追加）

#### **数据流设计**：

```
Phase 1: 数据导入
  ↓ 导入新CSV到phrases表
  ↓
Phase 0: 关键词扩展与分词 ← 【唯一的分词入口】
  ├─ Tab 1: 分词与N-gram提取
  │   ├─ 检测未分词的phrases
  │   ├─ 执行分词（1-6-gram，freq≥2）
  │   └─ 保存到word_segments表
  │       ├─ 单词（word_count=1）
  │       └─ 短语（word_count=2/3/4/5/6）
  ├─ Tab 2: 高频词筛选
  └─ Tab 3: Google扩展 → 循环回Phase 1
  ↓
Phase 2E: 君言方法（大组聚类）
  ├─ Tab 0: 种子词选择 ← 从word_segments读取 ✅ 无需重复分词
  ├─ Tab 1: 提取模板
  ├─ Tab 2: 提取变量
  └─ Tab 3: 质量分析
  ↓
Phase 3: 聚类筛选
  ↓
Phase 4: Token提取 ← 从word_segments读取 ✅ 无需重复分词
  └─ LLM分类（intent/action/object/other）
  ↓
Phase 5: 需求生成
```

---

### 2.2 Phase结构调整

#### **删除**：
- ❌ **Phase 2D: 模板发现**
  - 删除文件：`ui/pages/phase2d_templates.py`
  - 修改文件：`web_ui.py`（删除导航和路由）

#### **重命名**：
| 当前名称 | 新名称 | 说明 |
|---------|--------|------|
| Phase 2: 大组聚类 | **Phase 2A: 大组聚类-HDBSCAN** | 明确聚类方法 |
| Phase 2E: 君言方法 | **Phase 2E: 大组聚类-君言方法** | 强调是聚类方法 |

#### **调整说明**：
- Phase 2A和2E是**并列的可选方法**
- 用户根据数据特征选择：
  - **有明显模板结构**（如电商搜索） → 使用君言方法
  - **无明显模板，语义相关** → 使用HDBSCAN

---

### 2.3 核心文件修改清单

#### **1. Phase 0 优化**
**文件**: `ui/pages/phase0_expansion.py`

**修改内容**：
```python
# 确保默认开启N-gram提取
extract_ngrams = st.checkbox(
    "提取短语（N-grams）",
    value=True,  # ← 改为True（当前可能是False）
    help="提取2-6词的高频短语"
)
```

**新增功能**：
- 显示分词统计：
  - 单词数量（word_count=1）
  - 短语数量（word_count>1）
  - 按词长度分布（2-gram、3-gram等）

---

#### **2. Phase 2E Tab 0 改造**
**文件**: `ui/pages/phase2e_junyan.py`

**当前实现**（lines 121-155）：
```python
# ❌ 重复分词
if segment_btn:
    for phrase in phrases:
        words = segment_keywords(phrase)
        all_words.extend(words)
    word_freq = Counter(all_words)
```

**改造为**：
```python
# ✅ 从word_segments读取
if load_btn:
    with st.spinner("正在从Phase 0加载分词结果..."):
        with WordSegmentRepository() as ws_repo:
            # 加载单词和短语
            word_counter, ngram_counter, _, _, _, _ = ws_repo.load_segmentation_results(
                min_word_frequency=min_word_freq,
                min_ngram_frequency=min_word_freq
            )

        # 合并单词和短语作为候选种子词
        all_candidates = Counter()
        all_candidates.update(word_counter)
        all_candidates.update(ngram_counter)

        st.session_state.candidate_seeds = all_candidates.most_common(max_display)

    st.success(f"✅ 已加载 {len(all_candidates)} 个候选种子词（包括单词和短语）")
```

**新增检查**：
```python
# 分词结果新鲜度检查
if not check_segmentation_freshness():
    st.error("请先前往 Phase 0 Tab 1 执行分词")
    return
```

---

#### **3. Phase 4 Token提取改造**
**文件**: `core/token_extraction.py`

**新增函数**：
```python
def extract_tokens_from_word_segments(
    min_frequency: int = 3
) -> List[Tuple[str, int]]:
    """
    从word_segments表提取Tokens（单词+短语）

    Args:
        min_frequency: 最小频次

    Returns:
        [(token, frequency), ...]
    """
    with WordSegmentRepository() as ws_repo:
        word_counter, ngram_counter, _, _, _, _ = ws_repo.load_segmentation_results(
            min_word_frequency=min_frequency,
            min_ngram_frequency=min_frequency
        )

    # 合并单词和短语
    all_tokens = Counter()
    all_tokens.update(word_counter)
    all_tokens.update(ngram_counter)

    # 返回排序列表
    return all_tokens.most_common()
```

**文件**: `ui/pages/phase4_tokens.py`

**修改内容**：
```python
# ❌ 删除"采样功能"（不再需要）
# ❌ 删除"短语提取"选项（Phase 0已提取）

# ✅ 简化参数
min_frequency = st.slider(
    "最小词频",
    min_value=1,
    max_value=20,
    value=3
)

if extract_btn:
    with st.spinner("正在从Phase 0加载分词结果..."):
        # 直接从word_segments读取
        tokens = extract_tokens_from_word_segments(min_frequency)

    st.success(f"✅ 已加载 {len(tokens)} 个Tokens")
```

---

#### **4. 删除Phase 2D**
**文件**: `web_ui.py`

**删除内容**：
```python
# 删除导航（lines 146）
"🎯 Phase 2D: 模板发现",  # ❌ 删除

# 删除路由（lines 339-341）
elif page == "🎯 Phase 2D: 模板发现":  # ❌ 删除
    from ui.pages import phase2d_templates
    phase2d_templates.render_page()
```

**修改导航**：
```python
page = st.radio(
    "选择页面",
    [
        "🏠 首页概览",
        "📥 Phase 1: 数据导入",
        "📝 Phase 0: 关键词扩展",
        "📊 Phase 0: 基线测量",
        "🔄 Phase 2A: 大组聚类-HDBSCAN",  # 🔧 改名
        "🎯 Phase 2E: 大组聚类-君言方法",  # 🔧 改名
        "✅ Phase 3: 聚类筛选",
        "🏷️ Phase 4: Token提取",
        "📊 Phase 5: 需求生成",
        ...
    ]
)
```

---

#### **5. 新增工具函数**
**文件**: `utils/segmentation_check.py`（新建）

```python
"""分词结果新鲜度检查工具"""
import streamlit as st
from storage.repository import PhraseRepository
from storage.word_segment_repository import WordSegmentRepository

def check_segmentation_freshness() -> bool:
    """
    检查分词结果是否包含所有数据

    Returns:
        True: 分词结果可用
        False: 需要重新分词
    """
    with PhraseRepository() as phrase_repo:
        total_phrases = phrase_repo.session.query(Phrase).count()

    with WordSegmentRepository() as ws_repo:
        stats = ws_repo.get_statistics()
        total_words = stats.get('total_words', 0)

    if total_words == 0:
        st.error("❌ 未找到分词结果！请先前往 Phase 0 Tab 1 执行分词")
        return False

    st.info(f"📊 当前分词结果：{total_words} 个词/短语（来自 {total_phrases} 条原始数据）")
    return True
```

---

## 📅 三、实施计划

### 阶段1: 准备工作（预计1小时）
- [ ] 确认Phase 0当前`extract_ngrams`默认值
- [ ] 备份当前数据库
- [ ] 创建新分支：`refactor/eliminate-duplicate-segmentation`

### 阶段2: Phase 0优化（预计2小时）
- [ ] 修改`phase0_expansion.py`，确保默认开启N-gram
- [ ] 增加分词统计显示（单词/短语分布）
- [ ] 测试增量分词功能

### 阶段3: Phase 2E改造（预计2小时）
- [ ] 改造Tab 0，从`word_segments`读取
- [ ] 增加新鲜度检查
- [ ] 测试种子词选择（包括单词和短语）

### 阶段4: Phase 4改造（预计2小时）
- [ ] 创建`extract_tokens_from_word_segments()`函数
- [ ] 修改UI，删除采样功能
- [ ] 测试Token提取和LLM分类

### 阶段5: 删除Phase 2D（预计1小时）
- [ ] 删除`ui/pages/phase2d_templates.py`
- [ ] 修改`web_ui.py`（删除导航和路由）
- [ ] 测试导航和页面跳转

### 阶段6: 测试与验证（预计3小时）
- [ ] 完整流程测试：Phase 1 → Phase 0 → Phase 2E → Phase 4
- [ ] 验证分词只执行一次
- [ ] 验证增量分词功能
- [ ] 性能测试（对比优化前后耗时）

### 阶段7: 文档更新（预计1小时）
- [ ] 更新README.md
- [ ] 更新使用说明页面
- [ ] 创建优化报告

**总预计时间**: 12小时

---

## ✅ 四、验收标准

### 4.1 功能验收
- [ ] Phase 0分词后，`word_segments`表包含单词和短语
- [ ] Phase 2E Tab 0能正确加载单词和短语作为种子词
- [ ] Phase 4能正确加载Tokens（无需重复分词）
- [ ] Phase 2D已删除，导航正常

### 4.2 性能验收
- [ ] 首次分词耗时：< 5分钟（125K数据）
- [ ] Phase 2E Tab 0加载耗时：< 3秒
- [ ] Phase 4加载耗时：< 3秒
- [ ] 总体耗时减少：> 50%

### 4.3 数据一致性
- [ ] 同一个词在所有Phase显示的频次一致
- [ ] 增量分词后，频次正确累加
- [ ] 停用词过滤一致

---

## 📊 五、风险评估

| 风险 | 影响 | 概率 | 缓解措施 |
|------|------|------|---------|
| 数据库迁移失败 | 高 | 低 | 提前备份，使用事务 |
| 性能下降 | 中 | 低 | 添加索引，优化查询 |
| 用户习惯改变 | 低 | 高 | 提供详细说明，逐步迁移 |
| 短语提取质量下降 | 中 | 中 | 调整min_ngram_frequency参数 |

---

## 📝 六、后续优化方向

### 6.1 Phase 0增强
- 支持自定义N-gram长度（当前固定2-6）
- 提供词性标注可视化
- 支持导出分词结果为CSV

### 6.2 君言方法增强
- 支持批量种子词导入
- 自动推荐高质量种子词
- 模板效果评分系统

### 6.3 Token分类增强
- 支持自定义分类体系
- 提供分类准确率评估
- 支持人工标注与修正

---

## 🔗 七、相关文档

- [Phase 1用户指南](./Phase1用户指南.md)
- [君言方法实现文档](./君言方法实现文档.md)
- [模板发现优化计划](./模板化产品提取优化计划.md)
- [前端测试报告](./前端测试报告.md)

---

**最后更新**: 2026-01-05
**负责人**: Claude Code
**审核人**: 待定
