# 项目需求文档

> **文档说明**: 本文档基于代码分析自动生成，描述了词根聚类需求挖掘系统的功能需求和非功能需求。

## 1. 项目概述

**项目名称**: 词根聚类需求挖掘系统 (Keyword Clustering & Demand Mining System)

**项目类型**: 数据分析与需求挖掘平台

**目标用户**:
- 产品经理：发现产品机会方向
- SEO专家：分析关键词聚类和搜索意图
- 市场研究人员：挖掘用户需求和市场趋势

**核心价值**: 从大量英文关键词中通过语义聚类自动发现产品需求方向，将"单词 → 短语 → 语义簇 → 需求方向"的分析流程自动化。

## 2. 功能需求

### P0 - 核心功能（必须实现）

#### 2.1 数据导入与整合 (Phase 1)

**功能描述**: 从多个数据源导入关键词数据并进行清洗整合

**数据源支持**:
- SEMRUSH导出的CSV文件
- 下拉词CSV文件
- 相关搜索Excel文件

**核心能力**:
- 批量导入关键词短语（支持5-10万条数据）
- 自动去重和数据清洗
- 记录数据来源和首次出现轮次
- 支持增量导入（多轮数据）

**输入**: CSV/Excel文件
**输出**: phrases表填充完毕

---

#### 2.2 语义聚类 (Phase 2)

**功能描述**: 使用机器学习算法对关键词进行语义聚类

**聚类层级**:
- **大组聚类 (Level A)**: 将所有短语聚类为60-100个大组
- **小组聚类 (Level B)**: 对选中的大组进行细分，每个大组产生5-15个小组

**技术实现**:
- 使用Sentence Transformers生成语义向量（all-MiniLM-L6-v2）
- 支持多种聚类算法：
  - HDBSCAN（密度聚类）
  - Louvain（图聚类）
  - K-Means（质心聚类）
- Embedding缓存机制（避免重复计算）
- 增量更新支持（新数据自动分配到现有聚类）

**输出**:
- phrases表的cluster_id_A和cluster_id_B字段更新
- cluster_meta表填充聚类元数据

---

#### 2.3 聚类筛选与标注 (Phase 3)

**功能描述**: 对聚类结果进行人工筛选和AI辅助标注

**核心能力**:
- 生成聚类报告（HTML + CSV格式）
- AI自动生成聚类主题标签（使用LLM）
- 人工打分筛选（1-5分）
- 意图分析（informational/transactional/navigational）
- 导入人工筛选结果

**工作流程**:
1. 系统生成报告，展示每个聚类的示例短语
2. AI生成主题标签和意图分类
3. 人工在CSV中打分（4-5分=选中，1-3分=不选中）
4. 系统导入筛选结果，更新cluster_meta表

---

#### 2.4 需求卡片生成 (Phase 4)

**功能描述**: 基于选中的聚类生成结构化需求卡片

**需求卡片结构**:
- 需求标题 (title)
- 需求描述 (description)
- 用户场景 (user_scenario)
- 需求类型 (demand_type): tool/content/service/education/other
- 商业价值 (business_value): high/medium/low
- 状态 (status): idea/validated/in_progress/archived

**生成流程**:
1. 对选中大组执行小组聚类
2. 为每个小组调用LLM生成需求卡片初稿
3. 导出CSV供人工审核
4. 导入审核后的需求卡片

**输出**: 20-50个需求卡片（至少10个validated状态）

---

#### 2.5 Token提取与分类 (Phase 5)

**功能描述**: 从短语中提取高频关键词并进行语义分类

**Token类型**:
- **intent**: 意图词（如：how, best, free）
- **action**: 动作词（如：download, create, manage）
- **object**: 对象词（如：software, tool, app）
- **attribute**: 属性词（如：free, online, easy）
- **condition**: 条件词（如：for, with, without）
- **other**: 其他

**提取方法**:
- 穷尽式n-gram提取（1-gram到4-gram）
- 停用词过滤
- 频率统计
- LLM批量分类

**输出**: tokens表填充，生成CSV报告供人工审核

---

#### 2.6 Reddit板块分析与标注系统 (Phase 6)

**功能描述**: 导入Reddit板块数据，使用AI自动分析板块名称和描述，生成中文标签并进行重要性评估

**使用场景**:
- 了解每个Reddit板块的内容和特点
- 发现板块中的用户需求
- 筛选适合推广/研究的目标板块

**输入信息**:
- **文件格式**: CSV或Excel文件（无列名）
- **数据列顺序**:
  - 第1列：板块名称 (subreddit name)，如 r/Python
  - 第2列：板块描述 (description)
  - 第3列：关注人数 (subscribers)
- **数据量级**: 几万条板块数据

**功能要求**:

1. **数据导入**
   - 支持CSV和Excel格式
   - 自动解析无列名的文件（按列顺序识别）
   - 批量导入几万条数据
   - 自动去重（按板块名称）
   - 跳过描述为空的板块

2. **AI分析与标注**
   - **中文标签生成**: 每个板块生成3个中文标签
     - 基于板块名称和描述分析
     - 标签示例：["编程技术", "Python语言", "开发者社区"]
   - **重要性评分**: 1-5分评分系统
     - 根据可配置的提示词评估重要性
     - 评分依据由用户在提示词中定义
   - **提示词配置**: 在Web UI中配置AI提示词
     - 支持配置LLM模型（OpenAI/Anthropic/DeepSeek）
     - 支持自定义提示词模板
     - 可随时修改评分标准

3. **标签管理功能**
   - **标签分组**: 支持按标签对板块进行分组查看
   - **标签筛选**: 支持按标签筛选板块列表
   - **标签编辑**: 支持人工修改AI生成的标签

4. **数据展示与管理**
   - 在Web UI中以表格形式展示所有板块
   - 显示字段：板块名称、描述、关注人数、标签、重要性评分
   - 支持筛选：按标签、重要性评分、关注人数
   - 支持排序：按关注人数、重要性评分、创建时间
   - 支持人工编辑：标签和重要性评分可在UI中修改
   - 支持导出：导出分析结果为CSV/Excel

**数据处理**:
- **数据表**: reddit_subreddits
- **存储字段**:
  - subreddit_id (主键)
  - name (板块名称，唯一索引)
  - description (板块描述)
  - subscribers (关注人数)
  - tags (中文标签，JSON数组，3个标签)
  - importance_score (重要性评分，1-5)
  - ai_analysis_status (AI分析状态：pending/completed/failed)
  - reviewed (是否已人工审核)
  - created_at (创建时间)
  - updated_at (更新时间)

**验收标准**:
- [ ] 能够导入无列名的CSV/Excel文件（几万条数据）
- [ ] 自动调用AI分析每个板块（跳过描述为空的）
- [ ] AI生成3个准确的中文标签
- [ ] AI根据可配置的提示词评估重要性（1-5分）
- [ ] 在Web UI中配置和修改AI提示词
- [ ] 在Web UI中查看板块列表（表格形式）
- [ ] 支持按标签分组和筛选
- [ ] 支持按重要性、关注人数排序
- [ ] 支持人工编辑标签和评分
- [ ] 支持导出分析结果

**边界情况**:
- 描述为空：跳过AI分析，标记为pending状态
- 板块名称重复：保留关注人数最多的记录
- AI调用失败：标记为failed状态，支持重新分析
- 人工审核：AI生成后支持在UI中编辑修改

**与现有功能的关系**:
- 这是一个**完全独立的功能模块**
- 使用独立的数据表（reddit_subreddits）
- 复用现有的LLM集成模块（ai/client.py）
- 在Web UI中添加新的页面（Phase 6: Reddit分析）

**输出**:
- reddit_subreddits表填充完毕
- 每个板块包含AI生成的标签和评分
- 可导出的分析报告

---

#### 2.7 商品筛选与AI标注系统 (Phase 7)

**功能描述**: 从Etsy、Gumroad等电商平台导入商品数据，进行数据整理和清理，支持灵活的筛选、排序和字段管理，使用AI对商品进行标签标注和需求判断，提供类似飞书多维表格的数据管理能力。

**使用场景**:
- 电商选品：从多个平台导入商品数据，筛选出有潜力的商品
- 市场分析：分析商品类型、价格分布、需求方向
- 商品标注：使用AI自动给商品打标签、判断商品解决的需求
- 数据管理：灵活管理商品数据，支持自定义字段

**输入信息**:

**1. 数据源**
- **支持平台**: Etsy、Gumroad（未来可扩展其他平台）
- **文件格式**: CSV、Excel
- **列名支持**: 支持无列名文件（导入时手动指定字段映射）
- **数据量级**: 几万到几十万条商品数据

**2. 商品数据字段**

**核心字段**（导入时映射）:
- 商品名称 (product_name)
- 商品描述 (description)
- 价格 (price)
- 销量 (sales)
- 评分 (rating) - 平均评分（如4.5星）
- 评价数量 (review_count) - 有多少用户写了评价
- 商品链接 (url)
- 商品店铺 (shop_name)
- 平台来源 (platform) - Etsy/Gumroad

**AI生成字段**:
- 标签 (tags) - AI生成的3个中文标签
- 需求判断 (demand_analysis) - AI判断商品解决什么需求

**元数据字段**:
- 数据来源 (source_file)
- 导入时间 (imported_at)
- 最后更新时间 (updated_at)

**动态字段**:
- 用户可在Web UI中添加自定义字段
- 支持字段类型：文本、数字、日期、标签、链接等

**功能要求**:

**1. 数据导入**

**导入流程**:
1. 上传CSV/Excel文件
2. 系统自动识别列（显示前5行预览）
3. 用户手动指定每列对应的字段
   - 例如：第1列 → 商品名称，第2列 → 描述...
4. 选择数据清理规则
5. 执行导入

**字段映射**:
- 支持无列名文件
- 支持跳过某些列（不导入）
- 支持一列映射到多个字段（如：价格+货币单位）
- 保存字段映射配置（下次导入同类文件可复用）

**批量导入**:
- 支持批量导入（每批1000条）
- 显示导入进度条
- 导入完成后显示统计信息

**2. 数据整理与清理**

**去除无效数据**:
- 商品名称为空
- 价格为0或为空
- 链接无效或为空
- 描述为空（可选）

**统一格式**:
- 价格统一为美元（USD）
- 日期格式统一为YYYY-MM-DD
- 商品名称去除前后空格
- 链接格式验证

**过滤数据**:
- 支持设置过滤规则（可选）
  - 评分低于X星
  - 销量低于X
  - 价格范围
  - 评价数量低于X

**标准化处理**:
- 商品名称长度限制（如：最多200字符）
- 描述长度限制（如：最多5000字符）
- 去除特殊字符（可选）

**去重策略**:
- 按商品链接去重（优先）
- 按商品名称+店铺去重（备选）
- 重复数据保留最新的记录

**3. 筛选与排序**

**筛选功能**:
- 支持所有字段的筛选
- 筛选类型：
  - 文本字段：关键词搜索、精确匹配
  - 数字字段：范围筛选（大于、小于、等于、区间）
  - 标签字段：多选筛选
  - 日期字段：日期范围筛选
- 支持多条件组合筛选（AND/OR逻辑）
- 保存筛选条件（常用筛选可保存为预设）

**排序功能**:
- 支持所有字段的排序
- 支持升序/降序
- 支持多字段排序（如：先按评分降序，再按销量降序）

**4. AI标注与分析**

**AI配置**:
- 在Web UI中配置AI提示词
- 支持选择LLM模型（OpenAI/Anthropic/DeepSeek）
- 支持配置多个AI任务（标签生成、需求判断等）

**AI标注流程**:
1. 用户选择要标注的商品（可批量选择或筛选后全选）
2. 用户指定AI读取哪些字段（如：名称+描述）
3. 用户指定AI输出到哪些字段（如：tags, demand_analysis）
4. 系统批量调用AI（每批10条）
5. AI结果写入指定字段
6. 显示标注进度和结果

**标签生成**:
- 每个商品生成3个中文标签
- 标签示例：["电子产品", "设计工具", "数字下载"]
- 标签存储为JSON数组

**需求判断**:
- AI分析商品解决什么用户需求
- 输出准确的文本描述
- 示例："帮助设计师快速制作社交媒体图片，节省设计时间"

**AI任务管理**:
- 显示AI标注状态（pending/processing/completed/failed）
- 支持重新标注（覆盖或追加）
- 支持查看AI标注历史

**5. 动态字段管理（类似飞书多维表格）**

**字段管理功能**:
- 添加字段：指定字段名称、字段类型、默认值
- 删除字段：删除自定义字段（核心字段不可删除）
- 编辑字段：修改字段名称、字段类型
- 字段排序：调整字段在表格中的显示顺序

**支持的字段类型**:
- 文本 (text)
- 数字 (number)
- 日期 (date)
- 链接 (url)
- 标签 (tags) - 多选
- 单选 (select)
- 多选 (multi_select)
- 长文本 (textarea)

**字段配置**:
- 字段名称（中文/英文）
- 字段类型
- 是否必填
- 默认值
- 字段描述

**6. 数据展示与编辑**

**表格展示**:
- 以表格形式展示所有商品
- 支持分页（每页50/100/200条）
- 支持列宽调整
- 支持列显示/隐藏
- 支持固定列（如：商品名称固定在左侧）

**直接编辑**:
- 支持在表格中直接编辑单元格
- 支持批量编辑（选中多行，批量修改某个字段）
- 自动保存（编辑后自动保存到数据库）
- 编辑历史记录（可选）

**数据导出**:
- 导出当前筛选结果
- 导出格式：CSV、Excel
- 导出字段可选（选择要导出的字段）
- 导出文件命名：products_export_YYYYMMDD_HHMMSS.csv

**数据处理**:

**数据库设计**

**1. products表**（商品主表）

固定字段:
- product_id (主键)
- product_name (商品名称，索引)
- description (商品描述，TEXT)
- price (价格，DECIMAL)
- sales (销量，INT)
- rating (评分，DECIMAL)
- review_count (评价数量，INT)
- url (商品链接，唯一索引)
- shop_name (店铺名称)
- platform (平台来源：etsy/gumroad)
- source_file (数据来源文件)
- imported_at (导入时间)
- updated_at (最后更新时间)

AI生成字段:
- tags (标签，JSON数组)
- demand_analysis (需求判断，TEXT)
- ai_analysis_status (AI分析状态：pending/completed/failed)

动态字段:
- custom_fields (JSON) - 存储用户自定义字段的数据

**2. product_field_definitions 表**（字段定义表）

- field_id (主键)
- field_name (字段名称)
- field_key (字段键名，用于JSON存储)
- field_type (字段类型：text/number/date/tags等)
- is_required (是否必填)
- default_value (默认值)
- field_order (字段显示顺序)
- created_at (创建时间)

**3. product_import_logs 表**（导入日志表）

- log_id (主键)
- source_file (源文件名)
- platform (平台)
- total_rows (总行数)
- imported_rows (成功导入行数)
- skipped_rows (跳过行数)
- field_mapping (字段映射配置，JSON)
- imported_at (导入时间)

**4. ai_prompt_configs 表**（AI提示词配置表）

- 复用现有的ai_prompt_configs 表
- 添加新的 feature_name: 'product_tagging', 'product_demand_analysis'

**验收标准**:
- [ ] 能够导入Etsy、Gumroad的CSV/Excel文件（支持无列名）
- [ ] 导入时手动指定字段映射
- [ ] 自动去重和数据清理（去除无效数据、统一格式）
- [ ] 支持所有字段的筛选和排序
- [ ] 支持多条件组合筛选
- [ ] 在Web UI中配置AI提示词
- [ ] AI自动生成3个中文标签
- [ ] AI自动判断商品解决的需求
- [ ] 支持批量AI标注（每批10条）
- [ ] 支持动态添加/删除字段
- [ ] 支持多种字段类型（文本、数字、日期、标签等）
- [ ] 在Web UI中以表格形式展示商品列表
- [ ] 支持在表格中直接编辑
- [ ] 支持导出筛选结果为CSV/Excel
- [ ] 支持保存字段映射配置（下次导入可复用）
- [ ] 支持保存筛选条件预设

**边界情况**:

**数据导入**:
- 文件格式错误：提示用户文件格式不正确
- 文件过大：分批导入，显示进度
- 字段映射错误：导入前验证字段映射
- 重复数据：按去重策略处理

**数据清理**:
- 价格格式不统一：尝试自动转换，失败则标记为无效
- 日期格式不统一：尝试自动解析，失败则保留原始值
- 特殊字符：根据配置决定是否去除

**AI标注**:
- AI调用失败：标记为failed状态，支持重新标注
- AI返回格式错误：记录错误日志，跳过该条数据
- 批量标注中断：支持断点续传

**动态字段**:
- 字段名称重复：提示用户修改
- 删除字段：提示用户确认（删除后数据不可恢复）
- 字段类型变更：提示用户可能导致数据丢失

**数据编辑**:
- 编辑冲突：使用最后保存的数据（Last Write Wins）
- 必填字段为空：提示用户填写
- 数据格式错误：提示用户修正

**与现有功能的关系**:
- **独立功能模块**：与现有的词根聚类功能完全独立
- **复用LLM集成**：复用 ai/client.py 的LLM调用逻辑
- **复用数据库连接**：复用 storage/database.py 的数据库连接
- **复用Web UI框架**：在web_ui.py 中添加新页面（Phase 7: 商品筛选）
- **复用AI配置表**：复用 ai_prompt_configs 表存储AI提示词
- **未来集成**：后期可能与词根聚类功能集成（如：商品名称也进行聚类分析）

**输出**:
- products表填充完毕
- 每个商品包含AI生成的标签和需求判断
- 可导出的分析报告

---

### P1 - 重要功能

#### 2.8 Web可视化界面

**功能描述**: 基于Streamlit的可视化操作平台

**核心页面**:
- **仪表盘**: 数据库统计、系统状态
- **Phase 0**: 词根管理、词性标注、翻译
- **Phase 1**: 数据导入
- **Phase 2**: 聚类执行与查看
- **Phase 3**: 聚类筛选与意图分析
- **Phase 4**: 需求卡片管理
- **Phase 5**: Token管理
- **配置页面**: 数据库、LLM、聚类参数配置
- **文档页面**: 在线文档查看器

**交互特性**:
- 实时日志输出
- 进度条显示
- 数据表格编辑
- 参数配置保存
- 一键导出报告

---

#### 2.9 增量更新 (Phase 7)

**功能描述**: 支持导入新一轮数据并自动分配到现有聚类

**核心能力**:
- 新短语自动分配到现有大组（KNN算法）
- 过滤已处理的短语
- 低频噪音点自动归档
- 支持多轮数据管理

---

#### 2.10 模板发现与产品识别 (Phase 2D/2E)

**功能描述**: 从聚类中发现短语模板和产品名称

**模板发现**:
- 识别短语中的变量部分（如：[X] software, [X] tool）
- 提取模板和变量值
- 统计模板频率

**产品识别**:
- 识别产品名称（如：Photoshop, Excel）
- 提取产品相关短语
- 分析产品竞争格局

---

### P2 - 次要功能

#### 2.11 聚类质量评估

**功能描述**: 自动评估聚类质量

**评估指标**:
- Silhouette Score（轮廓系数）
- 聚类大小分布
- 噪音点比例
- LLM质量评分

---

#### 2.12 数据导出

**功能描述**: 支持多种格式的数据导出

**导出格式**:
- CSV（聚类报告、需求卡片、Token列表）
- HTML（可视化报告）
- Excel（多sheet报告）

---

## 3. 非功能需求

### 3.1 性能需求

- **数据规模**: 支持5-10万条短语
- **聚类速度**: 大组聚类 < 10分钟（使用缓存）
- **Embedding计算**: 支持GPU加速
- **批处理**: 支持批量操作（batch_size=256）

### 3.2 可用性需求

- **Web界面**: 提供友好的可视化操作界面
- **日志记录**: 详细的操作日志和错误提示
- **进度显示**: 长时间操作显示进度条
- **文档完善**: 提供完整的使用文档和API文档

### 3.3 可维护性需求

- **模块化设计**: 清晰的分层架构（core/storage/ai/ui）
- **配置管理**: 统一的配置文件（config/settings.py）
- **代码规范**: 遵循PEP 8规范
- **测试覆盖**: 单元测试覆盖率 > 55%

### 3.4 安全性需求

- **数据保护**: 所有数据文件不推送到GitHub
- **API密钥**: 使用.env文件管理敏感信息
- **SQL注入防护**: 使用ORM（SQLAlchemy）防止SQL注入
- **输入验证**: 对用户输入进行验证和清洗

### 3.5 扩展性需求

- **数据库支持**: 支持MySQL和SQLite
- **LLM提供商**: 支持OpenAI、Anthropic、DeepSeek
- **聚类算法**: 支持多种聚类算法切换
- **增量更新**: 支持多轮数据导入

---

## 4. 技术约束

### 4.1 技术栈

- **后端**: Python 3.8+
- **数据库**: MySQL/MariaDB（推荐）或 SQLite
- **机器学习**:
  - sentence-transformers（语义向量）
  - scikit-learn（聚类算法）
  - hdbscan（密度聚类）
  - networkx + python-louvain（图聚类）
- **LLM集成**: OpenAI API, Anthropic API, DeepSeek API
- **Web框架**: Streamlit
- **ORM**: SQLAlchemy 2.0+

### 4.2 环境要求

- Python >= 3.8
- 8GB+ RAM（用于embedding计算）
- MySQL/MariaDB 或 SQLite
- （可选）CUDA GPU（加速embedding计算）

### 4.3 依赖管理

- 使用requirements.txt管理依赖
- 使用.env管理环境变量
- 使用pyproject.toml管理项目元数据

---

## 5. 数据需求

### 5.1 输入数据

**原始数据格式**:
- SEMRUSH CSV: 包含keyword, volume等字段
- 下拉词CSV: 包含seed_word, dropdown_phrase等字段
- 相关搜索Excel: 包含seed_word, related_phrase等字段

**数据量级**: 5-10万条关键词短语

### 5.2 数据存储

**数据库表**:
- **phrases**: 短语总库（55,275条）
- **cluster_meta**: 聚类元数据（307个大组）
- **demands**: 需求卡片（20-50个）
- **tokens**: Token词库（数千个）
- **seed_words**: 种子词管理
- **word_segments**: 分词结果缓存

---

## 6. 用户故事

### 6.1 产品经理

**场景**: 我想从5万个关键词中发现10个可落地的产品需求

**流程**:
1. 导入SEMRUSH数据（Phase 1）
2. 执行大组聚类（Phase 2）
3. 查看聚类报告，筛选出10-15个目标大组（Phase 3）
4. 生成需求卡片，审核并标记validated（Phase 4）
5. 查看Token词库，了解用户意图（Phase 5）

---

### 6.2 SEO专家

**场景**: 我想分析关键词的语义聚类和搜索意图

**流程**:
1. 导入关键词数据（Phase 1）
2. 执行聚类（Phase 2）
3. 查看聚类结果和意图分析（Phase 3）
4. 导出聚类报告用于SEO策略制定

---

### 6.3 市场研究人员

**场景**: 我想了解某个领域的用户需求分布

**流程**:
1. 导入相关领域的关键词（Phase 1）
2. 执行聚类（Phase 2）
3. 查看聚类主题和需求卡片（Phase 3-4）
4. 分析Token分布，了解用户关注点（Phase 5）

---

## 7. 成功标准

### 7.1 MVP成功标准

- [x] phrases表有55,275条数据
- [x] cluster_meta表有307个大组（Level A）
- [x] 选中2个目标大组（测试）
- [x] cluster_meta表有6个小组（Level B，测试）
- [x] tokens表有79个tokens（测试）
- [ ] 生成10-20个validated需求卡片
- [ ] AI生成的需求卡片准确率 > 70%

### 7.2 可用性标准

- [ ] 从5万词产出10-20个可落地的需求想法
- [ ] 每个需求下有真实的搜索短语支撑
- [ ] 能够快速定位"哪些词属于同一需求"

---

## 8. 未来迭代方向

### 第二轮迭代（+2周）
- Phase 5完整版：tokens词库完善
- Phase 7完整版：增量小组重聚类
- 数据表字段扩展：添加tags, main_tokens (JSON)

### 第三轮迭代（+2周）
- Web UI增强：ClusterSelector, DemandEditor
- 批量操作功能：合并需求、批量标记
- 导出工具：SEO词表、Landing Page素材

### 第四轮迭代（有产品后）
- Phase 6：商业化字段（revenue, landing_url）
- 数据可视化：需求地图、词云、趋势图
- 自动化Pipeline：定期扫描新词、自动推送报告

---

**文档版本**: v1.2
**最后更新**: 2026-01-14
**生成方式**: 基于代码分析自动生成

## 版本变更记录

### v1.2 (2026-01-14)
- 新增P0核心功能 - 商品筛选与AI标注系统 (Phase 7)
- 支持从Etsy、Gumroad等平台导入商品数据
- 支持动态字段管理（类似飞书多维表格）
- 支持AI自动标注商品标签和需求判断
- 重新编号后续章节 (2.7 → 2.8, 2.8 → 2.9, 等)

### v1.1 (2026-01-09)
- 新增P0核心功能 - Reddit板块分析与标注系统 (Phase 6)
- 重新编号后续章节 (2.6 → 2.7, 2.7 → 2.8, 等)

### v1.0 (2026-01-08)
- 初始版本
- 基于代码分析自动生成
