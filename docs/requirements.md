# 项目需求文档

> **文档说明**: 本文档基于代码分析自动生成，描述了词根聚类需求挖掘系统的功能需求和非功能需求。

## 1. 项目概述

**项目名称**: 词根聚类需求挖掘系统 (Keyword Clustering & Demand Mining System)

**项目类型**: 数据分析与需求挖掘平台

**目标用户**:
- 产品经理：发现产品机会方向
- SEO专家：分析关键词聚类和搜索意图
- 市场研究人员：挖掘用户需求和市场趋势

**核心价值**: 从大量英文关键词中通过语义聚类自动发现产品需求方向，将"单词 → 短语 → 语义簇 → 需求方向"的分析流程自动化。

## 2. 功能需求

### P0 - 核心功能（必须实现）

#### 2.1 数据导入与整合 (Phase 1)

**功能描述**: 从多个数据源导入关键词数据并进行清洗整合

**数据源支持**:
- SEMRUSH导出的CSV文件
- 下拉词CSV文件
- 相关搜索Excel文件

**核心能力**:
- 批量导入关键词短语（支持5-10万条数据）
- 自动去重和数据清洗
- 记录数据来源和首次出现轮次
- 支持增量导入（多轮数据）

**输入**: CSV/Excel文件
**输出**: phrases表填充完毕

---

#### 2.2 语义聚类 (Phase 2)

**功能描述**: 使用机器学习算法对关键词进行语义聚类

**聚类层级**:
- **大组聚类 (Level A)**: 将所有短语聚类为60-100个大组
- **小组聚类 (Level B)**: 对选中的大组进行细分，每个大组产生5-15个小组

**技术实现**:
- 使用Sentence Transformers生成语义向量（all-MiniLM-L6-v2）
- 支持多种聚类算法：
  - HDBSCAN（密度聚类）
  - Louvain（图聚类）
  - K-Means（质心聚类）
- Embedding缓存机制（避免重复计算）
- 增量更新支持（新数据自动分配到现有聚类）

**输出**:
- phrases表的cluster_id_A和cluster_id_B字段更新
- cluster_meta表填充聚类元数据

---

#### 2.3 聚类筛选与标注 (Phase 3)

**功能描述**: 对聚类结果进行人工筛选和AI辅助标注

**核心能力**:
- 生成聚类报告（HTML + CSV格式）
- AI自动生成聚类主题标签（使用LLM）
- 人工打分筛选（1-5分）
- 意图分析（informational/transactional/navigational）
- 导入人工筛选结果

**工作流程**:
1. 系统生成报告，展示每个聚类的示例短语
2. AI生成主题标签和意图分类
3. 人工在CSV中打分（4-5分=选中，1-3分=不选中）
4. 系统导入筛选结果，更新cluster_meta表

---

#### 2.4 需求卡片生成 (Phase 4)

**功能描述**: 基于选中的聚类生成结构化需求卡片

**需求卡片结构**:
- 需求标题 (title)
- 需求描述 (description)
- 用户场景 (user_scenario)
- 需求类型 (demand_type): tool/content/service/education/other
- 商业价值 (business_value): high/medium/low
- 状态 (status): idea/validated/in_progress/archived

**生成流程**:
1. 对选中大组执行小组聚类
2. 为每个小组调用LLM生成需求卡片初稿
3. 导出CSV供人工审核
4. 导入审核后的需求卡片

**输出**: 20-50个需求卡片（至少10个validated状态）

---

#### 2.5 Token提取与分类 (Phase 5)

**功能描述**: 从短语中提取高频关键词并进行语义分类

**Token类型**:
- **intent**: 意图词（如：how, best, free）
- **action**: 动作词（如：download, create, manage）
- **object**: 对象词（如：software, tool, app）
- **attribute**: 属性词（如：free, online, easy）
- **condition**: 条件词（如：for, with, without）
- **other**: 其他

**提取方法**:
- 穷尽式n-gram提取（1-gram到4-gram）
- 停用词过滤
- 频率统计
- LLM批量分类

**输出**: tokens表填充，生成CSV报告供人工审核

---

#### 2.6 Reddit板块分析与标注系统 (Phase 6)

**功能描述**: 导入Reddit板块数据，使用AI自动分析板块名称和描述，生成中文标签并进行重要性评估

**使用场景**:
- 了解每个Reddit板块的内容和特点
- 发现板块中的用户需求
- 筛选适合推广/研究的目标板块

**输入信息**:
- **文件格式**: CSV或Excel文件（无列名）
- **数据列顺序**:
  - 第1列：板块名称 (subreddit name)，如 r/Python
  - 第2列：板块描述 (description)
  - 第3列：关注人数 (subscribers)
- **数据量级**: 几万条板块数据

**功能要求**:

1. **数据导入**
   - 支持CSV和Excel格式
   - 自动解析无列名的文件（按列顺序识别）
   - 批量导入几万条数据
   - 自动去重（按板块名称）
   - 跳过描述为空的板块

2. **AI分析与标注**
   - **中文标签生成**: 每个板块生成3个中文标签
     - 基于板块名称和描述分析
     - 标签示例：["编程技术", "Python语言", "开发者社区"]
   - **重要性评分**: 1-5分评分系统
     - 根据可配置的提示词评估重要性
     - 评分依据由用户在提示词中定义
   - **提示词配置**: 在Web UI中配置AI提示词
     - 支持配置LLM模型（OpenAI/Anthropic/DeepSeek）
     - 支持自定义提示词模板
     - 可随时修改评分标准

3. **标签管理功能**
   - **标签分组**: 支持按标签对板块进行分组查看
   - **标签筛选**: 支持按标签筛选板块列表
   - **标签编辑**: 支持人工修改AI生成的标签

4. **数据展示与管理**
   - 在Web UI中以表格形式展示所有板块
   - 显示字段：板块名称、描述、关注人数、标签、重要性评分
   - 支持筛选：按标签、重要性评分、关注人数
   - 支持排序：按关注人数、重要性评分、创建时间
   - 支持人工编辑：标签和重要性评分可在UI中修改
   - 支持导出：导出分析结果为CSV/Excel

**数据处理**:
- **数据表**: reddit_subreddits
- **存储字段**:
  - subreddit_id (主键)
  - name (板块名称，唯一索引)
  - description (板块描述)
  - subscribers (关注人数)
  - tags (中文标签，JSON数组，3个标签)
  - importance_score (重要性评分，1-5)
  - ai_analysis_status (AI分析状态：pending/completed/failed)
  - reviewed (是否已人工审核)
  - created_at (创建时间)
  - updated_at (更新时间)

**验收标准**:
- [ ] 能够导入无列名的CSV/Excel文件（几万条数据）
- [ ] 自动调用AI分析每个板块（跳过描述为空的）
- [ ] AI生成3个准确的中文标签
- [ ] AI根据可配置的提示词评估重要性（1-5分）
- [ ] 在Web UI中配置和修改AI提示词
- [ ] 在Web UI中查看板块列表（表格形式）
- [ ] 支持按标签分组和筛选
- [ ] 支持按重要性、关注人数排序
- [ ] 支持人工编辑标签和评分
- [ ] 支持导出分析结果

**边界情况**:
- 描述为空：跳过AI分析，标记为pending状态
- 板块名称重复：保留关注人数最多的记录
- AI调用失败：标记为failed状态，支持重新分析
- 人工审核：AI生成后支持在UI中编辑修改

**与现有功能的关系**:
- 这是一个**完全独立的功能模块**
- 使用独立的数据表（reddit_subreddits）
- 复用现有的LLM集成模块（ai/client.py）
- 在Web UI中添加新的页面（Phase 6: Reddit分析）

**输出**:
- reddit_subreddits表填充完毕
- 每个板块包含AI生成的标签和评分
- 可导出的分析报告

---

### P1 - 重要功能

#### 2.7 Web可视化界面

**功能描述**: 基于Streamlit的可视化操作平台

**核心页面**:
- **仪表盘**: 数据库统计、系统状态
- **Phase 0**: 词根管理、词性标注、翻译
- **Phase 1**: 数据导入
- **Phase 2**: 聚类执行与查看
- **Phase 3**: 聚类筛选与意图分析
- **Phase 4**: 需求卡片管理
- **Phase 5**: Token管理
- **配置页面**: 数据库、LLM、聚类参数配置
- **文档页面**: 在线文档查看器

**交互特性**:
- 实时日志输出
- 进度条显示
- 数据表格编辑
- 参数配置保存
- 一键导出报告

---

#### 2.8 增量更新 (Phase 7)

**功能描述**: 支持导入新一轮数据并自动分配到现有聚类

**核心能力**:
- 新短语自动分配到现有大组（KNN算法）
- 过滤已处理的短语
- 低频噪音点自动归档
- 支持多轮数据管理

---

#### 2.9 模板发现与产品识别 (Phase 2D/2E)

**功能描述**: 从聚类中发现短语模板和产品名称

**模板发现**:
- 识别短语中的变量部分（如：[X] software, [X] tool）
- 提取模板和变量值
- 统计模板频率

**产品识别**:
- 识别产品名称（如：Photoshop, Excel）
- 提取产品相关短语
- 分析产品竞争格局

---

### P2 - 次要功能

#### 2.10 聚类质量评估

**功能描述**: 自动评估聚类质量

**评估指标**:
- Silhouette Score（轮廓系数）
- 聚类大小分布
- 噪音点比例
- LLM质量评分

---

#### 2.11 数据导出

**功能描述**: 支持多种格式的数据导出

**导出格式**:
- CSV（聚类报告、需求卡片、Token列表）
- HTML（可视化报告）
- Excel（多sheet报告）

---

## 3. 非功能需求

### 3.1 性能需求

- **数据规模**: 支持5-10万条短语
- **聚类速度**: 大组聚类 < 10分钟（使用缓存）
- **Embedding计算**: 支持GPU加速
- **批处理**: 支持批量操作（batch_size=256）

### 3.2 可用性需求

- **Web界面**: 提供友好的可视化操作界面
- **日志记录**: 详细的操作日志和错误提示
- **进度显示**: 长时间操作显示进度条
- **文档完善**: 提供完整的使用文档和API文档

### 3.3 可维护性需求

- **模块化设计**: 清晰的分层架构（core/storage/ai/ui）
- **配置管理**: 统一的配置文件（config/settings.py）
- **代码规范**: 遵循PEP 8规范
- **测试覆盖**: 单元测试覆盖率 > 55%

### 3.4 安全性需求

- **数据保护**: 所有数据文件不推送到GitHub
- **API密钥**: 使用.env文件管理敏感信息
- **SQL注入防护**: 使用ORM（SQLAlchemy）防止SQL注入
- **输入验证**: 对用户输入进行验证和清洗

### 3.5 扩展性需求

- **数据库支持**: 支持MySQL和SQLite
- **LLM提供商**: 支持OpenAI、Anthropic、DeepSeek
- **聚类算法**: 支持多种聚类算法切换
- **增量更新**: 支持多轮数据导入

---

## 4. 技术约束

### 4.1 技术栈

- **后端**: Python 3.8+
- **数据库**: MySQL/MariaDB（推荐）或 SQLite
- **机器学习**:
  - sentence-transformers（语义向量）
  - scikit-learn（聚类算法）
  - hdbscan（密度聚类）
  - networkx + python-louvain（图聚类）
- **LLM集成**: OpenAI API, Anthropic API, DeepSeek API
- **Web框架**: Streamlit
- **ORM**: SQLAlchemy 2.0+

### 4.2 环境要求

- Python >= 3.8
- 8GB+ RAM（用于embedding计算）
- MySQL/MariaDB 或 SQLite
- （可选）CUDA GPU（加速embedding计算）

### 4.3 依赖管理

- 使用requirements.txt管理依赖
- 使用.env管理环境变量
- 使用pyproject.toml管理项目元数据

---

## 5. 数据需求

### 5.1 输入数据

**原始数据格式**:
- SEMRUSH CSV: 包含keyword, volume等字段
- 下拉词CSV: 包含seed_word, dropdown_phrase等字段
- 相关搜索Excel: 包含seed_word, related_phrase等字段

**数据量级**: 5-10万条关键词短语

### 5.2 数据存储

**数据库表**:
- **phrases**: 短语总库（55,275条）
- **cluster_meta**: 聚类元数据（307个大组）
- **demands**: 需求卡片（20-50个）
- **tokens**: Token词库（数千个）
- **seed_words**: 种子词管理
- **word_segments**: 分词结果缓存

---

## 6. 用户故事

### 6.1 产品经理

**场景**: 我想从5万个关键词中发现10个可落地的产品需求

**流程**:
1. 导入SEMRUSH数据（Phase 1）
2. 执行大组聚类（Phase 2）
3. 查看聚类报告，筛选出10-15个目标大组（Phase 3）
4. 生成需求卡片，审核并标记validated（Phase 4）
5. 查看Token词库，了解用户意图（Phase 5）

---

### 6.2 SEO专家

**场景**: 我想分析关键词的语义聚类和搜索意图

**流程**:
1. 导入关键词数据（Phase 1）
2. 执行聚类（Phase 2）
3. 查看聚类结果和意图分析（Phase 3）
4. 导出聚类报告用于SEO策略制定

---

### 6.3 市场研究人员

**场景**: 我想了解某个领域的用户需求分布

**流程**:
1. 导入相关领域的关键词（Phase 1）
2. 执行聚类（Phase 2）
3. 查看聚类主题和需求卡片（Phase 3-4）
4. 分析Token分布，了解用户关注点（Phase 5）

---

## 7. 成功标准

### 7.1 MVP成功标准

- [x] phrases表有55,275条数据
- [x] cluster_meta表有307个大组（Level A）
- [x] 选中2个目标大组（测试）
- [x] cluster_meta表有6个小组（Level B，测试）
- [x] tokens表有79个tokens（测试）
- [ ] 生成10-20个validated需求卡片
- [ ] AI生成的需求卡片准确率 > 70%

### 7.2 可用性标准

- [ ] 从5万词产出10-20个可落地的需求想法
- [ ] 每个需求下有真实的搜索短语支撑
- [ ] 能够快速定位"哪些词属于同一需求"

---

## 8. 未来迭代方向

### 第二轮迭代（+2周）
- Phase 5完整版：tokens词库完善
- Phase 7完整版：增量小组重聚类
- 数据表字段扩展：添加tags, main_tokens (JSON)

### 第三轮迭代（+2周）
- Web UI增强：ClusterSelector, DemandEditor
- 批量操作功能：合并需求、批量标记
- 导出工具：SEO词表、Landing Page素材

### 第四轮迭代（有产品后）
- Phase 6：商业化字段（revenue, landing_url）
- 数据可视化：需求地图、词云、趋势图
- 自动化Pipeline：定期扫描新词、自动推送报告

---

**文档版本**: v1.1
**最后更新**: 2026-01-09
**生成方式**: 基于代码分析自动生成

## 版本变更记录

### v1.1 (2026-01-09)
- 新增P0核心功能 - Reddit板块分析与标注系统 (Phase 6)
- 重新编号后续章节 (2.6 → 2.7, 2.7 → 2.8, 等)

### v1.0 (2026-01-08)
- 初始版本
- 基于代码分析自动生成
