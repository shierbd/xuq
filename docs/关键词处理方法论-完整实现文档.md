# 关键词处理方法论 - 完整实现文档

> **基于君言关键词处理体系的系统化总结**
> 本文档整理自三份核心资料：《面对1亿数据怎么提取需求》、《电商产品快速提取》、《软件特征词汇》

---

## 📑 目录

1. [核心理念](#核心理念)
2. [方法论架构](#方法论架构)
3. [完整处理流程](#完整处理流程)
4. [技术实现策略](#技术实现策略)
5. [与现有系统对比](#与现有系统对比)
6. [优化建议](#优化建议)

---

## 核心理念

### 基本原则

**核心思想**：通过合理的数据减压策略,在保留关键信息的前提下,从海量数据中提取需求。

**三大黄金法则**：
1. **信息守恒**：数据可以减少,但信息不能丢失
2. **逻辑优先**：所有策略必须符合逻辑,不能影响客观数据
3. **特征聚焦**：利用28原则,抓住核心特征快速覆盖主体

### 底层逻辑

```
海量数据 → 合理减压 → 特征提取 → 需求识别 → 框架构建
    ↓          ↓          ↓          ↓          ↓
  1.6亿     5000万    2万样本    需求类别   特征变量
```

---

## 方法论架构

### 架构图

```
┌─────────────────────────────────────────────────────────────┐
│                    君言关键词处理方法论                          │
├─────────────────────────────────────────────────────────────┤
│                                                                │
│  ┌─────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐      │
│  │ Phase 1 │→│ Phase 2  │→│ Phase 3  │→│ Phase 4  │      │
│  │数据挖掘 │  │数据清洗  │  │聚类分析  │  │特征提取  │      │
│  └─────────┘  └──────────┘  └──────────┘  └──────────┘      │
│       │             │             │             │              │
│       ↓             ↓             ↓             ↓              │
│  高频词组合   文字排序去重   分批聚类    片段统计            │
│  优先大词     停用词过滤     标签合并    模板提取            │
│       │             │             │             │              │
│       └─────────────┴─────────────┴─────────────┘              │
│                         │                                      │
│                         ↓                                      │
│              ┌────────────────┐                               │
│              │   需求框架      │                               │
│              ├────────────────┤                               │
│              │ • 需求类别     │                               │
│              │ • 特征变量     │                               │
│              │ • 搜索结构     │                               │
│              └────────────────┘                               │
└─────────────────────────────────────────────────────────────┘
```

### 三大核心方法

#### 方法1：高频词组合 + 优先大词

**挖掘策略**：
```python
策略组合 = 高频词组合 + 优先大词排序

目标：用最少的下载次数覆盖最多的有效数据
原理：利用长尾词的层级关系减少冗余
```

**执行步骤**：
1. 下载主词的长尾词数据（如"软件"的80万数据）
2. 分词并统计词频,取能覆盖80%数据的高频词
3. 主词 + 高频词组合,按**长尾词数量降序**下载
4. 去重合并,最终得到精简但完整的数据集

**实际效果**：
- 原始数据：1.6亿
- 挖掘数据：5000万
- 数据减少：68.75%
- 信息保留：95%+

#### 方法2：文字排序 + 停用词过滤

**去重策略**：
```python
唯一标识 = 文字按拼音排序 + 去除停用词

目标：识别并合并意思相同但表达不同的词
原理：相同意思的词排序后结果一致
```

**示例**：
```
"图片压缩"  → "图压片缩"
"压缩图片"  → "图压片缩"  → 合并为一个
"图片怎么压缩" → "图压片缩" → 合并为一个

结果：3个长尾词 → 1个唯一标识
```

**停用词库**：
- 疑问词：怎么、什么、如何、哪个、哪些
- 空格和符号
- 城市名称（可选）
- 其他无意义词汇

**实际效果**：
- 处理前：5000万
- 处理后：4000万
- 去除冗余：1000万 (20%)

#### 方法3：特征片段 + 模板变量

**核心创新**：通过n-gram片段统计发现搜索模板,再用模板提取变量

**理论基础**：
```
28原则现象:
- 1万个标识(< 1%) → 覆盖1400万长尾词(35%)
- 绝大部分数据集中在少数需求类型
- 存在极高频的搜索模式和结构
```

**实现流程**：

**步骤1：提取连续文本片段**
```python
示例：C盘清理软件下载

3-gram:
  C盘清、盘清理、清理软、理软件、软件下、件下载

4-gram:
  C盘清理、盘清理软、清理软件、理软件下、软件下载

5-gram:
  C盘清理软、盘清理软件、清理软件下、理软件下载
```

**步骤2：统计片段频次**
```
高频片段示例：
  "的软件"     - 680万次
  "软件下载"   - 450万次
  "什么软件"   - 320万次
```

**步骤3：选择Top片段映射需求**
```python
策略:
- 选择Top 1万个片段
- 每个片段提取2个母词
- 得到2万个代表性长尾词
- 这2万词可映射全局需求

时间成本: 2万词聚类 < 1小时
vs
原方案: 180万标识 = 不可能完成
```

---

## 完整处理流程

### Phase 1: 数据挖掘

#### 1.1 初始数据采集

**工具选择**：5118（量大、相对便宜）

**挖掘策略**：
```
第一轮：
  主词 → 下载长尾词(80万) → 分词统计

第二轮：
  选择Top高频词(覆盖80%) →
  主词 + 高频词组合(近300个组合) →
  批量下载（每次按长尾词数量降序）

第三轮：
  去重合并 → 存入数据库
```

**优先大词逻辑**：
```
问题：图片压缩 vs 图片怎么压缩

解决：
- 按"长尾词数量"降序下载
- 优先获得"图片压缩"（长尾词多）
- 自动过滤"图片怎么压缩"（长尾词少）

原理：
- 对于需求分析,两者等价
- 优先保留信息密度更高的词
```

#### 1.2 数据存储

**数据库选择**：SQLite3

**表结构设计**：
```sql
-- 原始数据表
CREATE TABLE raw_keywords (
    keyword TEXT PRIMARY KEY,  -- 自动去重
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 统计：5000万+条记录
```

### Phase 2: 数据清洗

#### 2.1 构建唯一标识

**处理逻辑**：
```python
def generate_unique_id(keyword):
    """
    生成关键词唯一标识

    步骤：
    1. 转小写
    2. 去除停用词（怎么、什么、如何等）
    3. 去除空格和符号
    4. 按拼音排序
    """
    # 示例实现
    keyword = keyword.lower()
    keyword = remove_stop_words(keyword)  # 去除停用词
    keyword = remove_symbols(keyword)      # 去除符号
    chars = list(keyword)
    chars.sort(key=lambda x: pypinyin.pinyin(x))  # 拼音排序
    return ''.join(chars)
```

**停用词库**：
```python
STOP_WORDS = {
    # 疑问词
    '怎么', '什么', '如何', '哪个', '哪些', '为什么',

    # 介词
    '的', '在', '从', '到', '和', '与',

    # 其他
    '可以', '能否', '是否', '有没有',
    # ... 可扩展
}
```

#### 2.2 去重存储

**新表结构**：
```sql
CREATE TABLE cleaned_keywords (
    unique_id TEXT PRIMARY KEY,   -- 唯一标识（自动去重）
    original TEXT NOT NULL,        -- 原始长尾词
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 统计：4000万条记录（去除了1000万冗余）
```

### Phase 3: 聚类分析

#### 3.1 分批聚类策略

**内存限制**：
- 16G内存 → 一次处理300万词
- 数据总量：4000万
- 解决方案：分批处理 + 全局合并

**分批方案**：
```python
# 分割数据
total_data = 40,000,000
batch_size = 2,000,000
num_batches = 20

# 每批单独聚类
for i in range(num_batches):
    batch_data = load_batch(i * batch_size, batch_size)
    cluster_result = clustering(batch_data)
    save_batch_result(i, cluster_result)
```

**全局合并**：
```python
# 合并策略
def merge_clusters():
    """
    将20份分批聚类结果合并

    原理：
    - 同样的标识 = 同一类别
    - 合并相同标识的长尾词
    """
    all_labels = {}

    for batch_id in range(20):
        batch_result = load_batch_result(batch_id)
        for label, keywords in batch_result.items():
            if label not in all_labels:
                all_labels[label] = []
            all_labels[label].extend(keywords)

    return all_labels
```

#### 3.2 聚类结果统计

**原始结果**：
```
总标识数：180万个
频次>1的标识：180万个
频次=1的标识：已过滤

问题：180万标识 = 无法人工审核
```

**Top标识示例**：
```
标识: "3d,建模"
  关联词数: 20万+
  示例词: 3d建模软件、建模软件推荐、3d建模工具...

标识: "视频,剪辑"
  关联词数: 15万+
  示例词: 视频剪辑软件、剪辑视频的软件...
```

### Phase 4: 特征提取

#### 4.1 N-gram片段提取

**连续文本分割算法**：
```python
def extract_ngrams(text, min_n=3, max_n=5):
    """
    提取连续n-gram片段

    Args:
        text: 输入文本
        min_n: 最小gram长度
        max_n: 最大gram长度

    Returns:
        所有n-gram片段列表
    """
    ngrams = []

    for n in range(min_n, max_n + 1):
        for i in range(len(text) - n + 1):
            segment = text[i:i+n]
            ngrams.append(segment)

    return ngrams

# 示例
text = "C盘清理软件下载"
ngrams = extract_ngrams(text, 3, 5)

输出:
  ['C盘清', '盘清理', '清理软', '理软件', '软件下', '件下载',
   'C盘清理', '盘清理软', '清理软件', '理软件下', '软件下载',
   'C盘清理软', '盘清理软件', '清理软件下', '理软件下载']
```

**全局统计**：
```python
def global_ngram_statistics(keywords_list):
    """
    对所有关键词进行n-gram统计
    """
    from collections import Counter

    ngram_counter = Counter()

    for keyword in keywords_list:
        ngrams = extract_ngrams(keyword, 3, 5)
        ngram_counter.update(ngrams)

    # 按频次降序
    return ngram_counter.most_common()
```

**高频片段特征**：
```
Top 10高频片段：
  1. "的软件"      - 680万次（断崖式领先）
  2. "软件下载"    - 450万次
  3. "什么软件"    - 320万次
  4. "件哪个"      - 280万次
  5. "软件好用"    - 250万次
  ...

观察：
- 高频片段与其他片段差距巨大
- 反映了主要的搜索意图和结构
```

#### 4.2 重新聚类（精简版）

**策略**：
```python
# 1. 选择Top 1万个高频片段
top_segments = ngram_counter.most_common(10000)

# 2. 每个片段提取2个母词
sample_keywords = []
for segment, freq in top_segments:
    mothers = find_mother_keywords(segment, count=2)
    sample_keywords.extend(mothers)

# 3. 去重得到约2万个代表性长尾词
sample_keywords = list(set(sample_keywords))  # ~20,000

# 4. 对2万词聚类
cluster_result = clustering(sample_keywords)
```

**时间成本对比**：
```
方案A（传统）：
  180万标识 × 人工审核 = 不可能完成

方案B（片段法）：
  2万样本词聚类 < 1小时
  人工审核2万词 < 1小时
  总计: 2小时内完成

效果：
  覆盖率 > 95%（通过片段映射全局）
```

#### 4.3 需求类别识别

**分类框架**：
```
软件领域需求类别（6大类）

1. 寻找类（95%+的搜索）
   ├─ 下载类：XXX软件下载、XXX软件安装包
   ├─ 推荐类：XXX软件推荐、XXX软件哪个好
   ├─ 对比类：XXX软件对比、最好的XXX软件
   └─ 免费类：免费XXX软件、XXX软件破解版

2. 操作类（<2%）
   └─ 安装类：XXX软件怎么安装、XXX软件安装教程

3. 问题类（<1%）
   └─ 故障类：XXX软件打不开、XXX软件闪退

4. 询价类（极少）
   └─ 价格类：XXX软件多少钱、XXX软件价格

5. 教程类（<1%）
   └─ 使用类：XXX软件教程、XXX软件怎么用

6. 其他类（<1%）
   └─ 杂项
```

**核心发现**：
```
惊人结论：
  软件领域 95%+ 的搜索 = 寻找某个软件

意义：
  - 指导SEO策略（聚焦软件详情页和聚合页）
  - 解释竞价现象（软件下载站广告多）
  - 明确产品方向（软件下载站价值巨大）
```

#### 4.4 特征变量提取

**模板变量提取法**（核心创新）

**理论基础**：
```
逆向思维：
  正向：搜索模板 → 包含变量
  逆向：变量 → 存在于多个模板

应用：
  1. 用确定的种子变量 → 发现模板
  2. 用发现的模板 → 提取更多变量
  3. 循环迭代 → 获得完整变量库
```

**操作流程**：

**Step 1: 准备种子变量**
```python
# 从无法聚类的长尾词中挑选
# 原因：无法聚类 = 包含极个性化词汇 = 优质种子

seed_variables = {
    '功能类': ['清理', '识别', '拍照', '修复', '压缩', '转换'],
    '对象类': ['图片', '视频', '文档', '照片', '文件'],
    '渠道类': ['微信', '微博', 'qq', '抖音'],
    '群体类': ['儿童', '学生', '老人', '程序员']
}
```

**Step 2: 提取模板**
```python
def extract_templates(keywords, variables):
    """
    用种子变量提取搜索模板

    逻辑：
    1. 在关键词中查找包含变量的词
    2. 将变量替换为占位符[X]
    3. 统计模板频次
    """
    from collections import Counter

    template_counter = Counter()

    for keyword in keywords:
        for var in variables:
            if var in keyword:
                template = keyword.replace(var, '[X]')
                template_counter[template] += 1

    return template_counter.most_common()

# 示例结果
模板示例：
  "[X]的软件" - 出现3500次（包含"清理的软件"、"拍照的软件"等）
  "[X]软件下载" - 出现2800次
  "什么[X]软件好" - 出现1200次
```

**Step 3: 用模板提取变量**
```python
def extract_variables(keywords, templates):
    """
    用模板提取新变量

    逻辑：
    1. 对每个模板,查找匹配的关键词
    2. 提取占位符[X]位置的词汇
    3. 统计变量频次和适配模板数
    """
    from collections import Counter, defaultdict

    variable_freq = Counter()
    variable_templates = defaultdict(set)

    for template in templates:
        pattern = template.replace('[X]', '(.+?)')

        for keyword in keywords:
            match = re.match(pattern, keyword)
            if match:
                variable = match.group(1)
                variable_freq[variable] += 1
                variable_templates[variable].add(template)

    # 质量过滤：适配模板数 >= 3
    quality_variables = [
        (var, freq) for var, freq in variable_freq.items()
        if len(variable_templates[var]) >= 3
    ]

    return sorted(quality_variables, key=lambda x: x[1], reverse=True)
```

**Step 4: 循环迭代**
```python
def iterative_extraction(keywords, initial_seeds, max_rounds=3):
    """
    循环提取模板和变量
    """
    variables = initial_seeds
    all_templates = set()
    all_variables = set(initial_seeds)

    for round_num in range(max_rounds):
        # 提取模板
        templates = extract_templates(keywords, variables)
        new_templates = [t for t, freq in templates if freq >= 10]
        all_templates.update(new_templates)

        # 提取变量
        new_vars = extract_variables(keywords, new_templates)
        new_vars = [v for v, freq in new_vars if freq >= 5]

        # 更新变量库
        variables = list(set(variables + new_vars))
        all_variables.update(new_vars)

        print(f"Round {round_num + 1}:")
        print(f"  模板数: {len(all_templates)}")
        print(f"  变量数: {len(all_variables)}")

    return all_templates, all_variables
```

**实际成果**：

根据文档《软件特征词汇》,最终提取到四类特征变量：

```python
特征变量统计：

渠道类（平台/品牌）:
  总数: 8000+ 个
  示例: 微博、小红书、抖音、快手、微信、qq、淘宝、拼多多...
  特点: 模板精准,数据干净,数量最多

功能类（软件功能）:
  总数: 几千个
  Top 500示例: 清理、识别、拍照、修复、压缩、转换、加密、
              截图、录屏、录音、播放、搜索、阅读、监控...

对象类（处理对象）:
  总数: 几千个
  示例: 照片、视频、笔记、相册、图片、文档、英语、
        日记、小说、漫画、音乐、钢琴...

群体类（目标用户）:
  总数: 几百个
  示例: 儿童、学生、老人、宝宝、情侣、美女、幼儿、
        高中生、程序员、摄影师、建筑师...
```

**数据价值**：
```
应用场景：
1. 深度研究：定向挖掘特定变量的长尾词
2. 内容创作：生成标题、TDK模板
3. SEM账户：科学划分广告结构
4. SEO策略：设计页面模板和URL结构
5. 竞品分析：了解行业全貌
```

#### 4.5 搜索结构提取

**来源**：高频n-gram片段

**提取方法**：
```python
def extract_search_patterns(high_freq_segments):
    """
    从高频片段中提取搜索结构

    过滤标准：
    1. 不是明显缺字的片段
    2. 有完整语义的结构
    3. 频次足够高（代表性强）
    """
    search_patterns = []

    for segment, freq in high_freq_segments:
        if is_complete_structure(segment):  # 人工判断或规则过滤
            search_patterns.append({
                'pattern': segment,
                'frequency': freq,
                'variables': extract_variable_positions(segment)
            })

    return search_patterns
```

**典型搜索结构**（软件领域）：
```
1. 下载类：
   - [X]软件下载
   - [X]软件安装包
   - 下载[X]软件
   - [X]软件官方下载

2. 推荐类：
   - [X]软件哪个好
   - 最好的[X]软件
   - [X]软件推荐
   - 好用的[X]软件

3. 疑问类：
   - [X]软件怎么用
   - 什么[X]软件好
   - 有没有[X]软件
   - [X]用什么软件

4. 评价类：
   - [X]软件好用吗
   - [X]软件怎么样
   - [X]软件评测
```

**应用价值**：
```
SEO应用：
  - TDK模板设计参考
  - 标题结构优化
  - 页面H1-H6布局

SEM应用：
  - 账户结构划分依据
  - 广告创意模板
  - 关键词分组逻辑

内容应用：
  - 自媒体标题公式
  - 文章结构设计
```

---

## 技术实现策略

### 核心算法

#### 算法1：高频词组合挖掘

```python
def high_frequency_combination(main_keyword):
    """
    高频词组合策略

    步骤：
    1. 下载主词长尾词
    2. 分词统计词频
    3. 选择覆盖80%的高频词
    4. 组合并批量下载
    """
    # Step 1: 初始下载
    initial_keywords = download_longtail(main_keyword, limit=800000)

    # Step 2: 分词统计
    word_freq = tokenize_and_count(initial_keywords)

    # Step 3: 选择高频词（帕累托原则）
    total_freq = sum(word_freq.values())
    cumsum = 0
    high_freq_words = []

    for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):
        cumsum += freq
        high_freq_words.append(word)
        if cumsum >= total_freq * 0.8:  # 覆盖80%
            break

    # Step 4: 组合并下载
    all_keywords = set(initial_keywords)
    for word in high_freq_words:
        combined = f"{main_keyword} {word}"
        longtails = download_longtail(combined, limit=800000, sort_by='longtail_count_desc')
        all_keywords.update(longtails)

    return list(all_keywords)
```

#### 算法2：文字排序去重

```python
import pypinyin

def create_unique_identifier(keyword, stop_words):
    """
    创建唯一标识符

    步骤：
    1. 去除停用词
    2. 去除符号和空格
    3. 按拼音排序
    """
    # Step 1: 去除停用词
    for stop_word in stop_words:
        keyword = keyword.replace(stop_word, '')

    # Step 2: 去除符号
    keyword = re.sub(r'[^\w]', '', keyword)

    # Step 3: 按拼音排序
    chars = list(keyword)
    chars.sort(key=lambda x: ''.join([p[0] for p in pypinyin.pinyin(x, style=pypinyin.NORMAL)]))

    return ''.join(chars)

# 去重逻辑
def deduplicate_keywords(keywords, stop_words):
    """
    基于唯一标识去重
    """
    unique_dict = {}  # {identifier: original_keyword}

    for keyword in keywords:
        identifier = create_unique_identifier(keyword, stop_words)
        if identifier not in unique_dict:
            unique_dict[identifier] = keyword

    return list(unique_dict.values())

# 示例
keywords = ["图片压缩", "压缩图片", "图片怎么压缩"]
stop_words = {"怎么", "什么", "如何"}

result = deduplicate_keywords(keywords, stop_words)
# 结果：["图片压缩"]  (3个词合并为1个)
```

#### 算法3：分批聚类与合并

```python
def batch_clustering(keywords, batch_size=2000000):
    """
    分批聚类大规模数据

    步骤：
    1. 分割数据为多个批次
    2. 每个批次独立聚类
    3. 全局合并相同标签
    """
    import math

    num_batches = math.ceil(len(keywords) / batch_size)
    batch_results = []

    # Step 1-2: 分批聚类
    for i in range(num_batches):
        start = i * batch_size
        end = min((i + 1) * batch_size, len(keywords))
        batch = keywords[start:end]

        print(f"Processing batch {i+1}/{num_batches}")
        cluster_result = keyword_clustering(batch)  # 调用聚类函数
        batch_results.append(cluster_result)

    # Step 3: 全局合并
    global_clusters = {}
    for batch_result in batch_results:
        for label, kw_list in batch_result.items():
            if label not in global_clusters:
                global_clusters[label] = []
            global_clusters[label].extend(kw_list)

    return global_clusters

def keyword_clustering(keywords):
    """
    关键词聚类（君言的关键词管理软件实现）

    策略：以空间换时间
    - 统计所有可能的词组合
    - 根据共现频次聚类
    """
    from collections import defaultdict, Counter

    # 分词并统计共现
    word_pairs = defaultdict(Counter)

    for keyword in keywords:
        tokens = tokenize(keyword)
        for i, token1 in enumerate(tokens):
            for token2 in tokens[i+1:]:
                key = tuple(sorted([token1, token2]))
                word_pairs[token1][token2] += 1

    # 基于共现频次聚类
    clusters = {}
    for keyword in keywords:
        tokens = tokenize(keyword)
        label = ','.join(sorted(tokens))  # 简化示例,实际算法更复杂

        if label not in clusters:
            clusters[label] = []
        clusters[label].append(keyword)

    return clusters
```

#### 算法4：N-gram片段统计

```python
from collections import Counter

def ngram_segment_statistics(keywords, min_n=3, max_n=5, top_k=10000):
    """
    N-gram片段统计

    步骤：
    1. 对每个关键词提取所有n-gram
    2. 全局统计频次
    3. 返回Top-K高频片段
    """
    ngram_counter = Counter()

    # Step 1-2: 提取并统计
    for keyword in keywords:
        for n in range(min_n, max_n + 1):
            for i in range(len(keyword) - n + 1):
                segment = keyword[i:i+n]
                ngram_counter[segment] += 1

    # Step 3: 返回Top-K
    return ngram_counter.most_common(top_k)

# 示例
keywords = ["图片压缩软件", "视频压缩软件", "压缩软件下载"]
top_segments = ngram_segment_statistics(keywords, min_n=3, max_n=4, top_k=5)

# 结果示例：
# [("压缩软", 3), ("软件", 3), ("缩软件", 3), ("图片压", 1), ("压缩软件", 2)]
```

#### 算法5：模板变量迭代提取

```python
def template_variable_extraction(keywords, seed_variables, max_iterations=3):
    """
    模板变量迭代提取算法

    核心创新：
    - 变量 ↔ 模板 双向迭代
    - 质量过滤（变量必须适配多个模板）
    """
    import re
    from collections import Counter, defaultdict

    all_templates = set()
    all_variables = set(seed_variables)

    for iteration in range(max_iterations):
        print(f"\n=== Iteration {iteration + 1} ===")

        # Phase 1: 用变量提取模板
        template_counter = Counter()
        for keyword in keywords:
            for var in all_variables:
                if var in keyword:
                    template = keyword.replace(var, '[X]')
                    template_counter[template] += 1

        # 过滤：频次 >= 5
        new_templates = [t for t, freq in template_counter.items() if freq >= 5]
        all_templates.update(new_templates)
        print(f"  Templates discovered: {len(new_templates)}")

        # Phase 2: 用模板提取变量
        variable_freq = Counter()
        variable_templates = defaultdict(set)

        for template in new_templates:
            pattern = template.replace('[X]', '(.+?)')
            for keyword in keywords:
                match = re.match(pattern, keyword)
                if match:
                    var = match.group(1)
                    variable_freq[var] += 1
                    variable_templates[var].add(template)

        # 过滤：适配模板数 >= 3 且 频次 >= 5
        new_variables = [
            var for var, freq in variable_freq.items()
            if freq >= 5 and len(variable_templates[var]) >= 3
        ]

        before_count = len(all_variables)
        all_variables.update(new_variables)
        after_count = len(all_variables)
        print(f"  Variables discovered: {after_count - before_count}")

        # 收敛判断
        if after_count == before_count:
            print("  Converged!")
            break

    return list(all_templates), list(all_variables)

# 使用示例
keywords = load_keywords()  # 加载4000万关键词（实际会采样）
seed_variables = ['微信', '抖音', 'qq', '清理', '压缩', '拍照']

templates, variables = template_variable_extraction(
    keywords,
    seed_variables,
    max_iterations=3
)

print(f"\n最终结果:")
print(f"  模板数: {len(templates)}")
print(f"  变量数: {len(variables)}")
```

### 数据结构设计

#### 数据库Schema

```sql
-- 1. 原始关键词表
CREATE TABLE raw_keywords (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT UNIQUE NOT NULL,
    source VARCHAR(50),  -- 来源：5118、百度等
    longtail_count INT,  -- 长尾词数量
    search_volume INT,   -- 搜索量
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_keyword ON raw_keywords(keyword);
CREATE INDEX idx_longtail ON raw_keywords(longtail_count DESC);

-- 2. 清洗后关键词表
CREATE TABLE cleaned_keywords (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    unique_id TEXT UNIQUE NOT NULL,  -- 唯一标识
    original TEXT NOT NULL,           -- 原始词
    raw_id INTEGER,                   -- 关联原始表
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (raw_id) REFERENCES raw_keywords(id)
);

CREATE INDEX idx_unique_id ON cleaned_keywords(unique_id);

-- 3. 聚类结果表
CREATE TABLE cluster_labels (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    label TEXT NOT NULL,              -- 聚类标签
    keyword_count INT DEFAULT 0,      -- 关联词数
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE cluster_keywords (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    cluster_id INTEGER NOT NULL,
    keyword_id INTEGER NOT NULL,
    FOREIGN KEY (cluster_id) REFERENCES cluster_labels(id),
    FOREIGN KEY (keyword_id) REFERENCES cleaned_keywords(id)
);

CREATE INDEX idx_cluster ON cluster_keywords(cluster_id);

-- 4. N-gram片段表
CREATE TABLE ngram_segments (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    segment TEXT UNIQUE NOT NULL,
    frequency INT NOT NULL,
    gram_size INT NOT NULL,  -- 3-gram, 4-gram, 5-gram
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_frequency ON ngram_segments(frequency DESC);
CREATE INDEX idx_gram_size ON ngram_segments(gram_size);

-- 5. 模板表
CREATE TABLE search_templates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    template TEXT UNIQUE NOT NULL,     -- 如"[X]软件下载"
    frequency INT NOT NULL,
    variable_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 6. 变量表（特征词汇）
CREATE TABLE feature_variables (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    variable TEXT UNIQUE NOT NULL,
    category VARCHAR(20),              -- 类别：功能、对象、渠道、群体
    frequency INT NOT NULL,
    template_count INT DEFAULT 0,      -- 适配的模板数
    verified BOOLEAN DEFAULT FALSE,     -- 是否人工验证
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_category ON feature_variables(category);
CREATE INDEX idx_verified ON feature_variables(verified);

-- 7. 需求分类表
CREATE TABLE demand_categories (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    category VARCHAR(50) NOT NULL,     -- 如"寻找-下载类"
    keyword_count INT DEFAULT 0,
    percentage DECIMAL(5,2),           -- 占比
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 性能优化策略

#### 1. 内存优化

```python
# 问题：4000万数据无法一次加载到内存

# 解决方案1：生成器（Generator）
def keyword_generator(db_path, batch_size=100000):
    """
    使用生成器逐批读取数据
    """
    import sqlite3

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    offset = 0
    while True:
        cursor.execute(
            "SELECT keyword FROM cleaned_keywords LIMIT ? OFFSET ?",
            (batch_size, offset)
        )
        batch = cursor.fetchall()

        if not batch:
            break

        for row in batch:
            yield row[0]

        offset += batch_size

    conn.close()

# 使用
for keyword in keyword_generator('keywords.db'):
    process(keyword)  # 逐个处理,不占用大量内存
```

```python
# 解决方案2：分批处理 + 中间结果持久化
def batch_process_with_checkpoint(keywords, process_func, batch_size=2000000):
    """
    分批处理并保存检查点
    """
    import math

    num_batches = math.ceil(len(keywords) / batch_size)

    for i in range(num_batches):
        # 检查是否已处理
        checkpoint_file = f"checkpoint_batch_{i}.pkl"
        if os.path.exists(checkpoint_file):
            print(f"Batch {i} already processed, skipping...")
            continue

        # 处理当前批次
        start = i * batch_size
        end = min((i + 1) * batch_size, len(keywords))
        batch = keywords[start:end]

        result = process_func(batch)

        # 保存检查点
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(result, f)

        print(f"Batch {i+1}/{num_batches} completed")

    # 合并所有结果
    all_results = []
    for i in range(num_batches):
        with open(f"checkpoint_batch_{i}.pkl", 'rb') as f:
            all_results.append(pickle.load(f))

    return merge_results(all_results)
```

#### 2. 计算优化

```python
# 问题：n-gram统计需要遍历所有关键词,计算量大

# 解决方案：使用Trie树 + 后缀数组
class TrieNode:
    def __init__(self):
        self.children = {}
        self.frequency = 0
        self.is_end = False

class NGramTrie:
    """
    使用Trie树加速n-gram统计
    """
    def __init__(self):
        self.root = TrieNode()

    def insert_ngrams(self, text, min_n=3, max_n=5):
        """
        插入一个文本的所有n-gram
        """
        for n in range(min_n, max_n + 1):
            for i in range(len(text) - n + 1):
                segment = text[i:i+n]
                self._insert(segment)

    def _insert(self, segment):
        """
        插入单个n-gram并更新频次
        """
        node = self.root
        for char in segment:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
            node.frequency += 1
        node.is_end = True

    def get_high_frequency_ngrams(self, min_freq=100):
        """
        获取高频n-gram
        """
        results = []
        self._dfs(self.root, "", results, min_freq)
        return sorted(results, key=lambda x: x[1], reverse=True)

    def _dfs(self, node, path, results, min_freq):
        """
        深度优先遍历Trie树
        """
        if node.is_end and node.frequency >= min_freq:
            results.append((path, node.frequency))

        for char, child in node.children.items():
            self._dfs(child, path + char, results, min_freq)

# 使用
trie = NGramTrie()
for keyword in keywords:
    trie.insert_ngrams(keyword, min_n=3, max_n=5)

high_freq_ngrams = trie.get_high_frequency_ngrams(min_freq=1000)
```

```python
# 并行处理
from multiprocessing import Pool

def parallel_ngram_extraction(keywords, num_processes=4):
    """
    并行提取n-gram
    """
    import math

    chunk_size = math.ceil(len(keywords) / num_processes)
    chunks = [keywords[i:i+chunk_size] for i in range(0, len(keywords), chunk_size)]

    with Pool(num_processes) as pool:
        results = pool.map(extract_ngrams_batch, chunks)

    # 合并结果
    from collections import Counter
    merged = Counter()
    for result in results:
        merged.update(result)

    return merged

def extract_ngrams_batch(keywords_chunk):
    """
    处理单个chunk
    """
    from collections import Counter
    counter = Counter()

    for keyword in keywords_chunk:
        ngrams = extract_ngrams(keyword, 3, 5)
        counter.update(ngrams)

    return counter
```

#### 3. 数据库优化

```sql
-- 优化1：建立合适的索引
CREATE INDEX idx_keyword_text ON cleaned_keywords(original);
CREATE INDEX idx_cluster_label ON cluster_labels(label);
CREATE INDEX idx_ngram_freq ON ngram_segments(frequency DESC);

-- 优化2：使用分区表（如果数据库支持）
CREATE TABLE ngram_segments_3gram AS
SELECT * FROM ngram_segments WHERE gram_size = 3;

CREATE TABLE ngram_segments_4gram AS
SELECT * FROM ngram_segments WHERE gram_size = 4;

-- 优化3：定期清理和vacuum
DELETE FROM ngram_segments WHERE frequency < 5;
VACUUM;

-- 优化4：使用临时表加速查询
CREATE TEMP TABLE temp_high_freq AS
SELECT segment, frequency FROM ngram_segments
WHERE frequency >= 1000
ORDER BY frequency DESC;
```

---

## 与现有系统对比

### 对比分析表

| 维度 | 君言方法论 | 我们的词根聚类系统 | 差异与优势 |
|------|------------|-------------------|------------|
| **数据采集** | 高频词组合 + 优先大词 | 直接导入原始数据 | 君言方法更节省成本,预先过滤冗余 |
| **数据清洗** | 文字排序 + 停用词过滤 | 基本去重 | 君言方法去重更彻底(20%冗余) |
| **聚类策略** | 分批聚类 + 全局合并 | HDBSCAN一次性聚类 | 君言适合超大数据,我们适合中等数据 |
| **需求提取** | 特征片段 + 重新聚类 | 直接从聚类结果提取 | 君言方法引入了创新的"片段法" |
| **变量提取** | 模板-变量迭代 | 未实现 | **核心差异**,君言有完整的特征词库构建 |
| **应用场景** | SEO/SEM/内容运营 | 需求管理/产品分析 | 定位不同但可互补 |

### 核心创新对比

#### 创新1：特征片段法

**君言的创新**：
```
传统方法：
  180万聚类标签 → 无法人工审核

君言方法：
  4000万数据 → n-gram片段统计 → Top 1万片段
  → 提取2万样本词 → 聚类 → 人工审核(<2小时)

创新点：
  - 用片段映射全局
  - 28原则的极致应用
  - 从"不可能"到"2小时"
```

**我们的方法**：
```
基于HDBSCAN聚类：
  全量数据 → 直接聚类 → 得到聚类结果 → 人工筛选

局限：
  - 数据量受限（内存限制）
  - 没有"片段映射"这一层
```

**结论**：可以引入片段法作为预处理步骤

#### 创新2：模板-变量迭代提取

**君言的创新**：
```
双向迭代：
  种子变量 → 提取模板 → 提取更多变量 → 提取更多模板 → ...

质量保证：
  - 变量必须适配>=3个模板
  - 模板必须频次>=5

结果：
  - 功能词库：几千个
  - 对象词库：几千个
  - 渠道词库：8000+个
  - 群体词库：几百个
```

**我们的方法**：
```
Token提取：
  短语 → 分词 → 统计频次 → LLM分类

局限：
  - 只提取单个token,没有提取"模板结构"
  - 没有按维度分类（功能/对象/渠道/群体）
  - 没有迭代机制扩充词库
```

**结论**：**强烈建议实现**模板-变量提取功能

#### 创新3：搜索结构识别

**君言的创新**：
```
从高频片段中提取：
  - 典型搜索模板
  - 用户搜索习惯
  - TDK设计参考

应用：
  - SEO: TDK模板设计
  - SEM: 账户结构划分
  - 内容: 标题公式
```

**我们的方法**：
```
需求模式分析：
  提取 [intent] [action] [object] 模式

局限：
  - 模式较抽象
  - 没有提取具体的搜索结构模板
```

**结论**：可以增加"搜索结构"分析模块

### 系统整合建议

**建议1：增加"特征片段"模块**

在Phase 4和Phase 5之间插入新阶段：

```python
# 新增 Phase 4.5: 特征片段分析
def run_phase4_5_segments(sample_size=10000, min_frequency=8):
    """
    Phase 4.5: 提取N-gram特征片段

    输入: 所有短语
    输出: 高频片段 + 2万样本词
    """
    print("\n【Phase 4.5】特征片段分析")

    # 1. 加载短语
    phrases = load_all_phrases()

    # 2. N-gram统计
    ngram_stats = extract_ngrams_global(
        phrases,
        min_n=3,
        max_n=5
    )

    # 3. 选择Top片段
    top_segments = ngram_stats.most_common(10000)

    # 4. 提取样本词
    sample_keywords = []
    for segment, freq in top_segments:
        mothers = find_mother_keywords(segment, count=2)
        sample_keywords.extend(mothers)

    sample_keywords = list(set(sample_keywords))

    # 5. 对样本词聚类
    sample_clusters = clustering(sample_keywords)

    # 6. 保存结果
    save_segments(top_segments)
    save_sample_clusters(sample_clusters)

    return sample_clusters
```

**建议2：实现"模板-变量提取"功能**

```python
# 新增功能：长尾变量提取
def run_template_variable_extraction():
    """
    提取特征变量（类似君言的"长尾变量"功能）

    输出：
    - 功能类变量（清理、压缩、转换等）
    - 对象类变量（图片、视频、文档等）
    - 渠道类变量（微信、抖音、淘宝等）
    - 群体类变量（学生、老人、程序员等）
    """
    print("\n【新功能】特征变量提取")

    # 1. 准备种子变量
    seed_vars = load_seed_variables()

    # 2. 迭代提取
    templates, variables = template_variable_extraction(
        keywords=load_all_phrases(),
        seed_variables=seed_vars,
        max_iterations=3
    )

    # 3. 按类别分类
    categorized = {
        'function': [],  # 功能类
        'object': [],    # 对象类
        'channel': [],   # 渠道类
        'audience': []   # 群体类
    }

    # 使用LLM分类或人工分类
    for var in variables:
        category = classify_variable(var)
        categorized[category].append(var)

    # 4. 保存到数据库
    save_feature_variables(categorized)

    return categorized
```

**建议3：添加"搜索结构"分析**

```python
# 在Phase 5报告中增加搜索结构部分
def analyze_search_structures(high_freq_segments):
    """
    从高频片段中提取搜索结构
    """
    structures = []

    # 人工筛选或规则过滤
    for segment, freq in high_freq_segments:
        if is_complete_structure(segment):
            structures.append({
                'structure': segment,
                'frequency': freq,
                'category': categorize_structure(segment)
            })

    return structures

# 示例输出
{
    'download': ['[X]软件下载', '下载[X]软件', '[X]下载安装'],
    'recommend': ['[X]软件推荐', '好用的[X]软件', '[X]软件哪个好'],
    'question': ['[X]软件怎么用', '什么[X]软件', '有没有[X]软件']
}
```

---

## 优化建议

### 短期优化（1-2周）

**1. 数据清洗优化**

当前：基本去重
```python
# 现状
dedup = df.drop_duplicates(subset=['phrase'])
```

优化：引入文字排序去重
```python
# 优化后
def create_unique_id(phrase):
    # 去除停用词
    for stop_word in STOP_WORDS:
        phrase = phrase.replace(stop_word, '')
    # 去除符号
    phrase = re.sub(r'[^\w]', '', phrase)
    # 拼音排序
    chars = list(phrase)
    chars.sort(key=lambda x: pypinyin.lazy_pinyin(x))
    return ''.join(chars)

df['unique_id'] = df['phrase'].apply(create_unique_id)
df = df.drop_duplicates(subset=['unique_id'])
```

预期效果：额外去除20%冗余数据

**2. 停用词库完善**

当前：使用基础英文停用词
```python
STOP_WORDS = {'the', 'a', 'an', ...}  # 英文为主
```

优化：构建中文停用词库
```python
STOP_WORDS = {
    # 疑问词
    '怎么', '什么', '如何', '哪个', '哪些', '为什么', '怎样',

    # 介词
    '的', '在', '从', '到', '和', '与', '对', '为',

    # 动词
    '可以', '能否', '是否', '有没有', '想要', '需要',

    # 形容词
    '好的', '不错的', '优质的',

    # 其他
    '吗', '呢', '啊', '哦'
}
```

**3. Token提取优化**

当前：仅提取单token
```python
tokens = extract_tokens(phrases, min_frequency=8)  # 只有1-gram
```

优化：引入N-gram提取
```python
# 已实现！就是你的extract_ngrams函数
ngrams = extract_ngrams(
    phrases,
    max_gram_size=4,
    min_frequency=5,
    priority_mode=True
)
```

✅ 这个已经完成了！

### 中期优化（1个月）

**1. 实现特征片段分析**

新增Phase 4.5（如前文建议）

预期价值：
- 从不可能审核 → 2小时审核
- 快速识别主要需求类别
- 为后续分析提供高质量样本

**2. 实现模板-变量提取**

核心功能（如前文建议）

预期成果：
- 功能词库：几千个
- 对象词库：几千个
- 渠道词库：8000+
- 群体词库：几百个

应用价值：
- 深度需求挖掘
- 竞品分析
- SEO/SEM策略

**3. 需求分类体系建立**

参考君言的6大需求类别：

```python
DEMAND_CATEGORIES = {
    'search': {  # 寻找类（最重要）
        'download': ['下载', '安装包', 'apk'],
        'recommend': ['推荐', '哪个好', '排行'],
        'compare': ['对比', '最好的', 'vs'],
        'free': ['免费', '破解版', '绿色版']
    },
    'operation': {  # 操作类
        'install': ['怎么安装', '安装教程'],
        'use': ['怎么用', '使用方法']
    },
    'problem': {  # 问题类
        'error': ['打不开', '闪退', '报错']
    },
    'price': {  # 询价类
        'cost': ['多少钱', '价格', '收费']
    },
    'tutorial': {  # 教程类
        'guide': ['教程', '入门', '学习']
    },
    'other': {}  # 其他类
}
```

实现自动分类：
```python
def categorize_demand(phrase, tokens_classified):
    """
    基于token分类和关键词匹配进行需求分类
    """
    # 方法1：基于token类型判断
    # 方法2：基于关键词匹配
    # 方法3：LLM辅助分类

    return category
```

### 长期优化（2-3个月）

**1. 构建完整的知识图谱**

```python
知识图谱结构：

需求类别 → 搜索模板 → 特征变量 → 具体短语

示例：
  寻找-下载类 →
    "[X]软件下载" →
      功能(压缩、清理、拍照) →
        "图片压缩软件下载"
        "c盘清理软件下载"
        "拍照软件下载"
```

**2. 开发可视化分析工具**

功能清单：
- 需求类别分布饼图
- 高频片段词云
- 特征变量分类树
- 搜索结构模板库
- 变量关联网络图

**3. 建立自动化分析流程**

```python
完整自动化流程：

数据导入 →
  自动清洗（文字排序去重） →
    分批聚类 →
      特征片段提取 →
        样本聚类 →
          需求分类 →
            模板提取 →
              变量提取（迭代3轮） →
                生成完整报告

时间：全自动 < 24小时
人工：仅需审核2万样本词（2小时）
```

---

## 总结

### 核心要点回顾

**1. 三大核心策略**

```
策略1：高频词组合 + 优先大词
  → 用最少下载获得最完整数据

策略2：文字排序 + 停用词过滤
  → 识别并合并相同意思的不同表达

策略3：特征片段 + 模板变量
  → 用2万样本词映射4000万数据
```

**2. 最大创新**

```
传统思路：
  180万聚类标签 → 人工无法审核 → 无解

君言创新：
  N-gram片段统计 → Top片段映射 → 2万样本
  → 可审核、可分析、可应用
```

**3. 价值体现**

```
SEO价值：
  - 页面结构设计（详情页+聚合页）
  - TDK模板参考
  - 内链结构优化

SEM价值：
  - 账户结构划分
  - 关键词分组
  - 广告创意模板

产品价值：
  - 需求发现
  - 竞品分析
  - 市场定位

运营价值：
  - 内容方向
  - 标题公式
  - 用户画像
```

### 实施路线图

```
第1周：
  ✓ 实现文字排序去重
  ✓ 完善停用词库
  ✓ 验证N-gram提取（已完成）

第2-3周：
  □ 实现Phase 4.5（特征片段分析）
  □ 开发样本聚类审核界面
  □ 建立需求分类体系

第4-6周：
  □ 实现模板-变量提取
  □ 构建特征变量词库
  □ 开发变量管理界面

第7-8周：
  □ 整合所有功能
  □ 完善Web UI
  □ 编写用户文档

第9-12周：
  □ 知识图谱构建
  □ 可视化工具开发
  □ 自动化流程优化
```

### 关键成功因素

1. **逻辑优先**：所有策略必须符合逻辑,不能影响客观数据
2. **迭代优化**：模板-变量双向迭代,循环提升质量
3. **质量过滤**：变量必须适配多个模板,频次满足阈值
4. **人工审核**：关键环节保留人工审核,确保准确性
5. **工具支持**：开发易用的工具,降低操作门槛

---

## 附录

### 附录A：完整代码示例

见独立代码文件（TODO）

### 附录B：数据示例

见《软件特征词汇.md》

### 附录C：参考资料

1. 《关键词报告：面对1亿数据怎么提取需求》- 君言
2. 《实操案例：电商产品快速提取》- 君言
3. 《软件特征词汇》- 数据文件

---

**文档版本**: v1.0
**创建日期**: 2025-12-23
**最后更新**: 2025-12-23
**维护者**: Claude + User

---

> 💡 **核心启示**：面对海量数据,不是暴力处理,而是用合理的策略逐层减压,在保留关键信息的前提下,化"不可能"为"可实现"。这才是真正的数据智慧。
