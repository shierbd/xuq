# æŠ€æœ¯å®ç°å®¡æŸ¥ä¸ä¼˜åŒ–å»ºè®®

**å®¡æŸ¥å¯¹è±¡**: è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜ç³»ç»Ÿå®Œæ•´æµç¨‹
**å®¡æŸ¥æ—¥æœŸ**: 2024-12-18
**å®¡æŸ¥ç›®æ ‡**: è¯„ä¼°æµç¨‹åˆç†æ€§ã€è‡ªåŠ¨åŒ–è¾¹ç•Œã€æŠ€æœ¯å®ç°æ–¹æ¡ˆ

---

## ğŸ“‹ ç›®å½•

1. [æµç¨‹é€»è¾‘åˆç†æ€§è¯„ä¼°](#1-æµç¨‹é€»è¾‘åˆç†æ€§è¯„ä¼°)
2. [è‡ªåŠ¨åŒ–è¾¹ç•Œåˆ’åˆ†](#2-è‡ªåŠ¨åŒ–è¾¹ç•Œåˆ’åˆ†)
3. [æ•°æ®è¡¨è®¾è®¡æ–¹æ¡ˆ](#3-æ•°æ®è¡¨è®¾è®¡æ–¹æ¡ˆ)
4. [ä»£ç æ¶æ„ä¼˜åŒ–](#4-ä»£ç æ¶æ„ä¼˜åŒ–)
5. [èšç±»å‚æ•°ä¼˜åŒ–](#5-èšç±»å‚æ•°ä¼˜åŒ–)
6. [å¢é‡æ›´æ–°æœºåˆ¶](#6-å¢é‡æ›´æ–°æœºåˆ¶)
7. [å®æ–½è·¯çº¿å›¾](#7-å®æ–½è·¯çº¿å›¾)
8. [é£é™©ä¸æŒ‘æˆ˜](#8-é£é™©ä¸æŒ‘æˆ˜)

---

## 1. æµç¨‹é€»è¾‘åˆç†æ€§è¯„ä¼°

### 1.1 æ•´ä½“æµç¨‹è¯„ä»·

**âœ… ä¼˜ç‚¹**:

1. **æ¸è¿›å¼èšç±»**: å¤§ç»„â†’å°ç»„çš„äºŒçº§èšç±»æ€è·¯æ¸…æ™°
   - é¿å…ä¸€æ¬¡æ€§é¢å¯¹5ä¸‡è¯çš„å¤æ‚åº¦
   - ç¬¦åˆäººç±»è®¤çŸ¥ï¼šå…ˆå®è§‚åå¾®è§‚

2. **å¢é‡æ›´æ–°æœºåˆ¶**: è®¾è®¡äº†å¤šè½®æ‰©è¯çš„å¢é‡é€»è¾‘
   - é¿å…æ¯æ¬¡ä»å¤´å¼€å§‹
   - æ•°æ®ç´¯ç§¯è€Œéä¸¢å¼ƒ

3. **ç»“æ„åŒ–éœ€æ±‚ç®¡ç†**: ä¸‰å¼ è¡¨ï¼ˆphrases/demands/tokensï¼‰æ¶æ„æ¸…æ™°
   - ä»"è¯"åˆ°"éœ€æ±‚"åˆ°"æ„æˆè¦ç´ "çš„å±‚æ¬¡åˆ†æ˜
   - å¯å¤ç”¨ã€å¯è¿­ä»£

4. **äººæœºåä½œè¾¹ç•Œ**: æ˜ç¡®äº†å“ªäº›ç¯èŠ‚éœ€è¦äººå·¥åˆ¤æ–­
   - å¤§ç»„ç­›é€‰ï¼ˆé˜¶æ®µ3ï¼‰
   - éœ€æ±‚ç†è§£ï¼ˆé˜¶æ®µ4ï¼‰
   - tokensåˆ†ç±»ï¼ˆé˜¶æ®µ5ï¼‰

**âš ï¸ æ½œåœ¨é—®é¢˜**:

1. **å¤§ç»„é€‰æ‹©çš„ä¸»è§‚æ€§**: é˜¶æ®µ3å®Œå…¨ä¾èµ–äººå·¥æ‰“åˆ†
   - å¦‚æœå¤§ç»„æœ‰100ä¸ªï¼Œé€ä¸ªè¯„ä¼°ä»ç„¶å¾ˆç´¯
   - å»ºè®®ï¼šå¢åŠ é¢„ç­›é€‰è§„åˆ™ï¼ˆæŒ‰é¢‘æ¬¡ã€å•†ä¸šä»·å€¼æŒ‡æ ‡ç­‰ï¼‰

2. **å°ç»„èšç±»çš„å‚æ•°æŒ‘æˆ˜**:
   - ä¸åŒå¤§ç»„å†…éƒ¨çš„æ•°æ®é‡å·®å¼‚å¤§ï¼ˆæœ‰çš„å‡ åæ¡ï¼Œæœ‰çš„å‡ ç™¾æ¡ï¼‰
   - ç»Ÿä¸€çš„`min_cluster_size`å¯èƒ½ä¸é€‚ç”¨
   - å»ºè®®ï¼šæ ¹æ®å¤§ç»„è§„æ¨¡åŠ¨æ€è°ƒæ•´å‚æ•°

3. **tokensè¯åº“çš„æ„å»ºæˆæœ¬**:
   - é˜¶æ®µ5éœ€è¦å¯¹å‡ åƒä¸ªtokené€ä¸ªåˆ†ç±»
   - å³ä½¿æœ‰AIè¾…åŠ©ï¼Œåˆæ¬¡æ„å»ºæˆæœ¬ä»ç„¶é«˜
   - å»ºè®®ï¼šåˆ†é˜¶æ®µæ„å»ºï¼Œå…ˆåšæ ¸å¿ƒtokensï¼ˆåŠ¨è¯+å¯¹è±¡ï¼‰

4. **å¢é‡æ›´æ–°çš„embeddingå¯¹é½**:
   - æ–°è¯æŒ‚åˆ°æ—§å¤§ç»„æ—¶ï¼Œembeddingæ¨¡å‹ç‰ˆæœ¬å¯èƒ½ä¸ä¸€è‡´
   - å»ºè®®ï¼šä¿å­˜åŸå§‹embeddingæˆ–ä½¿ç”¨å›ºå®šç‰ˆæœ¬æ¨¡å‹

### 1.2 ä¸å½“å‰å®ç°çš„å¯¹æ¯”

| æµç¨‹é˜¶æ®µ | æ–‡æ¡£è®¾è®¡ | å½“å‰å®ç° | å·®è· |
|---------|---------|---------|------|
| é˜¶æ®µ0-1 | æ•°æ®æ•´åˆæ¸…æ´— | âœ… å·²å®ç° | æ— å·®è· |
| é˜¶æ®µ2 | å¤§ç»„èšç±»ï¼ˆä¸–ç•Œåœ°å›¾ï¼‰| âœ… å·²å®ç°ï¼ˆstep_A3ï¼‰| å­—æ®µä¸¢å¤±é—®é¢˜ |
| é˜¶æ®µ3 | äººå·¥é€‰æ‹©å¤§ç»„ | âš ï¸ éƒ¨åˆ†å®ç° | ç¼ºå°‘é¢„ç­›é€‰è§„åˆ™ |
| é˜¶æ®µ4 | å°ç»„èšç±»+éœ€æ±‚å¡ç‰‡ | âš ï¸ éƒ¨åˆ†å®ç°ï¼ˆstep_B3ï¼‰| ç¼ºå°‘éœ€æ±‚å¡ç‰‡ç®¡ç† |
| é˜¶æ®µ5 | tokensè¯åº“ | âŒ æœªå®ç° | éœ€è¦å…¨æ–°å¼€å‘ |
| é˜¶æ®µ6 | ä¸‰å¼ è¡¨æ¶æ„ | âŒ æœªå®ç° | å½“å‰åªæœ‰CSVæ–‡ä»¶ |
| é˜¶æ®µ7 | å¢é‡æ›´æ–° | âŒ æœªå®ç° | éœ€è¦å…¨æ–°å¼€å‘ |
| é˜¶æ®µ8 | è½åœ°è¾“å‡º | âŒ æœªå®ç° | éœ€è¦æ ¹æ®ä¸šåŠ¡å®šåˆ¶ |

**ç»“è®º**:
- å½“å‰å®ç°è¦†ç›–äº†**é˜¶æ®µ0-2**ï¼Œä»¥åŠéƒ¨åˆ†**é˜¶æ®µ3-4**
- **é˜¶æ®µ5-7**æ˜¯æ ¸å¿ƒç¼ºå¤±éƒ¨åˆ†ï¼Œéœ€è¦é‡ç‚¹å¼€å‘
- **é˜¶æ®µ8**æ˜¯åº”ç”¨å±‚ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡åœºæ™¯å®šåˆ¶

---

## 2. è‡ªåŠ¨åŒ–è¾¹ç•Œåˆ’åˆ†

### 2.1 å®Œå…¨è‡ªåŠ¨åŒ–ï¼ˆPythonå®ç°ï¼‰

#### é˜¶æ®µ1: æ•°æ®æ•´åˆæ¸…æ´—
```
âœ… å…¨è‡ªåŠ¨
ç†ç”±: è§„åˆ™æ˜ç¡®ï¼Œæ— éœ€äººå·¥åˆ¤æ–­

è‡ªåŠ¨åŒ–å†…å®¹:
- å­—æ®µæ˜ å°„
- æ–‡æœ¬æ ‡å‡†åŒ–
- å»é‡åˆå¹¶
- å¼‚å¸¸å€¼è¿‡æ»¤

å·¥å…·: pandas + æ­£åˆ™è¡¨è¾¾å¼
```

#### é˜¶æ®µ2: å¤§ç»„èšç±»
```
âœ… å…¨è‡ªåŠ¨
ç†ç”±: ç®—æ³•é©±åŠ¨ï¼Œå¯é‡ç°

è‡ªåŠ¨åŒ–å†…å®¹:
- Embeddingç”Ÿæˆ
- HDBSCANèšç±»
- å¤§ç»„ç»Ÿè®¡
- HTMLæŠ¥å‘Šç”Ÿæˆ

å·¥å…·: sentence-transformers + hdbscan
```

#### é˜¶æ®µ7.1-7.2: å¢é‡å¯¼å…¥ä¸åˆ†é…
```
âœ… å…¨è‡ªåŠ¨
ç†ç”±: è§„åˆ™æ˜ç¡®ï¼Œå¯æ‰¹é‡å¤„ç†

è‡ªåŠ¨åŒ–å†…å®¹:
- æ–°æ—§æ•°æ®å»é‡
- first_seen_roundæ ‡è®°
- æ–°è¯å‘é‡ç”Ÿæˆ
- æ–°è¯åˆ†é…åˆ°å¤§ç»„

å·¥å…·: pandas + numpy + KNNç®—æ³•
```

### 2.2 åŠè‡ªåŠ¨åŒ–ï¼ˆAIè¾…åŠ©+äººå·¥å®¡æ ¸ï¼‰

#### é˜¶æ®µ3: å¤§ç»„ç­›é€‰
```
ğŸ¤– AIé¢„ç­›é€‰ + ğŸ‘¤ äººå·¥å†³ç­–

AIè¾…åŠ©éƒ¨åˆ†ï¼ˆå¯è‡ªåŠ¨ï¼‰:
1. æŒ‰è§„åˆ™æ’åº:
   - total_frequencyï¼ˆçƒ­åº¦ï¼‰
   - cluster_sizeï¼ˆè§„æ¨¡ï¼‰
   - å•†ä¸šä»·å€¼æŒ‡æ ‡ï¼ˆCPCã€ç«äº‰åº¦ï¼‰
2. è‡ªåŠ¨ç¿»è¯‘example_phrases
3. ç”¨LLMç”Ÿæˆæ¯ä¸ªå¤§ç»„çš„"ä¸»é¢˜æ ‡ç­¾"
   - è¾“å…¥: example_phrases
   - è¾“å‡º: [å·¥å…·ç±»/æ•™ç¨‹ç±»/è¯„æµ‹ç±»/ä¿¡æ¯æŸ¥è¯¢ç±»...]

äººå·¥å†³ç­–éƒ¨åˆ†ï¼ˆå¿…é¡»ï¼‰:
- é˜…è¯»AIç”Ÿæˆçš„ä¸»é¢˜æ ‡ç­¾
- ç»“åˆè‡ªèº«å…´è¶£å’Œèƒ½åŠ›æ‰“åˆ†
- æœ€ç»ˆé€‰å‡º10-15ä¸ªç›®æ ‡å¤§ç»„

å·¥å…·:
- Pythonåšé¢„ç­›é€‰
- å¤§æ¨¡å‹APIï¼ˆGPT-4/Claudeï¼‰ç”Ÿæˆæ ‡ç­¾
- äº¤äº’å¼ç•Œé¢ï¼ˆStreamlitæˆ–Jupyterï¼‰ä¾›äººå·¥é€‰æ‹©
```

#### é˜¶æ®µ4: å°ç»„èšç±»+éœ€æ±‚ç†è§£
```
ğŸ¤– AIç”Ÿæˆåˆç¨¿ + ğŸ‘¤ äººå·¥ç²¾ä¿®

AIè¾…åŠ©éƒ¨åˆ†ï¼ˆå¯è‡ªåŠ¨ï¼‰:
1. å°ç»„èšç±»ï¼ˆç®—æ³•ï¼‰
2. ä¸ºæ¯ä¸ªå°ç»„ç”Ÿæˆ"éœ€æ±‚å¡ç‰‡åˆç¨¿":
   - è¾“å…¥: å°ç»„çš„æ‰€æœ‰çŸ­è¯­
   - Prompt: "è¿™äº›æœç´¢è¯åæ˜ äº†ä»€ä¹ˆéœ€æ±‚ï¼Ÿç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œå¹¶æè¿°ç”¨æˆ·åœºæ™¯"
   - è¾“å‡º: {title, description}
3. è‡ªåŠ¨è¯†åˆ«é‡å¤éœ€æ±‚:
   - ç”¨embeddingæ¯”è¾ƒä¸åŒå°ç»„çš„éœ€æ±‚æè¿°
   - ç›¸ä¼¼åº¦>0.85çš„æ ‡è®°ä¸º"å¯èƒ½é‡å¤"

äººå·¥ç²¾ä¿®éƒ¨åˆ†ï¼ˆå¿…é¡»ï¼‰:
- é˜…è¯»AIç”Ÿæˆçš„éœ€æ±‚å¡ç‰‡
- åˆ¤æ–­æ˜¯å¦çœŸå®æœ‰æ•ˆï¼ˆä¸æ˜¯AIå¹»è§‰ï¼‰
- åˆå¹¶é‡å¤éœ€æ±‚
- è¡¥å……ç”¨æˆ·åœºæ™¯ç»†èŠ‚
- è¯„ä¼°å•†ä¸šä»·å€¼
- å†³å®š: ä¿ç•™/åˆå¹¶/ä¸¢å¼ƒ

å·¥å…·:
- Python + HDBSCANåšå°ç»„èšç±»
- LLM APIç”Ÿæˆéœ€æ±‚å¡ç‰‡
- Webç•Œé¢ä¾›äººå·¥å®¡æ ¸ç¼–è¾‘
```

#### é˜¶æ®µ5: tokensè¯åº“æ„å»º
```
ğŸ¤– AIç²—åˆ†ç±» + ğŸ‘¤ äººå·¥ç¡®è®¤

AIè¾…åŠ©éƒ¨åˆ†ï¼ˆå¯è‡ªåŠ¨ï¼‰:
1. Pythonç²—æ‹†è¯ï¼ˆåˆ†è¯ã€å»åœç”¨è¯ï¼‰
2. ç»Ÿè®¡é¢‘æ¬¡ã€æ’åº
3. ç”¨LLMç»™tokenæ‰“æ ‡ç­¾:
   - Prompt: "è¿™ä¸ªè¯æ˜¯åŠ¨è¯/åè¯/å½¢å®¹è¯ï¼Ÿå±äºæ„å›¾/å¯¹è±¡/å±æ€§/æ¡ä»¶ï¼Ÿ"
   - æ‰¹é‡å¤„ç†ï¼ˆæ¯æ¬¡50ä¸ªtokenï¼‰
4. è‡ªåŠ¨è¿‡æ»¤ä½ä»·å€¼è¯:
   - é¢‘æ¬¡<5çš„
   - çº¯æ•°å­—ã€çº¯ç¬¦å·
   - è¿‡äºé€šç”¨çš„è¯ï¼ˆthing/stuffï¼‰

äººå·¥ç¡®è®¤éƒ¨åˆ†ï¼ˆå»ºè®®ï¼‰:
- æŠ½æ ·æ£€æŸ¥AIåˆ†ç±»çš„å‡†ç¡®ç‡ï¼ˆæ¯è½®æŠ½10%ï¼‰
- çº æ­£æ˜æ˜¾é”™è¯¯
- è¡¥å……ç‰¹æ®Šé¢†åŸŸè¯æ±‡

å·¥å…·:
- Python + spaCyåšåˆ†è¯
- LLM APIæ‰¹é‡åˆ†ç±»
- äººå·¥å®¡æ ¸ç•Œé¢ï¼ˆå¯é€‰ï¼‰
```

### 2.3 çº¯äººå·¥å†³ç­–ï¼ˆä¸é€‚åˆè‡ªåŠ¨åŒ–ï¼‰

#### é˜¶æ®µ8: è½åœ°è¾“å‡º
```
ğŸ‘¤ çº¯äººå·¥å†³ç­–

ç†ç”±:
- æ¶‰åŠå•†ä¸šç­–ç•¥ã€èµ„æºåˆ†é…
- éœ€è¦å¯¹å¸‚åœºã€ç«äº‰ã€è‡ªèº«èƒ½åŠ›çš„ç»¼åˆåˆ¤æ–­
- ç®—æ³•æ— æ³•æ›¿ä»£

äººå·¥å†³ç­–å†…å®¹:
- å“ªäº›éœ€æ±‚å€¼å¾—åšæˆäº§å“ï¼Ÿ
- ä¼˜å…ˆçº§å¦‚ä½•æ’åºï¼Ÿ
- ç”¨ä»€ä¹ˆæ–¹å¼è½åœ°ï¼ˆå·¥å…·/å†…å®¹/æœåŠ¡ï¼‰ï¼Ÿ
- å¦‚ä½•å®šä»·ï¼Ÿå¦‚ä½•æ¨å¹¿ï¼Ÿ

ç³»ç»Ÿæ”¯æŒ:
- æä¾›éœ€æ±‚å¡ç‰‡çš„æ‰€æœ‰æ•°æ®ï¼ˆé¢‘æ¬¡ã€CPCã€ç«äº‰åº¦ï¼‰
- æä¾›ç›¸å…³çŸ­è¯­ä½œä¸ºå‚è€ƒ
- æä¾›tokensç»„åˆå»ºè®®
```

### 2.4 è‡ªåŠ¨åŒ–è¾¹ç•Œæ€»ç»“è¡¨

| é˜¶æ®µ | è‡ªåŠ¨åŒ–ç¨‹åº¦ | å·¥å…·ç»„åˆ | äººå·¥å·¥ä½œé‡ |
|------|-----------|---------|-----------|
| 0-1 æ•°æ®æ•´åˆ | 100% | Python | 0% |
| 2 å¤§ç»„èšç±» | 100% | Python | 0% |
| 3 å¤§ç»„ç­›é€‰ | 60% | Python+LLM â†’ äººå·¥ | 40% |
| 4 å°ç»„+éœ€æ±‚ | 70% | Python+LLM â†’ äººå·¥ | 30% |
| 5 tokensåº“ | 80% | Python+LLM â†’ äººå·¥ | 20% |
| 6 å»ºè¡¨ | 100% | Python/SQL | 0% |
| 7 å¢é‡æ›´æ–° | 95% | Python | 5% |
| 8 è½åœ°è¾“å‡º | 20% | ç³»ç»Ÿæä¾›æ•°æ® â†’ äººå·¥ | 80% |

**å…³é”®åŸåˆ™**:
- **æ‰¹é‡å¤„ç† â†’ è‡ªåŠ¨åŒ–**ï¼ˆå¦‚embeddingã€èšç±»ï¼‰
- **è§„åˆ™æ˜ç¡® â†’ è‡ªåŠ¨åŒ–**ï¼ˆå¦‚å»é‡ã€åˆ†é…ï¼‰
- **ä¸»è§‚åˆ¤æ–­ â†’ äººå·¥**ï¼ˆå¦‚å•†ä¸šä»·å€¼è¯„ä¼°ï¼‰
- **åˆ›æ„è®¾è®¡ â†’ äººå·¥**ï¼ˆå¦‚äº§å“è®¾è®¡ã€å†…å®¹åˆ›ä½œï¼‰
- **é‡å¤æ€§é«˜ â†’ AIè¾…åŠ©**ï¼ˆå¦‚éœ€æ±‚å¡ç‰‡åˆç¨¿ã€tokenåˆ†ç±»ï¼‰

---

## 3. æ•°æ®è¡¨è®¾è®¡æ–¹æ¡ˆ

### 3.1 æ ¸å¿ƒä¸‰å¼ è¡¨

#### è¡¨1: phrasesï¼ˆçŸ­è¯­æ¡£æ¡ˆåº“ï¼‰

```sql
CREATE TABLE phrases (
    -- ä¸»é”®
    phrase_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    phrase VARCHAR(255) UNIQUE NOT NULL,  -- çŸ­è¯­æ–‡æœ¬ï¼ˆå”¯ä¸€çº¦æŸï¼‰

    -- æ¥æºä¿¡æ¯
    seed_word VARCHAR(100),
    source_type ENUM('semrush', 'dropdown', 'related_search', 'manual'),
    first_seen_round INT NOT NULL,        -- ç¬¬å‡ è½®å‡ºç°

    -- ç»Ÿè®¡æ•°æ®
    frequency BIGINT DEFAULT 1,           -- ç´¯è®¡é¢‘æ¬¡
    volume BIGINT DEFAULT 0,              -- æœç´¢é‡
    cpc DECIMAL(10,2),                    -- CPCæˆæœ¬
    keyword_difficulty INT,               -- å…³é”®è¯éš¾åº¦

    -- èšç±»åˆ†é…
    cluster_id_A INT,                     -- å¤§ç»„ID
    cluster_id_B INT,                     -- å°ç»„IDï¼ˆå¯ä¸ºç©ºï¼‰

    -- éœ€æ±‚å…³è”
    mapped_demand_id INT,                 -- å…³è”çš„éœ€æ±‚å¡ç‰‡ID

    -- å¤„ç†çŠ¶æ€
    processed_status ENUM('unseen', 'reviewed', 'assigned', 'archived') DEFAULT 'unseen',

    -- ç‰¹å¾å­—æ®µï¼ˆä»A3.2ä¿ç•™ï¼‰
    word_count INT,
    phrase_length INT,
    query_type ENUM('question', 'best_list', 'tutorial', 'normal'),
    has_question_word BOOLEAN,

    -- å…ƒæ•°æ®
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    -- ç´¢å¼•
    INDEX idx_cluster_A (cluster_id_A),
    INDEX idx_cluster_B (cluster_id_B),
    INDEX idx_demand (mapped_demand_id),
    INDEX idx_status (processed_status),
    INDEX idx_round (first_seen_round)
);
```

**è®¾è®¡è¦ç‚¹**:
- `phrase`åŠ å”¯ä¸€çº¦æŸï¼Œä¿è¯å»é‡
- `processed_status`è¿½è¸ªå¤„ç†è¿›åº¦
- `first_seen_round`æ”¯æŒå¢é‡åˆ†æ
- ä¿ç•™åŸå§‹ç»Ÿè®¡æ•°æ®ï¼ˆvolume, cpcç­‰ï¼‰
- å¤šä¸ªç´¢å¼•æ”¯æŒå¸¸è§æŸ¥è¯¢

**å¸¸è§æŸ¥è¯¢**:
```sql
-- æŸ¥è¯¢æœªå¤„ç†çš„æ–°è¯
SELECT * FROM phrases
WHERE processed_status = 'unseen'
  AND first_seen_round = 3;

-- æŸ¥è¯¢æŸä¸ªå¤§ç»„çš„æ‰€æœ‰çŸ­è¯­
SELECT * FROM phrases
WHERE cluster_id_A = 5
  AND mapped_demand_id IS NULL;

-- ç»Ÿè®¡å„å¤§ç»„çš„è§„æ¨¡
SELECT cluster_id_A, COUNT(*) as size, SUM(frequency) as total_freq
FROM phrases
WHERE cluster_id_A >= 0
GROUP BY cluster_id_A
ORDER BY total_freq DESC;
```

#### è¡¨2: demandsï¼ˆéœ€æ±‚å¡ç‰‡åº“ï¼‰

```sql
CREATE TABLE demands (
    -- ä¸»é”®
    demand_id INT PRIMARY KEY AUTO_INCREMENT,

    -- éœ€æ±‚æè¿°
    title VARCHAR(255) NOT NULL,          -- ä¸€å¥è¯æ¦‚æ‹¬
    description TEXT,                     -- è¯¦ç»†æè¿°
    user_scenario TEXT,                   -- ç”¨æˆ·åœºæ™¯

    -- åˆ†ç±»æ ‡ç­¾
    demand_type ENUM('tool', 'content', 'service', 'education', 'other'),
    category VARCHAR(100),                -- ç»†åˆ†ç±»åˆ«
    tags JSON,                            -- å¤šæ ‡ç­¾ ["compress", "pdf", "online"]

    -- å…³è”ä¿¡æ¯
    source_cluster_A INT,                 -- æ¥è‡ªå“ªä¸ªå¤§ç»„
    source_cluster_B INT,                 -- æ¥è‡ªå“ªä¸ªå°ç»„
    related_phrases_count INT DEFAULT 0,  -- å…³è”çŸ­è¯­æ•°
    main_tokens JSON,                     -- æ ¸å¿ƒtokens ["compress", "pdf", "free"]

    -- å•†ä¸šè¯„ä¼°
    business_value ENUM('high', 'medium', 'low', 'unknown') DEFAULT 'unknown',
    competition_level ENUM('high', 'medium', 'low', 'unknown') DEFAULT 'unknown',
    monetization_potential DECIMAL(3,2),  -- 0-1è¯„åˆ†

    -- çŠ¶æ€è¿½è¸ª
    status ENUM('idea', 'validated', 'in_progress', 'launched', 'profitable', 'archived') DEFAULT 'idea',
    priority INT DEFAULT 0,               -- ä¼˜å…ˆçº§æ’åº

    -- è½åœ°ä¿¡æ¯
    landing_url VARCHAR(255),             -- è½åœ°é¡µURL
    product_id VARCHAR(100),              -- äº§å“ID
    revenue DECIMAL(10,2),                -- ç´¯è®¡æ”¶å…¥

    -- å…ƒæ•°æ®
    created_by VARCHAR(100),              -- åˆ›å»ºäººï¼ˆAIæˆ–äººå·¥ï¼‰
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    -- ç´¢å¼•
    INDEX idx_status (status),
    INDEX idx_type (demand_type),
    INDEX idx_value (business_value),
    INDEX idx_cluster_A (source_cluster_A)
);
```

**è®¾è®¡è¦ç‚¹**:
- `title`å’Œ`description`æ”¯æŒAIç”Ÿæˆ+äººå·¥ç¼–è¾‘
- `tags`å’Œ`main_tokens`ç”¨JSONå­˜å‚¨ï¼ˆçµæ´»æ€§ï¼‰
- `status`è¿½è¸ªä»æƒ³æ³•åˆ°ç›ˆåˆ©çš„å…¨ç”Ÿå‘½å‘¨æœŸ
- `business_value`ç­‰å­—æ®µæ”¯æŒä¼˜å…ˆçº§æ’åº
- `landing_url`å’Œ`revenue`è¿æ¥ä¸šåŠ¡ç»“æœ

**å¸¸è§æŸ¥è¯¢**:
```sql
-- æŸ¥è¯¢é«˜ä»·å€¼å¾…éªŒè¯çš„éœ€æ±‚
SELECT * FROM demands
WHERE business_value = 'high'
  AND status = 'idea'
ORDER BY related_phrases_count DESC;

-- ç»Ÿè®¡å„ç±»å‹éœ€æ±‚çš„åˆ†å¸ƒ
SELECT demand_type, COUNT(*) as count, AVG(related_phrases_count) as avg_phrases
FROM demands
GROUP BY demand_type;

-- æŸ¥è¯¢å·²ç›ˆåˆ©çš„éœ€æ±‚
SELECT * FROM demands
WHERE status = 'profitable'
ORDER BY revenue DESC;
```

#### è¡¨3: tokensï¼ˆéœ€æ±‚æ¡†æ¶è¯åº“ï¼‰

```sql
CREATE TABLE tokens (
    -- ä¸»é”®
    token_id INT PRIMARY KEY AUTO_INCREMENT,
    token_text VARCHAR(100) UNIQUE NOT NULL,  -- tokenæ–‡æœ¬ï¼ˆå”¯ä¸€ï¼‰

    -- åˆ†ç±»æ ‡ç­¾
    token_type ENUM('intent', 'action', 'object', 'attribute', 'condition', 'other') NOT NULL,
    sub_category VARCHAR(100),            -- å­ç±»åˆ«ï¼ˆå¦‚actionâ†’compress/convert/downloadï¼‰

    -- ç»Ÿè®¡ä¿¡æ¯
    in_phrase_count INT DEFAULT 0,       -- å‡ºç°åœ¨å¤šå°‘ä¸ªphraseä¸­
    in_demand_count INT DEFAULT 0,       -- å…³è”åˆ°å¤šå°‘ä¸ªdemand
    total_frequency BIGINT DEFAULT 0,    -- ç´¯è®¡é¢‘æ¬¡

    -- æ¥æºè¿½è¸ª
    first_seen_round INT NOT NULL,       -- ç¬¬å‡ è½®å‡ºç°
    source ENUM('auto', 'manual', 'ai') DEFAULT 'auto',  -- å‘ç°æ–¹å¼

    -- å…³è”ä¿¡æ¯
    synonyms JSON,                       -- åŒä¹‰è¯ ["compress", "compression", "zip"]
    related_tokens JSON,                 -- ç›¸å…³tokens ["compress" â†’ ["pdf", "image", "video"]]

    -- å•†ä¸šå±æ€§
    commercial_value DECIMAL(3,2),       -- å•†ä¸šä»·å€¼è¯„åˆ† 0-1
    avg_cpc DECIMAL(10,2),               -- å¹³å‡CPC
    avg_competition DECIMAL(3,2),        -- å¹³å‡ç«äº‰åº¦

    -- å…ƒæ•°æ®
    verified BOOLEAN DEFAULT FALSE,      -- æ˜¯å¦ç»è¿‡äººå·¥éªŒè¯
    notes TEXT,                          -- å¤‡æ³¨
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    -- ç´¢å¼•
    INDEX idx_type (token_type),
    INDEX idx_frequency (total_frequency DESC),
    INDEX idx_round (first_seen_round),
    INDEX idx_verified (verified)
);
```

**è®¾è®¡è¦ç‚¹**:
- `token_text`å”¯ä¸€çº¦æŸï¼Œé¿å…é‡å¤
- `token_type`æ ¸å¿ƒåˆ†ç±»ï¼ˆæ„å›¾/åŠ¨ä½œ/å¯¹è±¡ç­‰ï¼‰
- `synonyms`å’Œ`related_tokens`ç”¨JSONå­˜å‚¨å…³è”å…³ç³»
- `commercial_value`ç­‰æŒ‡æ ‡æ”¯æŒä¼˜å…ˆçº§æ’åº
- `verified`æ ‡è®°æ˜¯å¦ç»è¿‡äººå·¥ç¡®è®¤

**å¸¸è§æŸ¥è¯¢**:
```sql
-- æŸ¥è¯¢é«˜ä»·å€¼åŠ¨ä½œè¯
SELECT * FROM tokens
WHERE token_type = 'action'
  AND commercial_value > 0.7
ORDER BY in_phrase_count DESC;

-- æŸ¥è¯¢æŸä¸ªtokençš„å…³è”ä¿¡æ¯
SELECT * FROM tokens
WHERE token_text = 'compress';

-- ç»Ÿè®¡å„ç±»å‹tokençš„æ•°é‡
SELECT token_type, COUNT(*) as count, AVG(in_phrase_count) as avg_usage
FROM tokens
GROUP BY token_type;

-- æŸ¥è¯¢æœªéªŒè¯çš„é«˜é¢‘token
SELECT * FROM tokens
WHERE verified = FALSE
  AND in_phrase_count > 100
ORDER BY total_frequency DESC;
```

### 3.2 è¾…åŠ©è¡¨

#### è¡¨4: cluster_metaï¼ˆèšç±»å…ƒæ•°æ®ï¼‰

```sql
CREATE TABLE cluster_meta (
    cluster_id INT PRIMARY KEY,
    cluster_level ENUM('A', 'B') NOT NULL,  -- å¤§ç»„æˆ–å°ç»„
    parent_cluster_id INT,                  -- å°ç»„çš„çˆ¶å¤§ç»„ID

    -- ç»Ÿè®¡ä¿¡æ¯
    size INT,
    total_frequency BIGINT,
    avg_frequency DECIMAL(10,2),

    -- ä»£è¡¨ä¿¡æ¯
    example_phrases TEXT,                   -- ä»£è¡¨çŸ­è¯­ï¼ˆåˆ†å·åˆ†éš”ï¼‰
    main_theme VARCHAR(255),                -- AIç”Ÿæˆçš„ä¸»é¢˜æ ‡ç­¾
    seed_words_in_cluster TEXT,             -- ç§å­è¯åˆ—è¡¨

    -- è´¨é‡æŒ‡æ ‡
    cohesion_score DECIMAL(3,2),            -- å†…èšåº¦ï¼ˆ0-1ï¼‰
    noise_ratio DECIMAL(3,2),               -- å™ªéŸ³æ¯”ä¾‹

    -- é€‰æ‹©çŠ¶æ€
    is_selected BOOLEAN DEFAULT FALSE,      -- æ˜¯å¦è¢«é€‰ä¸­è¿›å…¥ä¸‹ä¸€æ­¥
    selection_score INT,                    -- äººå·¥æ‰“åˆ†ï¼ˆ1-5æ˜Ÿï¼‰
    selection_reason TEXT,                  -- é€‰æ‹©åŸå› 

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    INDEX idx_level (cluster_level),
    INDEX idx_parent (parent_cluster_id),
    INDEX idx_selected (is_selected)
);
```

#### è¡¨5: processing_roundsï¼ˆè½®æ¬¡è¿½è¸ªï¼‰

```sql
CREATE TABLE processing_rounds (
    round_id INT PRIMARY KEY AUTO_INCREMENT,
    round_name VARCHAR(100),                -- å¦‚"Round 1: Initial 50K"

    -- æ•°æ®ç»Ÿè®¡
    new_phrases_count INT,                  -- æ–°å¢çŸ­è¯­æ•°
    total_phrases_count INT,                -- ç´¯è®¡æ€»æ•°
    new_demands_count INT,                  -- æ–°å¢éœ€æ±‚æ•°
    new_tokens_count INT,                   -- æ–°å¢tokensæ•°

    -- å¤„ç†ç»“æœ
    clusters_A_count INT,                   -- å¤§ç»„æ•°é‡
    clusters_B_count INT,                   -- å°ç»„æ•°é‡
    noise_ratio DECIMAL(3,2),               -- å™ªéŸ³æ¯”ä¾‹

    -- æ—¶é—´ä¿¡æ¯
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    duration_minutes INT,

    -- å¤‡æ³¨
    notes TEXT,
    config_snapshot JSON,                   -- ä¿å­˜è¯¥è½®ä½¿ç”¨çš„é…ç½®

    INDEX idx_round (round_id)
);
```

### 3.3 æ•°æ®è¡¨å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ•°æ®è¡¨å…³ç³»å›¾                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ processing_roundsâ”‚
       â”‚  round_id (PK)  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”‚ first_seen_round
                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚     phrases      â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  phrase_id (PK)  â”‚          â”‚
       â”‚  phrase (UNIQUE) â”‚          â”‚
       â”‚  cluster_id_A    â”œâ”€â”€â”       â”‚
       â”‚  cluster_id_B    â”œâ”€â”â”‚       â”‚ example_phrases
       â”‚  mapped_demand_idâ”œâ”â”‚â”‚       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚â”‚       â”‚
                           â”‚â”‚â”‚       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚       â”‚
         â”‚                  â”‚â”‚       â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚       â”‚
         â”‚  â”‚                â”‚       â”‚
         â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
         â”‚  â”‚  â”‚                     â”‚
         â–¼  â–¼  â–¼                     â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
       â”‚   cluster_meta   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚  cluster_id (PK) â”‚
       â”‚  cluster_level   â”‚
       â”‚  parent_cluster  â”‚
       â”‚  is_selected     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”‚ source_cluster_A/B
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     demands     â”‚
         â”‚  demand_id (PK) â”‚
         â”‚  title          â”‚
         â”‚  main_tokens    â”‚â—„â”€â”
         â”‚  status         â”‚  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                              â”‚ token_textåˆ—è¡¨
                              â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     tokens      â”‚
                       â”‚  token_id (PK)  â”‚
                       â”‚  token_text (U) â”‚
                       â”‚  token_type     â”‚
                       â”‚  related_tokens â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³ç³»è¯´æ˜**:
- `phrases.cluster_id_A/B` â†’ `cluster_meta.cluster_id` (å¤šå¯¹ä¸€)
- `phrases.mapped_demand_id` â†’ `demands.demand_id` (å¤šå¯¹ä¸€)
- `demands.main_tokens` â†’ `tokens.token_text` (JSONæ•°ç»„å…³è”)
- `phrases.first_seen_round` â†’ `processing_rounds.round_id` (å¤šå¯¹ä¸€)

---

## 4. ä»£ç æ¶æ„ä¼˜åŒ–

### 4.1 å½“å‰æ¶æ„ vs ç›®æ ‡æ¶æ„

**å½“å‰æ¶æ„**ï¼ˆåŸºäºæ–‡ä»¶çš„æµæ°´çº¿ï¼‰:
```
scripts/
  core/
    step_A2_merge_csv.py      â†’ CSVè¾“å‡º
    step_A3_clustering.py     â†’ CSVè¾“å‡º
    step_B3_cluster_stageB.py â†’ CSVè¾“å‡º
  tools/
    cluster_stats.py
    generate_html_viewer.py
```

**é—®é¢˜**:
- æ¯ä¸ªè„šæœ¬ç‹¬ç«‹è¿è¡Œï¼Œç¼ºå°‘çŠ¶æ€ç®¡ç†
- æ•°æ®å­˜å‚¨åœ¨CSVï¼Œä¸æ”¯æŒå¤æ‚æŸ¥è¯¢
- æ²¡æœ‰å¢é‡æ›´æ–°æœºåˆ¶
- ç¼ºå°‘AIé›†æˆæ¥å£

**ç›®æ ‡æ¶æ„**ï¼ˆåŸºäºæ•°æ®åº“çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼‰:
```
è¯æ ¹èšç±»éœ€æ±‚æŒ–æ˜/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py          # æ•°æ®åº“è¿æ¥é…ç½®
â”‚   â”œâ”€â”€ clustering.py        # èšç±»å‚æ•°é…ç½®
â”‚   â”œâ”€â”€ llm.py               # LLM APIé…ç½®
â”‚   â””â”€â”€ paths.py             # è·¯å¾„é…ç½®
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py          # SQLAlchemyæ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ Phrase
â”‚   â”‚   â”œâ”€â”€ Demand
â”‚   â”‚   â”œâ”€â”€ Token
â”‚   â”‚   â””â”€â”€ ClusterMeta
â”‚   â””â”€â”€ schemas.py           # Pydanticæ¨¡å‹ï¼ˆæ•°æ®éªŒè¯ï¼‰
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_integration.py  # é˜¶æ®µ1ï¼šæ•°æ®æ•´åˆ
â”‚   â”œâ”€â”€ clustering_engine.py # é˜¶æ®µ2+4ï¼šèšç±»å¼•æ“
â”‚   â”‚   â”œâ”€â”€ ClusteringEngine
â”‚   â”‚   â”‚   â”œâ”€â”€ fit_large_clusters()     # å¤§ç»„èšç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ fit_small_clusters()     # å°ç»„èšç±»
â”‚   â”‚   â”‚   â””â”€â”€ assign_new_phrases()     # å¢é‡åˆ†é…
â”‚   â”œâ”€â”€ embedding_service.py # Embeddingç®¡ç†
â”‚   â”‚   â”œâ”€â”€ EmbeddingService
â”‚   â”‚   â”‚   â”œâ”€â”€ generate()
â”‚   â”‚   â”‚   â”œâ”€â”€ cache()
â”‚   â”‚   â”‚   â””â”€â”€ load_cache()
â”‚   â””â”€â”€ incremental_updater.py # é˜¶æ®µ7ï¼šå¢é‡æ›´æ–°
â”‚
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_client.py        # LLM APIå°è£…
â”‚   â”‚   â”œâ”€â”€ LLMClient
â”‚   â”‚   â”‚   â”œâ”€â”€ generate_cluster_theme()
â”‚   â”‚   â”‚   â”œâ”€â”€ generate_demand_card()
â”‚   â”‚   â”‚   â””â”€â”€ classify_token()
â”‚   â”œâ”€â”€ prompts.py           # Promptæ¨¡æ¿
â”‚   â””â”€â”€ batch_processor.py   # æ‰¹é‡å¤„ç†å·¥å…·
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ phrase_service.py    # phrasesè¡¨CRUD
â”‚   â”œâ”€â”€ demand_service.py    # demandsè¡¨CRUD
â”‚   â”œâ”€â”€ token_service.py     # tokensè¡¨CRUD
â”‚   â””â”€â”€ cluster_service.py   # cluster_metaè¡¨CRUD
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ initial_pipeline.py  # ç¬¬ä¸€æ¬¡å…¨é‡æµç¨‹
â”‚   â”‚   â””â”€â”€ run_initial()
â”‚   â”‚       â”œâ”€â”€ integrate_data()
â”‚   â”‚       â”œâ”€â”€ cluster_large()
â”‚   â”‚       â”œâ”€â”€ select_clusters()   # äººå·¥äº¤äº’
â”‚   â”‚       â”œâ”€â”€ cluster_small()
â”‚   â”‚       â”œâ”€â”€ generate_demands()  # AI+äººå·¥
â”‚   â”‚       â””â”€â”€ extract_tokens()
â”‚   â””â”€â”€ incremental_pipeline.py # å¢é‡æµç¨‹
â”‚       â””â”€â”€ run_incremental()
â”‚           â”œâ”€â”€ import_new_data()
â”‚           â”œâ”€â”€ assign_to_clusters()
â”‚           â””â”€â”€ update_demands()
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cluster_selector.py  # é˜¶æ®µ3ï¼šå¤§ç»„é€‰æ‹©ç•Œé¢
â”‚   â”œâ”€â”€ demand_editor.py     # é˜¶æ®µ4ï¼šéœ€æ±‚ç¼–è¾‘ç•Œé¢
â”‚   â””â”€â”€ token_reviewer.py    # é˜¶æ®µ5ï¼štokenå®¡æ ¸ç•Œé¢
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_processing.py   # æ–‡æœ¬å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ metrics.py           # è¯„ä¼°æŒ‡æ ‡è®¡ç®—
â”‚   â””â”€â”€ export.py            # æ•°æ®å¯¼å‡ºå·¥å…·
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_database.py     # åˆå§‹åŒ–æ•°æ®åº“
â”‚   â”œâ”€â”€ run_pipeline.py      # è¿è¡Œæµç¨‹
â”‚   â”œâ”€â”€ export_results.py    # å¯¼å‡ºç»“æœ
â”‚   â””â”€â”€ maintenance.py       # ç»´æŠ¤è„šæœ¬
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_clustering.py
â”‚   â”œâ”€â”€ test_services.py
â”‚   â””â”€â”€ test_pipelines.py
â”‚
â”œâ”€â”€ migrations/              # æ•°æ®åº“è¿ç§»è„šæœ¬
â”‚   â””â”€â”€ alembic/
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

### 4.2 æ ¸å¿ƒæ¨¡å—è®¾è®¡

#### 4.2.1 ClusteringEngineï¼ˆèšç±»å¼•æ“ï¼‰

```python
# core/clustering_engine.py

from typing import List, Tuple, Optional
import numpy as np
import hdbscan
from sklearn.neighbors import NearestNeighbors
from core.embedding_service import EmbeddingService
from models.database import Phrase, ClusterMeta
from config.clustering import CLUSTERING_CONFIG

class ClusteringEngine:
    """èšç±»å¼•æ“ï¼šæ”¯æŒå¤§ç»„ã€å°ç»„ã€å¢é‡åˆ†é…"""

    def __init__(self, embedding_service: EmbeddingService):
        self.embedding_service = embedding_service
        self.large_cluster_params = CLUSTERING_CONFIG['large']
        self.small_cluster_params = CLUSTERING_CONFIG['small']

    def fit_large_clusters(
        self,
        phrases: List[Phrase],
        save_embeddings: bool = True
    ) -> Tuple[np.ndarray, ClusterMeta]:
        """
        å¤§ç»„èšç±»ï¼ˆé˜¶æ®µ2ï¼‰

        Args:
            phrases: çŸ­è¯­åˆ—è¡¨
            save_embeddings: æ˜¯å¦ä¿å­˜embeddingsåˆ°ç¼“å­˜

        Returns:
            labels: ç°‡æ ‡ç­¾æ•°ç»„
            cluster_meta: èšç±»å…ƒæ•°æ®
        """
        # 1. ç”Ÿæˆembeddings
        texts = [p.phrase for p in phrases]
        embeddings = self.embedding_service.generate(texts)

        if save_embeddings:
            self.embedding_service.cache(phrases, embeddings)

        # 2. HDBSCANèšç±»
        min_cluster_size = self._calculate_min_cluster_size(
            len(phrases),
            self.large_cluster_params
        )

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=self.large_cluster_params['min_samples'],
            metric='euclidean',
            cluster_selection_method='eom'
        )

        labels = clusterer.fit_predict(embeddings)

        # 3. è®¡ç®—å…ƒæ•°æ®
        cluster_meta = self._compute_cluster_meta(
            phrases, labels, embeddings, cluster_level='A'
        )

        return labels, cluster_meta

    def fit_small_clusters(
        self,
        phrases: List[Phrase],
        parent_cluster_id: int
    ) -> Tuple[np.ndarray, ClusterMeta]:
        """
        å°ç»„èšç±»ï¼ˆé˜¶æ®µ4ï¼‰

        Args:
            phrases: å±äºåŒä¸€å¤§ç»„çš„çŸ­è¯­åˆ—è¡¨
            parent_cluster_id: çˆ¶å¤§ç»„ID

        Returns:
            labels: å°ç»„æ ‡ç­¾æ•°ç»„
            cluster_meta: èšç±»å…ƒæ•°æ®
        """
        # åŠ¨æ€è°ƒæ•´å‚æ•°ï¼ˆå°ç»„æ•°æ®é‡å°‘ï¼‰
        min_cluster_size = max(
            5,
            int(len(phrases) * 0.05)  # è‡³å°‘5%
        )

        texts = [p.phrase for p in phrases]
        embeddings = self.embedding_service.generate(texts)

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=2,  # å°ç»„è¦æ±‚æ›´å®½æ¾
            metric='euclidean'
        )

        labels = clusterer.fit_predict(embeddings)

        cluster_meta = self._compute_cluster_meta(
            phrases, labels, embeddings,
            cluster_level='B',
            parent_cluster_id=parent_cluster_id
        )

        return labels, cluster_meta

    def assign_new_phrases(
        self,
        new_phrases: List[Phrase],
        reference_clusters: List[ClusterMeta]
    ) -> np.ndarray:
        """
        å¢é‡åˆ†é…ï¼šå°†æ–°çŸ­è¯­åˆ†é…åˆ°å·²æœ‰å¤§ç»„

        Args:
            new_phrases: æ–°çŸ­è¯­åˆ—è¡¨
            reference_clusters: ç°æœ‰çš„å¤§ç»„å…ƒæ•°æ®

        Returns:
            labels: åˆ†é…çš„å¤§ç»„IDæ•°ç»„
        """
        # 1. ç”Ÿæˆæ–°çŸ­è¯­çš„embeddings
        new_texts = [p.phrase for p in new_phrases]
        new_embeddings = self.embedding_service.generate(new_texts)

        # 2. åŠ è½½å·²æœ‰ç°‡çš„ä¸­å¿ƒç‚¹embeddings
        cluster_centers = self._load_cluster_centers(reference_clusters)

        # 3. ä½¿ç”¨KNNåˆ†é…
        nn = NearestNeighbors(n_neighbors=1, metric='euclidean')
        nn.fit(cluster_centers)

        distances, indices = nn.kneighbors(new_embeddings)

        # 4. è·ç¦»é˜ˆå€¼è¿‡æ»¤ï¼ˆè¿‡è¿œçš„æ ‡è®°ä¸ºå™ªéŸ³-1ï¼‰
        threshold = self.large_cluster_params.get('assignment_threshold', 0.5)
        labels = []
        for dist, idx in zip(distances, indices):
            if dist[0] < threshold:
                labels.append(reference_clusters[idx[0]].cluster_id)
            else:
                labels.append(-1)  # å™ªéŸ³

        return np.array(labels)

    def _calculate_min_cluster_size(self, n_samples: int, params: dict) -> int:
        """åŠ¨æ€è®¡ç®—min_cluster_size"""
        if params.get('use_dynamic'):
            import math
            return max(10, int(math.log10(n_samples) * 5))
        else:
            return params['min_cluster_size']

    def _compute_cluster_meta(
        self,
        phrases: List[Phrase],
        labels: np.ndarray,
        embeddings: np.ndarray,
        cluster_level: str,
        parent_cluster_id: Optional[int] = None
    ) -> List[ClusterMeta]:
        """è®¡ç®—èšç±»å…ƒæ•°æ®"""
        import pandas as pd

        df = pd.DataFrame({
            'phrase': [p.phrase for p in phrases],
            'frequency': [p.frequency for p in phrases],
            'cluster_id': labels,
            'embedding': list(embeddings)
        })

        meta_list = []
        for cluster_id in df['cluster_id'].unique():
            cluster_df = df[df['cluster_id'] == cluster_id]

            # ä»£è¡¨çŸ­è¯­ï¼ˆé¢‘æ¬¡æœ€é«˜çš„5æ¡ï¼‰
            top_phrases = cluster_df.nlargest(5, 'frequency')['phrase'].tolist()

            # èšç±»ä¸­å¿ƒ
            cluster_embeddings = np.array(cluster_df['embedding'].tolist())
            center = cluster_embeddings.mean(axis=0)

            # å†…èšåº¦ï¼ˆå¹³å‡åˆ°ä¸­å¿ƒçš„è·ç¦»ï¼‰
            distances = np.linalg.norm(cluster_embeddings - center, axis=1)
            cohesion = 1 - (distances.mean() / distances.max())  # å½’ä¸€åŒ–

            meta = ClusterMeta(
                cluster_id=cluster_id,
                cluster_level=cluster_level,
                parent_cluster_id=parent_cluster_id,
                size=len(cluster_df),
                total_frequency=cluster_df['frequency'].sum(),
                avg_frequency=cluster_df['frequency'].mean(),
                example_phrases='; '.join(top_phrases),
                cohesion_score=cohesion
            )
            meta_list.append(meta)

        return meta_list

    def _load_cluster_centers(self, clusters: List[ClusterMeta]) -> np.ndarray:
        """åŠ è½½ç°‡ä¸­å¿ƒç‚¹çš„embeddings"""
        # ä»ç¼“å­˜æˆ–æ•°æ®åº“åŠ è½½æ¯ä¸ªç°‡çš„ä¸­å¿ƒembedding
        # è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦ä»ç¼“å­˜åŠ è½½
        pass
```

#### 4.2.2 LLMClientï¼ˆAIå®¢æˆ·ç«¯ï¼‰

```python
# ai/llm_client.py

from typing import List, Dict, Optional
import openai
from anthropic import Anthropic
from config.llm import LLM_CONFIG
from ai.prompts import PROMPTS

class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼šå°è£…GPT/Claude APIè°ƒç”¨"""

    def __init__(self, provider: str = 'openai'):
        self.provider = provider
        self.config = LLM_CONFIG[provider]

        if provider == 'openai':
            openai.api_key = self.config['api_key']
            self.model = self.config['model']
        elif provider == 'anthropic':
            self.client = Anthropic(api_key=self.config['api_key'])
            self.model = self.config['model']

    def generate_cluster_theme(
        self,
        example_phrases: List[str],
        cluster_size: int,
        total_frequency: int
    ) -> Dict[str, str]:
        """
        ä¸ºå¤§ç»„ç”Ÿæˆä¸»é¢˜æ ‡ç­¾ï¼ˆé˜¶æ®µ3è¾…åŠ©ï¼‰

        Args:
            example_phrases: ä»£è¡¨çŸ­è¯­åˆ—è¡¨
            cluster_size: ç°‡å¤§å°
            total_frequency: æ€»é¢‘æ¬¡

        Returns:
            {
                'theme': 'ä¸»é¢˜æ ‡ç­¾',
                'category': 'ç±»åˆ«',
                'description': 'æè¿°'
            }
        """
        prompt = PROMPTS['cluster_theme'].format(
            phrases='\n'.join(f"- {p}" for p in example_phrases),
            size=cluster_size,
            frequency=total_frequency
        )

        response = self._call_api(prompt)

        # è§£æJSONå“åº”
        import json
        try:
            result = json.loads(response)
            return result
        except:
            return {
                'theme': response[:100],
                'category': 'unknown',
                'description': response
            }

    def generate_demand_card(
        self,
        phrases: List[str],
        cluster_theme: str
    ) -> Dict[str, str]:
        """
        ç”Ÿæˆéœ€æ±‚å¡ç‰‡åˆç¨¿ï¼ˆé˜¶æ®µ4è¾…åŠ©ï¼‰

        Args:
            phrases: å°ç»„å†…æ‰€æœ‰çŸ­è¯­
            cluster_theme: å¤§ç»„ä¸»é¢˜

        Returns:
            {
                'title': 'éœ€æ±‚æ ‡é¢˜',
                'description': 'éœ€æ±‚æè¿°',
                'user_scenario': 'ç”¨æˆ·åœºæ™¯',
                'demand_type': 'tool/content/service/education'
            }
        """
        prompt = PROMPTS['demand_card'].format(
            theme=cluster_theme,
            phrases='\n'.join(f"- {p}" for p in phrases[:20])  # æœ€å¤š20æ¡
        )

        response = self._call_api(prompt, max_tokens=500)

        import json
        try:
            result = json.loads(response)
            return result
        except:
            return {
                'title': 'æœªçŸ¥éœ€æ±‚',
                'description': response,
                'user_scenario': '',
                'demand_type': 'other'
            }

    def classify_token(
        self,
        token: str,
        context_phrases: Optional[List[str]] = None
    ) -> Dict[str, str]:
        """
        åˆ†ç±»tokenï¼ˆé˜¶æ®µ5è¾…åŠ©ï¼‰

        Args:
            token: å¾…åˆ†ç±»çš„token
            context_phrases: ä¸Šä¸‹æ–‡çŸ­è¯­ï¼ˆå¯é€‰ï¼‰

        Returns:
            {
                'token_type': 'intent/action/object/attribute/condition',
                'sub_category': 'å­ç±»åˆ«',
                'confidence': 'ç½®ä¿¡åº¦'
            }
        """
        context = ''
        if context_phrases:
            context = f"\n\nå‡ºç°åœ¨è¿™äº›çŸ­è¯­ä¸­ï¼š\n" + '\n'.join(f"- {p}" for p in context_phrases[:5])

        prompt = PROMPTS['token_classification'].format(
            token=token,
            context=context
        )

        response = self._call_api(prompt, max_tokens=150)

        import json
        try:
            result = json.loads(response)
            return result
        except:
            # å›é€€è§„åˆ™
            return {
                'token_type': 'other',
                'sub_category': '',
                'confidence': 'low'
            }

    def batch_classify_tokens(
        self,
        tokens: List[str],
        batch_size: int = 50
    ) -> List[Dict]:
        """æ‰¹é‡åˆ†ç±»tokens"""
        results = []
        for i in range(0, len(tokens), batch_size):
            batch = tokens[i:i+batch_size]

            prompt = PROMPTS['batch_token_classification'].format(
                tokens='\n'.join(f"{idx+1}. {t}" for idx, t in enumerate(batch))
            )

            response = self._call_api(prompt, max_tokens=1000)

            # è§£ææ‰¹é‡å“åº”
            batch_results = self._parse_batch_response(response, batch)
            results.extend(batch_results)

        return results

    def _call_api(self, prompt: str, max_tokens: int = 300) -> str:
        """è°ƒç”¨API"""
        if self.provider == 'openai':
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆã€‚è¯·ç”¨JSONæ ¼å¼å›ç­”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.3
            )
            return response.choices[0].message.content

        elif self.provider == 'anthropic':
            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text

    def _parse_batch_response(self, response: str, tokens: List[str]) -> List[Dict]:
        """è§£ææ‰¹é‡å“åº”"""
        # å®ç°æ‰¹é‡å“åº”è§£æé€»è¾‘
        pass
```

#### 4.2.3 InitialPipelineï¼ˆåˆæ¬¡å®Œæ•´æµç¨‹ï¼‰

```python
# pipelines/initial_pipeline.py

from typing import List
from core.data_integration import DataIntegrator
from core.clustering_engine import ClusteringEngine
from core.embedding_service import EmbeddingService
from ai.llm_client import LLMClient
from services.phrase_service import PhraseService
from services.demand_service import DemandService
from services.token_service import TokenService
from ui.cluster_selector import ClusterSelector
from ui.demand_editor import DemandEditor

class InitialPipeline:
    """åˆæ¬¡å®Œæ•´æµç¨‹ï¼ˆé˜¶æ®µ0-5ï¼‰"""

    def __init__(self, round_id: int = 1):
        self.round_id = round_id

        # åˆå§‹åŒ–æœåŠ¡
        self.embedding_service = EmbeddingService()
        self.clustering_engine = ClusteringEngine(self.embedding_service)
        self.llm_client = LLMClient(provider='openai')

        self.phrase_service = PhraseService()
        self.demand_service = DemandService()
        self.token_service = TokenService()

    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("=" * 60)
        print(f"åˆæ¬¡å®Œæ•´æµç¨‹ - Round {self.round_id}")
        print("=" * 60)

        # é˜¶æ®µ1: æ•°æ®æ•´åˆ
        print("\n[é˜¶æ®µ1] æ•°æ®æ•´åˆ...")
        phrases = self._stage1_integrate_data()

        # é˜¶æ®µ2: å¤§ç»„èšç±»
        print("\n[é˜¶æ®µ2] å¤§ç»„èšç±»...")
        cluster_meta_A = self._stage2_large_clustering(phrases)

        # é˜¶æ®µ3: äººå·¥é€‰æ‹©å¤§ç»„
        print("\n[é˜¶æ®µ3] å¤§ç»„é€‰æ‹©ï¼ˆäººå·¥äº¤äº’ï¼‰...")
        selected_clusters = self._stage3_select_clusters(cluster_meta_A)

        # é˜¶æ®µ4: å°ç»„èšç±» + éœ€æ±‚æŒ–æ˜
        print("\n[é˜¶æ®µ4] å°ç»„èšç±»ä¸éœ€æ±‚æŒ–æ˜...")
        demands = self._stage4_small_clustering_and_demands(
            phrases, selected_clusters
        )

        # é˜¶æ®µ5: tokensè¯åº“æ„å»º
        print("\n[é˜¶æ®µ5] tokensè¯åº“æ„å»º...")
        tokens = self._stage5_extract_tokens(demands)

        print("\n" + "=" * 60)
        print("æµç¨‹å®Œæˆï¼")
        print("=" * 60)
        print(f"æ€»çŸ­è¯­æ•°: {len(phrases)}")
        print(f"å¤§ç»„æ•°: {len(cluster_meta_A)}")
        print(f"é€‰ä¸­å¤§ç»„: {len(selected_clusters)}")
        print(f"éœ€æ±‚å¡ç‰‡: {len(demands)}")
        print(f"tokensæ•°: {len(tokens)}")

    def _stage1_integrate_data(self) -> List:
        """é˜¶æ®µ1ï¼šæ•°æ®æ•´åˆ"""
        integrator = DataIntegrator()

        # è¯»å–åŸå§‹æ•°æ®
        semrush_data = integrator.load_semrush("åŸå§‹è¯/små¯¼å‡ºè¯")
        dropdown_data = integrator.load_dropdown("åŸå§‹è¯/ä¸‹æ‹‰è¯")
        related_data = integrator.load_related_search("åŸå§‹è¯/ç›¸å…³æœç´¢.xlsx")

        # æ•´åˆå¹¶æ¸…æ´—
        merged = integrator.merge_all([semrush_data, dropdown_data, related_data])
        cleaned = integrator.clean(merged)

        # ä¿å­˜åˆ°æ•°æ®åº“
        phrases = self.phrase_service.bulk_create(
            cleaned,
            first_seen_round=self.round_id
        )

        return phrases

    def _stage2_large_clustering(self, phrases: List) -> List:
        """é˜¶æ®µ2ï¼šå¤§ç»„èšç±»"""
        labels, cluster_meta = self.clustering_engine.fit_large_clusters(
            phrases,
            save_embeddings=True
        )

        # æ›´æ–°æ•°æ®åº“
        self.phrase_service.update_cluster_A(phrases, labels)
        cluster_meta_saved = self.cluster_service.save_meta(cluster_meta)

        # ç”¨AIç”Ÿæˆä¸»é¢˜æ ‡ç­¾
        for cluster in cluster_meta_saved:
            if cluster.cluster_id == -1:
                continue  # è·³è¿‡å™ªéŸ³

            theme = self.llm_client.generate_cluster_theme(
                example_phrases=cluster.example_phrases.split('; '),
                cluster_size=cluster.size,
                total_frequency=cluster.total_frequency
            )

            cluster.main_theme = theme['theme']
            cluster.category = theme['category']
            self.cluster_service.update(cluster)

        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_cluster_report(cluster_meta_saved, level='A')

        return cluster_meta_saved

    def _stage3_select_clusters(self, cluster_meta: List) -> List:
        """é˜¶æ®µ3ï¼šäººå·¥é€‰æ‹©å¤§ç»„"""
        # å¯åŠ¨äº¤äº’å¼ç•Œé¢
        selector = ClusterSelector(cluster_meta)
        selected_ids = selector.run()  # è¿”å›é€‰ä¸­çš„cluster_idåˆ—è¡¨

        # æ›´æ–°æ•°æ®åº“
        self.cluster_service.mark_selected(selected_ids)

        selected_clusters = [c for c in cluster_meta if c.cluster_id in selected_ids]
        return selected_clusters

    def _stage4_small_clustering_and_demands(
        self,
        all_phrases: List,
        selected_clusters: List
    ) -> List:
        """é˜¶æ®µ4ï¼šå°ç»„èšç±» + éœ€æ±‚æŒ–æ˜"""
        all_demands = []

        for cluster_A in selected_clusters:
            print(f"\nå¤„ç†å¤§ç»„ {cluster_A.cluster_id}: {cluster_A.main_theme}")

            # è·å–è¯¥å¤§ç»„çš„æ‰€æœ‰çŸ­è¯­
            phrases_in_A = self.phrase_service.get_by_cluster_A(cluster_A.cluster_id)

            # å°ç»„èšç±»
            labels_B, cluster_meta_B = self.clustering_engine.fit_small_clusters(
                phrases_in_A,
                parent_cluster_id=cluster_A.cluster_id
            )

            # æ›´æ–°æ•°æ®åº“
            self.phrase_service.update_cluster_B(phrases_in_A, labels_B)
            cluster_meta_B_saved = self.cluster_service.save_meta(cluster_meta_B)

            # ä¸ºæ¯ä¸ªå°ç»„ç”Ÿæˆéœ€æ±‚å¡ç‰‡
            for cluster_B in cluster_meta_B_saved:
                if cluster_B.cluster_id == -1:
                    continue

                # AIç”Ÿæˆéœ€æ±‚å¡ç‰‡åˆç¨¿
                phrases_in_B = self.phrase_service.get_by_cluster_B(cluster_B.cluster_id)
                phrase_texts = [p.phrase for p in phrases_in_B]

                demand_draft = self.llm_client.generate_demand_card(
                    phrases=phrase_texts,
                    cluster_theme=cluster_A.main_theme
                )

                # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¾…äººå·¥å®¡æ ¸ï¼‰
                demand = self.demand_service.create(
                    title=demand_draft['title'],
                    description=demand_draft['description'],
                    user_scenario=demand_draft['user_scenario'],
                    demand_type=demand_draft['demand_type'],
                    source_cluster_A=cluster_A.cluster_id,
                    source_cluster_B=cluster_B.cluster_id,
                    status='idea',
                    created_by='AI'
                )

                all_demands.append(demand)

        # å¯åŠ¨éœ€æ±‚ç¼–è¾‘ç•Œé¢ï¼ˆäººå·¥ç²¾ä¿®ï¼‰
        editor = DemandEditor(all_demands)
        reviewed_demands = editor.run()  # è¿”å›å®¡æ ¸åçš„éœ€æ±‚åˆ—è¡¨

        # æ›´æ–°phrasesçš„mapped_demand_id
        for demand in reviewed_demands:
            if demand.status != 'archived':  # ä¿ç•™çš„éœ€æ±‚
                phrases = self.phrase_service.get_by_cluster_B(demand.source_cluster_B)
                self.phrase_service.map_to_demand(phrases, demand.demand_id)

        return reviewed_demands

    def _stage5_extract_tokens(self, demands: List) -> List:
        """é˜¶æ®µ5ï¼štokensè¯åº“æ„å»º"""
        from utils.text_processing import TokenExtractor

        # è·å–æ‰€æœ‰å·²å…³è”éœ€æ±‚çš„çŸ­è¯­
        phrases = self.phrase_service.get_mapped_to_demands()
        phrase_texts = [p.phrase for p in phrases]

        # Pythonç²—æ‹†è¯
        extractor = TokenExtractor()
        candidate_tokens = extractor.extract_and_rank(phrase_texts)

        # AIæ‰¹é‡åˆ†ç±»
        token_classifications = self.llm_client.batch_classify_tokens(
            tokens=[t['text'] for t in candidate_tokens],
            batch_size=50
        )

        # ä¿å­˜åˆ°æ•°æ®åº“
        tokens = []
        for candidate, classification in zip(candidate_tokens, token_classifications):
            token = self.token_service.create(
                token_text=candidate['text'],
                token_type=classification['token_type'],
                sub_category=classification['sub_category'],
                in_phrase_count=candidate['frequency'],
                first_seen_round=self.round_id,
                source='ai',
                verified=False  # å¾…äººå·¥éªŒè¯
            )
            tokens.append(token)

        return tokens

    def _generate_cluster_report(self, cluster_meta: List, level: str):
        """ç”Ÿæˆèšç±»æŠ¥å‘Š"""
        # ç”ŸæˆHTMLæŠ¥å‘Š
        pass
```

### 4.3 æ¶æ„ä¼˜åŠ¿

**æ–°æ¶æ„ç›¸æ¯”å½“å‰æ¶æ„çš„ä¼˜åŠ¿**:

1. **çŠ¶æ€ç®¡ç†**: æ•°æ®åº“å­˜å‚¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘
2. **æ¨¡å—è§£è€¦**: æ¯ä¸ªæ¨¡å—èŒè´£å•ä¸€ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
3. **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°çš„æ•°æ®æºã€AIæä¾›å•†ã€UIç•Œé¢
4. **å¢é‡æ”¯æŒ**: ä»è®¾è®¡ä¸Šæ”¯æŒå¤šè½®è¿­ä»£
5. **AIé›†æˆ**: ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œæ˜“äºåˆ‡æ¢æ¨¡å‹
6. **äººæœºåä½œ**: UIæ¨¡å—ä¸“é—¨å¤„ç†äººå·¥äº¤äº’ç¯èŠ‚

---

## 5. èšç±»å‚æ•°ä¼˜åŒ–

### 5.1 å‚æ•°é…ç½®æ–¹æ¡ˆ

```python
# config/clustering.py

CLUSTERING_CONFIG = {
    'large': {  # å¤§ç»„èšç±»ï¼ˆé˜¶æ®µ2ï¼‰
        'min_cluster_size': 30,
        'min_samples': 3,
        'use_dynamic': False,  # å½“å‰ç¦ç”¨
        'dynamic_formula': 'log',  # 'log' or 'sqrt'
        'assignment_threshold': 0.5,  # å¢é‡åˆ†é…çš„è·ç¦»é˜ˆå€¼
        'metric': 'euclidean',
        'cluster_selection_method': 'eom',

        # æ•°æ®é‡åˆ†æ®µå‚æ•°ï¼ˆå¯é€‰ï¼‰
        'size_based_params': {
            (0, 10000): {'min_cluster_size': 20, 'min_samples': 2},
            (10000, 50000): {'min_cluster_size': 30, 'min_samples': 3},
            (50000, 100000): {'min_cluster_size': 40, 'min_samples': 3},
            (100000, float('inf')): {'min_cluster_size': 50, 'min_samples': 4}
        }
    },

    'small': {  # å°ç»„èšç±»ï¼ˆé˜¶æ®µ4ï¼‰
        'min_cluster_size_ratio': 0.05,  # å¤§ç»„è§„æ¨¡çš„5%
        'min_cluster_size_min': 5,        # æœ€å°å€¼
        'min_cluster_size_max': 20,       # æœ€å¤§å€¼
        'min_samples': 2,
        'metric': 'euclidean',
        'cluster_selection_method': 'eom'
    }
}
```

### 5.2 åŠ¨æ€å‚æ•°æ”¹è¿›

**å½“å‰é—®é¢˜**: å…¬å¼ `N/500` å¯¹å¤§æ•°æ®ä¸é€‚ç”¨

**æ”¹è¿›æ–¹æ¡ˆ**: ä½¿ç”¨å¯¹æ•°æˆ–å¼€æ–¹å…¬å¼

```python
def calculate_min_cluster_size(n_samples: int, formula: str = 'log') -> int:
    """
    æ”¹è¿›çš„åŠ¨æ€å‚æ•°è®¡ç®—

    Args:
        n_samples: æ ·æœ¬æ•°é‡
        formula: 'log' æˆ– 'sqrt'

    Returns:
        min_cluster_size
    """
    import math

    if formula == 'log':
        # å¯¹æ•°å…¬å¼
        size = int(math.log10(n_samples) * 6)
        return max(10, min(size, 50))  # é™åˆ¶åœ¨10-50ä¹‹é—´

    elif formula == 'sqrt':
        # å¼€æ–¹å…¬å¼
        size = int(math.sqrt(n_samples) / 10)
        return max(10, min(size, 50))

    else:
        raise ValueError(f"Unknown formula: {formula}")

# æµ‹è¯•ä¸åŒæ•°æ®é‡
test_cases = [1000, 5000, 10000, 50000, 100000, 500000]

print("æ•°æ®é‡ | å¯¹æ•°å…¬å¼ | å¼€æ–¹å…¬å¼ | åŸå…¬å¼(N/500)")
print("-" * 60)
for n in test_cases:
    log_size = calculate_min_cluster_size(n, 'log')
    sqrt_size = calculate_min_cluster_size(n, 'sqrt')
    old_size = max(10, round(n / 500))

    print(f"{n:>6} | {log_size:>8} | {sqrt_size:>8} | {old_size:>15}")

"""
è¾“å‡ºç¤ºä¾‹:
æ•°æ®é‡ | å¯¹æ•°å…¬å¼ | å¼€æ–¹å…¬å¼ | åŸå…¬å¼(N/500)
------------------------------------------------------------
  1000 |       18 |        3 |               2
  5000 |       22 |        7 |              10
 10000 |       24 |       10 |              20
 50000 |       28 |       22 |             100  â† é—®é¢˜ï¼
100000 |       30 |       31 |             200  â† é—®é¢˜ï¼
500000 |       34 |       70 |            1000  â† é—®é¢˜ï¼

ç»“è®º: å¯¹æ•°å…¬å¼æœ€ç¨³å®šï¼Œå¢é•¿ç¼“æ…¢
"""
```

**å»ºè®®é‡‡ç”¨å¯¹æ•°å…¬å¼**:
- å¯¹äº1K-500Kæ•°æ®ï¼Œ`min_cluster_size`ä¿æŒåœ¨18-34ä¹‹é—´
- å¢é•¿å¹³ç¼“ï¼Œä¸ä¼šçªå˜
- ç¬¦åˆèšç±»ç²’åº¦çš„é¢„æœŸ

### 5.3 å°ç»„èšç±»å‚æ•°è‡ªé€‚åº”

```python
def calculate_small_cluster_params(parent_cluster_size: int) -> dict:
    """
    æ ¹æ®å¤§ç»„è§„æ¨¡è®¡ç®—å°ç»„èšç±»å‚æ•°

    Args:
        parent_cluster_size: çˆ¶å¤§ç»„çš„çŸ­è¯­æ•°é‡

    Returns:
        {'min_cluster_size': int, 'min_samples': int}
    """
    # åŸºæœ¬è§„åˆ™ï¼šå¤§ç»„è¶Šå°ï¼Œå°ç»„å‚æ•°è¶Šå®½æ¾
    if parent_cluster_size < 50:
        return {'min_cluster_size': 5, 'min_samples': 2}
    elif parent_cluster_size < 200:
        return {'min_cluster_size': 8, 'min_samples': 2}
    elif parent_cluster_size < 500:
        return {'min_cluster_size': 12, 'min_samples': 3}
    else:
        return {'min_cluster_size': 15, 'min_samples': 3}

# ç¤ºä¾‹
for size in [30, 80, 150, 300, 600]:
    params = calculate_small_cluster_params(size)
    expected_clusters = size // params['min_cluster_size']
    print(f"å¤§ç»„è§„æ¨¡={size:>3} â†’ å°ç»„å‚æ•°={params} â†’ é¢„æœŸå°ç»„æ•°â‰ˆ{expected_clusters}")

"""
è¾“å‡º:
å¤§ç»„è§„æ¨¡= 30 â†’ å°ç»„å‚æ•°={'min_cluster_size': 5, 'min_samples': 2} â†’ é¢„æœŸå°ç»„æ•°â‰ˆ6
å¤§ç»„è§„æ¨¡= 80 â†’ å°ç»„å‚æ•°={'min_cluster_size': 8, 'min_samples': 2} â†’ é¢„æœŸå°ç»„æ•°â‰ˆ10
å¤§ç»„è§„æ¨¡=150 â†’ å°ç»„å‚æ•°={'min_cluster_size': 12, 'min_samples': 3} â†’ é¢„æœŸå°ç»„æ•°â‰ˆ12
å¤§ç»„è§„æ¨¡=300 â†’ å°ç»„å‚æ•°={'min_cluster_size': 15, 'min_samples': 3} â†’ é¢„æœŸå°ç»„æ•°â‰ˆ20
å¤§ç»„è§„æ¨¡=600 â†’ å°ç»„å‚æ•°={'min_cluster_size': 15, 'min_samples': 3} â†’ é¢„æœŸå°ç»„æ•°â‰ˆ40
"""
```

### 5.4 å™ªéŸ³å¤„ç†ç­–ç•¥

**å½“å‰é—®é¢˜**: 33%çš„å™ªéŸ³ç‡åé«˜

**åˆ†æ**:
```python
# è¯Šæ–­å™ªéŸ³æ¥æº
def analyze_noise(phrases_df):
    """åˆ†æå™ªéŸ³ç‚¹çš„ç‰¹å¾"""
    noise = phrases_df[phrases_df['cluster_id_A'] == -1]

    print(f"å™ªéŸ³ç‚¹æ€»æ•°: {len(noise)} ({len(noise)/len(phrases_df)*100:.1f}%)")

    # æŒ‰seed_wordç»Ÿè®¡
    print("\nå™ªéŸ³åˆ†å¸ƒ by seed_word:")
    noise_by_seed = noise.groupby('seed_word').size().sort_values(ascending=False)
    print(noise_by_seed.head(10))

    # æŒ‰source_typeç»Ÿè®¡
    print("\nå™ªéŸ³åˆ†å¸ƒ by source_type:")
    noise_by_source = noise.groupby('source_type').size()
    print(noise_by_source)

    # é•¿åº¦åˆ†å¸ƒ
    print("\nå™ªéŸ³ç‚¹çš„é•¿åº¦åˆ†å¸ƒ:")
    print(noise['word_count'].describe())

    # é¢‘æ¬¡åˆ†å¸ƒ
    print("\nå™ªéŸ³ç‚¹çš„é¢‘æ¬¡åˆ†å¸ƒ:")
    print(noise['frequency'].describe())
```

**ä¼˜åŒ–ç­–ç•¥**:

1. **åˆ†ç»„èšç±»** (å¦‚æœå™ªéŸ³æ¥è‡ªç‰¹å®šseed_word)
```python
# å¯¹è·¨åº¦å¤§çš„ç§å­è¯å•ç‹¬èšç±»
high_variance_seeds = ['code', 'online', 'best']

for seed in high_variance_seeds:
    phrases_subset = phrases[phrases['seed_word'] == seed]
    labels = clustering_engine.fit_large_clusters(
        phrases_subset,
        min_cluster_size=15  # æ›´å°çš„å‚æ•°
    )
```

2. **äºŒæ¬¡åˆ†é…** (å¯¹å™ªéŸ³ç‚¹å°è¯•æ›´å®½æ¾çš„æ ‡å‡†)
```python
# ç¬¬ä¸€è½®èšç±»åï¼Œå¯¹å™ªéŸ³ç‚¹äºŒæ¬¡åˆ†é…
noise_phrases = phrases[phrases['cluster_id_A'] == -1]

# ä½¿ç”¨KNNæ‰¾æœ€è¿‘çš„ç°‡ï¼Œä½†é™ä½è·ç¦»é˜ˆå€¼
labels = clustering_engine.assign_new_phrases(
    noise_phrases,
    reference_clusters,
    threshold=0.7  # æ¯”æ­£å¸¸çš„0.5æ›´å®½æ¾
)
```

3. **æ¥å—å™ªéŸ³** (å¯¹äºçœŸæ­£å¤šæ ·åŒ–çš„æ•°æ®)
```
å¦‚æœå™ªéŸ³ç‚¹ç¡®å®è¯­ä¹‰åˆ†æ•£ï¼Œä¸å¼ºæ±‚å…¨éƒ¨èšç±»
å°†å™ªéŸ³ç‚¹ä½œä¸º"é•¿å°¾è¯åº“"ä¿ç•™ï¼Œä¾›åç»­æ‰‹åŠ¨ç­›é€‰æˆ–SEOä½¿ç”¨
```

---

## 6. å¢é‡æ›´æ–°æœºåˆ¶

### 6.1 å¢é‡æ›´æ–°æµç¨‹è®¾è®¡

```python
# pipelines/incremental_pipeline.py

class IncrementalPipeline:
    """å¢é‡æ›´æ–°æµç¨‹ï¼ˆé˜¶æ®µ7ï¼‰"""

    def __init__(self, round_id: int):
        self.round_id = round_id
        self.embedding_service = EmbeddingService()
        self.clustering_engine = ClusteringEngine(self.embedding_service)
        self.phrase_service = PhraseService()
        self.demand_service = DemandService()
        self.token_service = TokenService()

    def run(self, new_data_sources: List[str]):
        """è¿è¡Œå¢é‡æµç¨‹"""
        print("=" * 60)
        print(f"å¢é‡æ›´æ–°æµç¨‹ - Round {self.round_id}")
        print("=" * 60)

        # 7.1 å¯¼å…¥æ–°æ•°æ®
        print("\n[7.1] å¯¼å…¥æ–°æ•°æ®...")
        new_phrases, updated_phrases = self._import_new_data(new_data_sources)

        # 7.2 åˆ†é…åˆ°å¤§ç»„
        print("\n[7.2] åˆ†é…æ–°è¯åˆ°å¤§ç»„...")
        self._assign_to_large_clusters(new_phrases)

        # 7.3 è¿‡æ»¤ï¼šåªå¤„ç†"çœŸæ­£æ–°"çš„å†…å®¹
        print("\n[7.3] è¿‡æ»¤å¤„ç†èŒƒå›´...")
        actionable_phrases = self._filter_actionable(new_phrases)

        # 7.4 å°ç»„èšç±»ï¼ˆä»…é’ˆå¯¹é€‰ä¸­å¤§ç»„çš„æ–°è¯ï¼‰
        print("\n[7.4] å°ç»„èšç±»...")
        new_demands = self._incremental_small_clustering(actionable_phrases)

        # 7.5 æ›´æ–°tokens
        print("\n[7.5] æ›´æ–°tokensè¯åº“...")
        new_tokens = self._update_tokens(actionable_phrases)

        print("\n" + "=" * 60)
        print("å¢é‡æ›´æ–°å®Œæˆï¼")
        print("=" * 60)
        print(f"æ–°å¢çŸ­è¯­: {len(new_phrases)}")
        print(f"æ›´æ–°çŸ­è¯­: {len(updated_phrases)}")
        print(f"å¯å¤„ç†çŸ­è¯­: {len(actionable_phrases)}")
        print(f"æ–°å¢éœ€æ±‚: {len(new_demands)}")
        print(f"æ–°å¢tokens: {len(new_tokens)}")

    def _import_new_data(self, sources: List[str]) -> Tuple[List, List]:
        """å¯¼å…¥æ–°æ•°æ®å¹¶å»é‡"""
        from core.data_integration import DataIntegrator

        integrator = DataIntegrator()
        new_data = integrator.load_multiple_sources(sources)

        new_phrases = []
        updated_phrases = []

        for record in new_data:
            existing = self.phrase_service.find_by_phrase(record['phrase'])

            if existing:
                # æ›´æ–°é¢‘æ¬¡/volume
                existing.frequency = max(existing.frequency, record['frequency'])
                if 'volume' in record:
                    existing.volume = max(existing.volume, record['volume'])
                self.phrase_service.update(existing)
                updated_phrases.append(existing)
            else:
                # æ–°çŸ­è¯­
                new_phrase = self.phrase_service.create(
                    phrase=record['phrase'],
                    seed_word=record['seed_word'],
                    source_type=record['source_type'],
                    frequency=record['frequency'],
                    first_seen_round=self.round_id,
                    processed_status='unseen'
                )
                new_phrases.append(new_phrase)

        return new_phrases, updated_phrases

    def _assign_to_large_clusters(self, new_phrases: List):
        """å°†æ–°è¯åˆ†é…åˆ°å·²æœ‰å¤§ç»„"""
        # åŠ è½½ç°æœ‰å¤§ç»„
        reference_clusters = self.cluster_service.get_all_large_clusters()

        # ä½¿ç”¨KNNåˆ†é…
        labels = self.clustering_engine.assign_new_phrases(
            new_phrases,
            reference_clusters
        )

        # æ›´æ–°æ•°æ®åº“
        self.phrase_service.update_cluster_A(new_phrases, labels)

        # ç»Ÿè®¡
        assigned = sum(1 for l in labels if l != -1)
        noise = sum(1 for l in labels if l == -1)
        print(f"  åˆ†é…åˆ°å¤§ç»„: {assigned}")
        print(f"  æ ‡è®°ä¸ºå™ªéŸ³: {noise}")

    def _filter_actionable(self, new_phrases: List) -> List:
        """
        è¿‡æ»¤å‡º"å€¼å¾—å¤„ç†"çš„æ–°çŸ­è¯­

        è§„åˆ™:
        1. åªçœ‹processed_status='unseen'çš„
        2. æ’é™¤å·²æœ‰æ˜ç¡®éœ€æ±‚ä¸”çŠ¶æ€>=validatedçš„
        3. æ’é™¤å™ªéŸ³ç‚¹ä¸­é¢‘æ¬¡å¾ˆä½çš„ï¼ˆ<10ï¼‰
        """
        actionable = []

        for phrase in new_phrases:
            # è§„åˆ™1: å¿…é¡»æ˜¯unseen
            if phrase.processed_status != 'unseen':
                continue

            # è§„åˆ™2: å¦‚æœå·²å…³è”éœ€æ±‚ï¼Œæ£€æŸ¥éœ€æ±‚çŠ¶æ€
            if phrase.mapped_demand_id:
                demand = self.demand_service.get(phrase.mapped_demand_id)
                if demand.status in ['validated', 'in_progress', 'launched', 'profitable']:
                    # å·²æœ‰ç¨³å®šéœ€æ±‚ï¼Œä¸å†å¤„ç†
                    phrase.processed_status = 'archived'
                    self.phrase_service.update(phrase)
                    continue

            # è§„åˆ™3: å™ªéŸ³ç‚¹çš„é¢‘æ¬¡è¿‡æ»¤
            if phrase.cluster_id_A == -1 and phrase.frequency < 10:
                phrase.processed_status = 'archived'
                self.phrase_service.update(phrase)
                continue

            actionable.append(phrase)

        return actionable

    def _incremental_small_clustering(self, actionable_phrases: List) -> List:
        """å¢é‡å°ç»„èšç±»"""
        # æŒ‰å¤§ç»„åˆ†ç»„
        phrases_by_cluster_A = {}
        for phrase in actionable_phrases:
            cluster_A = phrase.cluster_id_A
            if cluster_A == -1:
                continue  # è·³è¿‡å™ªéŸ³

            if cluster_A not in phrases_by_cluster_A:
                phrases_by_cluster_A[cluster_A] = []
            phrases_by_cluster_A[cluster_A].append(phrase)

        new_demands = []

        for cluster_A, phrases in phrases_by_cluster_A.items():
            # æ£€æŸ¥è¯¥å¤§ç»„æ˜¯å¦è¢«é€‰ä¸­
            cluster_meta = self.cluster_service.get(cluster_A)
            if not cluster_meta.is_selected:
                continue  # è·³è¿‡æœªé€‰ä¸­çš„å¤§ç»„

            # è·å–è¯¥å¤§ç»„çš„æ‰€æœ‰çŸ­è¯­ï¼ˆåŒ…æ‹¬æ—§è¯ï¼‰
            all_phrases_in_A = self.phrase_service.get_by_cluster_A(cluster_A)

            # é‡æ–°å°ç»„èšç±»ï¼ˆæ–°è¯+æ—§è¯ä¸€èµ·ï¼‰
            labels_B, cluster_meta_B = self.clustering_engine.fit_small_clusters(
                all_phrases_in_A,
                parent_cluster_id=cluster_A
            )

            # æ›´æ–°æ•°æ®åº“
            self.phrase_service.update_cluster_B(all_phrases_in_A, labels_B)

            # å¯¹æ–°å‡ºç°çš„å°ç»„ï¼Œç”Ÿæˆéœ€æ±‚å¡ç‰‡
            for cluster_B in cluster_meta_B:
                if cluster_B.cluster_id == -1:
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²æœ‰éœ€æ±‚å¡ç‰‡
                existing_demand = self.demand_service.find_by_cluster_B(cluster_B.cluster_id)
                if existing_demand:
                    continue  # å·²æœ‰éœ€æ±‚ï¼Œä¸é‡å¤ç”Ÿæˆ

                # ç”Ÿæˆæ–°éœ€æ±‚å¡ç‰‡
                phrases_in_B = self.phrase_service.get_by_cluster_B(cluster_B.cluster_id)
                demand_draft = self.llm_client.generate_demand_card(
                    phrases=[p.phrase for p in phrases_in_B],
                    cluster_theme=cluster_meta.main_theme
                )

                demand = self.demand_service.create(
                    title=demand_draft['title'],
                    description=demand_draft['description'],
                    demand_type=demand_draft['demand_type'],
                    source_cluster_A=cluster_A,
                    source_cluster_B=cluster_B.cluster_id,
                    status='idea',
                    created_by='AI_Incremental'
                )

                new_demands.append(demand)

        return new_demands

    def _update_tokens(self, actionable_phrases: List) -> List:
        """æ›´æ–°tokensè¯åº“"""
        from utils.text_processing import TokenExtractor

        # åªå¯¹å·²å…³è”éœ€æ±‚çš„æ–°çŸ­è¯­æå–tokens
        mapped_phrases = [p for p in actionable_phrases if p.mapped_demand_id]

        if not mapped_phrases:
            return []

        # æå–tokens
        extractor = TokenExtractor()
        new_tokens_candidate = extractor.extract_and_rank(
            [p.phrase for p in mapped_phrases]
        )

        # è¿‡æ»¤ï¼šåªä¿ç•™ä¸åœ¨ç°æœ‰è¯åº“ä¸­çš„
        existing_tokens = set(self.token_service.get_all_texts())
        new_tokens_candidate = [
            t for t in new_tokens_candidate
            if t['text'] not in existing_tokens
        ]

        if not new_tokens_candidate:
            return []

        # AIæ‰¹é‡åˆ†ç±»
        classifications = self.llm_client.batch_classify_tokens(
            tokens=[t['text'] for t in new_tokens_candidate],
            batch_size=50
        )

        # ä¿å­˜
        new_tokens = []
        for candidate, classification in zip(new_tokens_candidate, classifications):
            token = self.token_service.create(
                token_text=candidate['text'],
                token_type=classification['token_type'],
                sub_category=classification.get('sub_category', ''),
                in_phrase_count=candidate['frequency'],
                first_seen_round=self.round_id,
                source='ai',
                verified=False
            )
            new_tokens.append(token)

        return new_tokens
```

### 6.2 Embeddingç‰ˆæœ¬ç®¡ç†

**é—®é¢˜**: å¤šè½®æ›´æ–°æ—¶ï¼Œembeddingæ¨¡å‹å¯èƒ½æ›´æ–°

**è§£å†³æ–¹æ¡ˆ**: ç‰ˆæœ¬æ§åˆ¶ + å…¼å®¹ç­–ç•¥

```python
# core/embedding_service.py

class EmbeddingService:
    """EmbeddingæœåŠ¡ï¼šæ”¯æŒç‰ˆæœ¬ç®¡ç†"""

    def __init__(self):
        self.model_name = 'all-MiniLM-L6-v2'
        self.model_version = self._get_model_version()
        self.cache_dir = Path('data/embeddings_cache')
        self.cache_dir.mkdir(exist_ok=True)

    def generate(self, texts: List[str]) -> np.ndarray:
        """ç”Ÿæˆembeddings"""
        from sentence_transformers import SentenceTransformer

        model = SentenceTransformer(self.model_name)
        embeddings = model.encode(texts, convert_to_numpy=True)

        return embeddings

    def cache(self, phrases: List, embeddings: np.ndarray):
        """ç¼“å­˜embeddings"""
        cache_file = self.cache_dir / f"round_{round_id}_v{self.model_version}.npz"

        phrase_ids = [p.phrase_id for p in phrases]

        np.savez_compressed(
            cache_file,
            phrase_ids=phrase_ids,
            embeddings=embeddings,
            model_name=self.model_name,
            model_version=self.model_version
        )

    def load_cache(self, phrase_ids: List[int]) -> Optional[np.ndarray]:
        """åŠ è½½ç¼“å­˜çš„embeddings"""
        # å°è¯•åŠ è½½æœ€æ–°ç‰ˆæœ¬
        cache_files = sorted(self.cache_dir.glob(f"*_v{self.model_version}.npz"))

        if not cache_files:
            return None

        # åˆå¹¶æ‰€æœ‰ç¼“å­˜
        all_embeddings = {}
        for cache_file in cache_files:
            data = np.load(cache_file)
            for pid, emb in zip(data['phrase_ids'], data['embeddings']):
                all_embeddings[pid] = emb

        # è¿”å›è¯·æ±‚çš„embeddings
        result = []
        for pid in phrase_ids:
            if pid in all_embeddings:
                result.append(all_embeddings[pid])
            else:
                return None  # ç¼ºå¤±ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ

        return np.array(result)

    def _get_model_version(self) -> str:
        """è·å–æ¨¡å‹ç‰ˆæœ¬"""
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(self.model_name)
        return f"{model.__version__}"
```

---

## 7. å®æ–½è·¯çº¿å›¾

### 7.1 Phase 1: åŸºç¡€æ¶æ„æ­å»ºï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: æ­å»ºæ–°çš„ä»£ç æ¶æ„å’Œæ•°æ®åº“

**ä»»åŠ¡**:
1. [ ] è®¾è®¡å¹¶åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆphrases, demands, tokens, cluster_metaï¼‰
2. [ ] å®ç°ORMæ¨¡å‹ï¼ˆSQLAlchemyï¼‰
3. [ ] å®ç°åŸºç¡€æœåŠ¡å±‚ï¼ˆPhraseService, DemandServiceç­‰ï¼‰
4. [ ] è¿ç§»ç°æœ‰CSVæ•°æ®åˆ°æ•°æ®åº“
5. [ ] å®ç°EmbeddingServiceï¼ˆå¸¦ç¼“å­˜ï¼‰

**éªŒæ”¶æ ‡å‡†**:
- æ•°æ®åº“å»ºè¡¨æˆåŠŸ
- ç°æœ‰5ä¸‡è¯å¯¼å…¥æ•°æ®åº“
- å¯ä»¥é€šè¿‡æœåŠ¡å±‚æŸ¥è¯¢æ•°æ®

### 7.2 Phase 2: èšç±»å¼•æ“ä¼˜åŒ–ï¼ˆ3å¤©ï¼‰

**ç›®æ ‡**: æ”¹è¿›èšç±»ç®—æ³•å’Œå‚æ•°

**ä»»åŠ¡**:
1. [ ] å®ç°ClusteringEngineç±»
2. [ ] å®ç°åŠ¨æ€å‚æ•°è®¡ç®—ï¼ˆå¯¹æ•°å…¬å¼ï¼‰
3. [ ] å®ç°å°ç»„èšç±»è‡ªé€‚åº”å‚æ•°
4. [ ] å®ç°å¢é‡åˆ†é…ï¼ˆKNNï¼‰
5. [ ] æµ‹è¯•å¹¶éªŒè¯èšç±»è´¨é‡

**éªŒæ”¶æ ‡å‡†**:
- å¤§ç»„èšç±»ç»“æœç¬¦åˆé¢„æœŸï¼ˆ60-80ä¸ªç°‡ï¼‰
- å™ªéŸ³ç‡é™ä½åˆ°25%ä»¥ä¸‹
- å°ç»„èšç±»èƒ½å¤„ç†ä¸åŒè§„æ¨¡çš„å¤§ç»„

### 7.3 Phase 3: AIé›†æˆï¼ˆ3å¤©ï¼‰

**ç›®æ ‡**: é›†æˆLLM APIï¼Œå®ç°è‡ªåŠ¨åŒ–è¾…åŠ©

**ä»»åŠ¡**:
1. [ ] å®ç°LLMClientç±»ï¼ˆæ”¯æŒOpenAIå’ŒClaudeï¼‰
2. [ ] ç¼–å†™Promptæ¨¡æ¿
3. [ ] å®ç°æ‰¹é‡å¤„ç†é€»è¾‘
4. [ ] æµ‹è¯•ç”Ÿæˆè´¨é‡ï¼ˆä¸»é¢˜æ ‡ç­¾ã€éœ€æ±‚å¡ç‰‡ã€tokenåˆ†ç±»ï¼‰
5. [ ] å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•

**éªŒæ”¶æ ‡å‡†**:
- èƒ½æˆåŠŸè°ƒç”¨LLM API
- ç”Ÿæˆçš„ä¸»é¢˜æ ‡ç­¾å‡†ç¡®ç‡>70%
- éœ€æ±‚å¡ç‰‡è´¨é‡å¯ç”¨ï¼ˆéœ€äººå·¥ç²¾ä¿®ï¼‰

### 7.4 Phase 4: åˆæ¬¡å®Œæ•´æµç¨‹ï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: å®ç°åˆæ¬¡å…¨é‡æµç¨‹ï¼ˆé˜¶æ®µ0-5ï¼‰

**ä»»åŠ¡**:
1. [ ] å®ç°InitialPipelineç±»
2. [ ] å®ç°é˜¶æ®µ3çš„ClusterSelector UI
3. [ ] å®ç°é˜¶æ®µ4çš„DemandEditor UI
4. [ ] å®ç°TokenExtractorå·¥å…·
5. [ ] ç«¯åˆ°ç«¯æµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- ä»åŸå§‹æ•°æ®åˆ°éœ€æ±‚å¡ç‰‡å…¨æµç¨‹è·‘é€š
- ç”Ÿæˆè‡³å°‘5ä¸ªé«˜è´¨é‡éœ€æ±‚å¡ç‰‡
- tokensè¯åº“è‡³å°‘300ä¸ª

### 7.5 Phase 5: å¢é‡æ›´æ–°æœºåˆ¶ï¼ˆ3å¤©ï¼‰

**ç›®æ ‡**: å®ç°å¤šè½®è¿­ä»£çš„å¢é‡æ›´æ–°

**ä»»åŠ¡**:
1. [ ] å®ç°IncrementalPipelineç±»
2. [ ] å®ç°è¿‡æ»¤é€»è¾‘ï¼ˆactionable phrasesï¼‰
3. [ ] å®ç°embeddingç¼“å­˜å’Œç‰ˆæœ¬ç®¡ç†
4. [ ] æµ‹è¯•å¢é‡æµç¨‹

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¯¼å…¥10ä¸‡è¯å¹¶æ­£ç¡®å»é‡
- æ–°è¯èƒ½æ­£ç¡®åˆ†é…åˆ°å¤§ç»„
- åªå¤„ç†çœŸæ­£æ–°çš„å†…å®¹

### 7.6 Phase 6: UIæ”¹è¿›å’Œè½åœ°å·¥å…·ï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: æå‡ç”¨æˆ·ä½“éªŒï¼Œæ”¯æŒè½åœ°åº”ç”¨

**ä»»åŠ¡**:
1. [ ] æ”¹è¿›ClusterSelectorï¼ˆé¢„ç­›é€‰ã€æ’åºã€æœç´¢ï¼‰
2. [ ] æ”¹è¿›DemandEditorï¼ˆæ‰¹é‡ç¼–è¾‘ã€åˆå¹¶ã€è¯„åˆ†ï¼‰
3. [ ] å®ç°TokenReviewerï¼ˆå¿«é€ŸéªŒè¯å’Œçº æ­£ï¼‰
4. [ ] å®ç°å¯¼å‡ºå·¥å…·ï¼ˆSEOè¯åº“ã€Landing Pageç´ æï¼‰

**éªŒæ”¶æ ‡å‡†**:
- äººå·¥é€‰æ‹©å¤§ç»„æ—¶é—´ä»2å°æ—¶é™åˆ°30åˆ†é’Ÿ
- éœ€æ±‚ç¼–è¾‘ç•Œé¢å‹å¥½ï¼Œæ”¯æŒæ‰¹é‡æ“ä½œ
- èƒ½å¯¼å‡ºå¯ç”¨çš„SEOè¯è¡¨

### 7.7 æ€»æ—¶é—´ä¼°ç®—

| Phase | ä»»åŠ¡ | ä¼°ç®—æ—¶é—´ | ä¾èµ– |
|-------|------|---------|------|
| 1 | åŸºç¡€æ¶æ„ | 1å‘¨ | - |
| 2 | èšç±»å¼•æ“ | 3å¤© | Phase 1 |
| 3 | AIé›†æˆ | 3å¤© | Phase 1 |
| 4 | åˆæ¬¡æµç¨‹ | 1å‘¨ | Phase 1,2,3 |
| 5 | å¢é‡æ›´æ–° | 3å¤© | Phase 4 |
| 6 | UIæ”¹è¿› | 1å‘¨ | Phase 4,5 |

**æ€»è®¡**: çº¦**3-4å‘¨**ï¼ˆå…¨èŒå¼€å‘ï¼‰

**é‡Œç¨‹ç¢‘**:
- âœ… Phase 1å®Œæˆï¼šæœ‰æ•°æ®åº“ï¼Œå¯ä»¥æŸ¥è¯¢
- âœ… Phase 4å®Œæˆï¼šèƒ½è·‘é€šç¬¬ä¸€è½®ï¼Œäº§å‡ºéœ€æ±‚å¡ç‰‡
- âœ… Phase 5å®Œæˆï¼šèƒ½è¿›è¡Œç¬¬äºŒè½®è¿­ä»£
- âœ… Phase 6å®Œæˆï¼šç³»ç»Ÿå¯æŠ•å…¥å®é™…ä½¿ç”¨

---

## 8. é£é™©ä¸æŒ‘æˆ˜

### 8.1 æŠ€æœ¯é£é™©

#### é£é™©1: AIç”Ÿæˆè´¨é‡ä¸ç¨³å®š

**é£é™©æè¿°**:
- LLMç”Ÿæˆçš„ä¸»é¢˜æ ‡ç­¾ã€éœ€æ±‚å¡ç‰‡å¯èƒ½å‡ºç°å¹»è§‰
- æ‰¹é‡åˆ†ç±»tokenæ—¶ï¼Œå‡†ç¡®ç‡å¯èƒ½ä¸è¶³

**ç¼“è§£æªæ–½**:
1. è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä½äº0.7çš„æ ‡è®°ä¸ºå¾…äººå·¥å®¡æ ¸ï¼‰
2. äººå·¥æŠ½æ ·éªŒè¯ï¼ˆæ¯è½®æŠ½10%ï¼‰
3. å¤šæ¨¡å‹å¯¹æ¯”ï¼ˆGPT vs Claudeï¼‰
4. Promptå·¥ç¨‹ä¼˜åŒ–

**åº”æ€¥æ–¹æ¡ˆ**:
- é™ä½AIä½¿ç”¨æ¯”ä¾‹ï¼Œå¢åŠ äººå·¥å®¡æ ¸ç¯èŠ‚

#### é£é™©2: å¤§æ•°æ®æ€§èƒ½é—®é¢˜

**é£é™©æè¿°**:
- 100ä¸‡è¯çš„embeddingç”Ÿæˆå¯èƒ½éœ€è¦æ•°å°æ—¶
- æ•°æ®åº“æŸ¥è¯¢å¯èƒ½å˜æ…¢

**ç¼“è§£æªæ–½**:
1. ä½¿ç”¨GPUåŠ é€Ÿembedding
2. å®ç°åˆ†æ‰¹å¤„ç†
3. ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
4. ä½¿ç”¨ç¼“å­˜æœºåˆ¶

**åº”æ€¥æ–¹æ¡ˆ**:
- åˆ†ç»„å¤„ç†ï¼ˆæŒ‰seed_wordæ‹†åˆ†ï¼‰
- ä½¿ç”¨æ›´å¿«çš„embeddingæ¨¡å‹

#### é£é™©3: Embeddingç‰ˆæœ¬ä¸å…¼å®¹

**é£é™©æè¿°**:
- æ¨¡å‹æ›´æ–°åï¼Œæ–°æ—§embeddingä¸å¯æ¯”

**ç¼“è§£æªæ–½**:
1. å›ºå®šæ¨¡å‹ç‰ˆæœ¬
2. å®ç°ç‰ˆæœ¬æ£€æµ‹å’Œè­¦å‘Š
3. å¿…è¦æ—¶é‡æ–°ç”Ÿæˆæ‰€æœ‰embedding

### 8.2 äº§å“é£é™©

#### é£é™©4: äººå·¥å·¥ä½œé‡ä»ç„¶è¿‡å¤§

**é£é™©æè¿°**:
- å³ä½¿æœ‰AIè¾…åŠ©ï¼Œäººå·¥å®¡æ ¸ä»éœ€å¤§é‡æ—¶é—´
- é˜¶æ®µ3-5çš„äººå·¥ç¯èŠ‚å¯èƒ½æˆä¸ºç“¶é¢ˆ

**ç¼“è§£æªæ–½**:
1. æŒç»­ä¼˜åŒ–é¢„ç­›é€‰è§„åˆ™ï¼Œå‡å°‘å€™é€‰é¡¹
2. æ”¹è¿›UIï¼Œæé«˜æ“ä½œæ•ˆç‡
3. åŸ¹è®­AIæ¨¡å‹ï¼Œæé«˜é¦–æ¬¡ç”Ÿæˆè´¨é‡
4. å®ç°"æ‰¹é‡æ“ä½œ"åŠŸèƒ½

**åº”æ€¥æ–¹æ¡ˆ**:
- é™ä½è´¨é‡è¦æ±‚ï¼Œå¿«é€Ÿè¿­ä»£
- åˆ†é˜¶æ®µå¤„ç†ï¼ˆå…ˆåštop 20%çš„é«˜ä»·å€¼éƒ¨åˆ†ï¼‰

#### é£é™©5: éœ€æ±‚å¡ç‰‡éš¾ä»¥è½åœ°

**é£é™©æè¿°**:
- ç”Ÿæˆäº†å¾ˆå¤šéœ€æ±‚å¡ç‰‡ï¼Œä½†ä¸çŸ¥é“å¦‚ä½•å˜ç°
- å¸‚åœºè°ƒç ”å’Œäº§å“å¼€å‘éœ€è¦é¢å¤–æ—¶é—´

**ç¼“è§£æªæ–½**:
1. åœ¨éœ€æ±‚å¡ç‰‡ä¸­æ·»åŠ "å¯è¡Œæ€§è¯„ä¼°"å­—æ®µ
2. é›†æˆå¸‚åœºæ•°æ®ï¼ˆCPCã€ç«äº‰åº¦ã€è¶‹åŠ¿ï¼‰
3. æä¾›"MVPå¿«é€ŸéªŒè¯"æ¨¡æ¿
4. å»ºç«‹"éœ€æ±‚-äº§å“"æ•°æ®åº“ï¼Œç§¯ç´¯ç»éªŒ

### 8.3 è¿ç»´é£é™©

#### é£é™©6: APIæˆæœ¬è¿‡é«˜

**é£é™©æè¿°**:
- LLM APIè°ƒç”¨å¯èƒ½äº§ç”Ÿé«˜é¢è´¹ç”¨

**æˆæœ¬ä¼°ç®—**:
```
å‡è®¾:
- 100ä¸ªå¤§ç»„ Ã— 1æ¬¡ä¸»é¢˜ç”Ÿæˆ = 100æ¬¡è°ƒç”¨
- 500ä¸ªå°ç»„ Ã— 1æ¬¡éœ€æ±‚å¡ç‰‡ = 500æ¬¡è°ƒç”¨
- 5000ä¸ªtokens Ã— (1/50)æ‰¹é‡åˆ†ç±» = 100æ¬¡è°ƒç”¨
æ€»è®¡: 700æ¬¡è°ƒç”¨

æˆæœ¬ï¼ˆGPT-4ï¼‰:
- æ¯æ¬¡å¹³å‡500 tokensè¾“å…¥ + 200 tokensè¾“å‡º
- ä»·æ ¼: $0.01/1K input + $0.03/1K output
- å•æ¬¡æˆæœ¬: $0.005 + $0.006 = $0.011
- æ€»æˆæœ¬: 700 Ã— $0.011 = $7.7

ä½¿ç”¨GPT-3.5:
- ä»·æ ¼: $0.0005/1K input + $0.0015/1K output
- æ€»æˆæœ¬: çº¦$0.77

ç»“è®º: æˆæœ¬å¯æ§
```

**ç¼“è§£æªæ–½**:
1. ä¼˜å…ˆä½¿ç”¨GPT-3.5æˆ–Claude Haiku
2. å®ç°ç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
3. è®¾ç½®æ¯è½®é¢„ç®—ä¸Šé™

#### é£é™©7: ç³»ç»Ÿå¤æ‚åº¦å¢åŠ ï¼Œç»´æŠ¤å›°éš¾

**é£é™©æè¿°**:
- æ–°æ¶æ„æ¯”åŸæœ‰CSVæ–¹æ¡ˆå¤æ‚å¾—å¤š
- éœ€è¦ç»´æŠ¤æ•°æ®åº“ã€ç¼“å­˜ã€AIé›†æˆç­‰

**ç¼“è§£æªæ–½**:
1. è¯¦ç»†æ–‡æ¡£
2. å•å…ƒæµ‹è¯•è¦†ç›–æ ¸å¿ƒæ¨¡å—
3. ä½¿ç”¨ORMå‡å°‘SQLç›´å†™
4. æ¨¡å—åŒ–è®¾è®¡ï¼Œé™ä½è€¦åˆ

---

## 9. æ€»ç»“ä¸å»ºè®®

### 9.1 æµç¨‹åˆç†æ€§æ€»ç»“

âœ… **æµç¨‹è®¾è®¡åˆç†**:
- æ¸è¿›å¼èšç±»ï¼ˆå¤§ç»„â†’å°ç»„ï¼‰ç¬¦åˆè®¤çŸ¥
- å¢é‡æ›´æ–°æœºåˆ¶è®¾è®¡æ¸…æ™°
- ä¸‰å¼ è¡¨æ¶æ„ï¼ˆphrases/demands/tokensï¼‰ç»“æ„åˆç†

âš ï¸ **éœ€è¦ä¼˜åŒ–çš„ç‚¹**:
1. é˜¶æ®µ3çš„å¤§ç»„ç­›é€‰éœ€è¦é¢„ç­›é€‰è§„åˆ™
2. é˜¶æ®µ5çš„tokensæ„å»ºæˆæœ¬è¾ƒé«˜ï¼Œéœ€åˆ†é˜¶æ®µ
3. å¢é‡æ›´æ–°çš„embeddingå¯¹é½éœ€è¦ç‰ˆæœ¬ç®¡ç†

### 9.2 è‡ªåŠ¨åŒ–è¾¹ç•Œå»ºè®®

| ç¯èŠ‚ | è‡ªåŠ¨åŒ–ç¨‹åº¦ | å»ºè®® |
|------|-----------|------|
| æ•°æ®æ•´åˆ | 100% | âœ… å®Œå…¨è‡ªåŠ¨åŒ– |
| å¤§ç»„èšç±» | 100% | âœ… å®Œå…¨è‡ªåŠ¨åŒ– |
| å¤§ç»„ç­›é€‰ | 60% AI + 40% äººå·¥ | âš ï¸ AIé¢„ç­›é€‰+æ’åºï¼Œäººå·¥æœ€ç»ˆå†³ç­– |
| å°ç»„èšç±» | 100% | âœ… å®Œå…¨è‡ªåŠ¨åŒ– |
| éœ€æ±‚å¡ç‰‡ | 70% AI + 30% äººå·¥ | âš ï¸ AIç”Ÿæˆåˆç¨¿ï¼Œäººå·¥ç²¾ä¿® |
| tokensåº“ | 80% AI + 20% äººå·¥ | âš ï¸ AIæ‰¹é‡åˆ†ç±»ï¼Œäººå·¥æŠ½æ ·éªŒè¯ |
| å¢é‡æ›´æ–° | 95% | âœ… å‡ ä¹å®Œå…¨è‡ªåŠ¨åŒ– |

**å…³é”®åŸåˆ™**:
- æ‰¹é‡ä»»åŠ¡ â†’ è‡ªåŠ¨åŒ–
- åˆ¤æ–­ä»»åŠ¡ â†’ AIè¾…åŠ© + äººå·¥å†³ç­–
- åˆ›æ„ä»»åŠ¡ â†’ äººå·¥ä¸»å¯¼

### 9.3 æ ¸å¿ƒæŠ€æœ¯å»ºè®®

1. **æ•°æ®åº“**:
   - ä½¿ç”¨PostgreSQLï¼ˆæ”¯æŒJSONã€å…¨æ–‡æœç´¢ï¼‰
   - æˆ–SQLiteï¼ˆè½»é‡ã€æ˜“éƒ¨ç½²ï¼‰

2. **Embedding**:
   - æ¨¡å‹: all-MiniLM-L6-v2ï¼ˆå½“å‰ï¼‰
   - å‡çº§: all-mpnet-base-v2ï¼ˆè´¨é‡æ›´é«˜ï¼‰
   - ç¼“å­˜: å¿…é¡»å®ç°

3. **èšç±»å‚æ•°**:
   - å¤§ç»„: ä½¿ç”¨å¯¹æ•°å…¬å¼åŠ¨æ€è®¡ç®—
   - å°ç»„: æ ¹æ®çˆ¶ç»„è§„æ¨¡è‡ªé€‚åº”
   - æ¥å—25-30%çš„å™ªéŸ³ç‡

4. **AIé›†æˆ**:
   - ä¼˜å…ˆä½¿ç”¨GPT-3.5ï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
   - å…³é”®ä»»åŠ¡ç”¨GPT-4ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰
   - å®ç°æ‰¹é‡å¤„ç†é™ä½æˆæœ¬

5. **å¢é‡æ›´æ–°**:
   - ä½¿ç”¨KNNåˆ†é…æ–°è¯
   - å®ç°embeddingç‰ˆæœ¬ç®¡ç†
   - è¿‡æ»¤è§„åˆ™å‡å°‘å¤„ç†é‡

### 9.4 ä¼˜å…ˆçº§å»ºè®®

**å¿…é¡»åšï¼ˆMVPï¼‰**:
1. âœ… æ•°æ®åº“è¿ç§»ï¼ˆä»CSVåˆ°æ•°æ®åº“ï¼‰
2. âœ… å¤§ç»„èšç±»ä¼˜åŒ–ï¼ˆå¯¹æ•°å…¬å¼ï¼‰
3. âœ… å¢é‡æ›´æ–°æœºåˆ¶
4. âœ… AIç”Ÿæˆéœ€æ±‚å¡ç‰‡åˆç¨¿

**å»ºè®®åšï¼ˆæå‡ä½“éªŒï¼‰**:
1. ğŸ”¶ å¤§ç»„é¢„ç­›é€‰è§„åˆ™
2. ğŸ”¶ éœ€æ±‚ç¼–è¾‘UIä¼˜åŒ–
3. ğŸ”¶ tokensè¯åº“ç®¡ç†
4. ğŸ”¶ å¯¼å‡ºå·¥å…·

**å¯é€‰ï¼ˆé”¦ä¸Šæ·»èŠ±ï¼‰**:
1. ğŸ”¹ GPUåŠ é€Ÿ
2. ğŸ”¹ å¤šæ¨¡å‹å¯¹æ¯”
3. ğŸ”¹ è‡ªåŠ¨åŒ–æµ‹è¯•
4. ğŸ”¹ Webç•Œé¢

### 9.5 æœ€åçš„å»ºè®®

**å»ºè®®é‡‡ç”¨çš„å®æ–½ç­–ç•¥**:
1. **å…ˆè¿ç§»ï¼Œåä¼˜åŒ–**: å…ˆæŠŠç°æœ‰CSVæ•°æ®è¿ç§»åˆ°æ•°æ®åº“ï¼Œä¿è¯ä¸ä¸¢å¤±
2. **MVPä¼˜å…ˆ**: å…ˆå®ç°æ ¸å¿ƒæµç¨‹ï¼ˆé˜¶æ®µ0-4ï¼‰ï¼Œå†ä¼˜åŒ–ç»†èŠ‚
3. **å¢é‡å¼€å‘**: æ¯ä¸ªPhaseå®Œæˆåç«‹å³æµ‹è¯•ï¼Œä¸è¦ç­‰å…¨éƒ¨å®Œæˆ
4. **æ–‡æ¡£å…ˆè¡Œ**: æ¯ä¸ªæ¨¡å—å¼€å‘å‰å…ˆå†™æ¸…æ¥šè®¾è®¡æ–‡æ¡£
5. **æŒç»­éªŒè¯**: æ¯è½®è¿­ä»£åï¼Œç”¨çœŸå®æ•°æ®éªŒè¯è´¨é‡

**å¦‚æœèµ„æºæœ‰é™ï¼Œå»ºè®®çš„æœ€å°åŒ–æ–¹æ¡ˆ**:
- Phase 1: æ•°æ®åº“ + æœåŠ¡å±‚ï¼ˆå¿…é¡»ï¼‰
- Phase 2: èšç±»å¼•æ“ä¼˜åŒ–ï¼ˆå¿…é¡»ï¼‰
- Phase 4: åˆæ¬¡æµç¨‹ï¼ˆå¿…é¡»ï¼‰
- å…¶ä»–: å»¶åæˆ–ç®€åŒ–

è¿™æ ·å¯ä»¥åœ¨**2å‘¨å†…**å®ŒæˆMVPï¼Œå¼€å§‹ç¬¬ä¸€æ¬¡å®Œæ•´è¿­ä»£ã€‚

---

**å®¡æŸ¥ç»“æŸ**

è¿™ä»½æŠ€æœ¯å®¡æŸ¥æ¶µç›–äº†æµç¨‹åˆç†æ€§ã€è‡ªåŠ¨åŒ–è¾¹ç•Œã€æ•°æ®è¡¨è®¾è®¡ã€ä»£ç æ¶æ„ã€èšç±»å‚æ•°ã€å¢é‡æ›´æ–°ç­‰æ‰€æœ‰å…³é”®æ–¹é¢ã€‚å¸Œæœ›å¯¹æ‚¨çš„é¡¹ç›®å®æ–½æœ‰å¸®åŠ©ï¼