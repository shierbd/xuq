# 技术实现优先级规划

## 📋 规划目的

防止"一步到胃"，按照轻量版→完整版的思路，分阶段实现代码。

**GPT的警告**：
> 现在文档里虽然有"轻量版执行路径"，但那是面向"操作"的；
> 你后面给 Cloud Code 写指令时，最好再单独有一份
> **"第一批只实现 X/Y/Z 三个脚本"**的小计划，防止又一步到胃。

---

## 🎯 分阶段实施策略

### 总体原则

1. **Phase 1（必须）**：核心聚类功能，确保基础pipeline能跑通
2. **Phase 2（重要）**：方向分析和简易可视化
3. **Phase 3（可选）**：完整功能（大模型、Trends、需求库）

**关键**：每个Phase完成并验证后，再开始下一个Phase

---

## 📦 Phase 1：核心聚类（必须实现）

### 🎯 目标
能够从种子词生成聚类结果，并人工筛选出方向

### 📝 核心脚本

#### 1.1 step_A2_merge.py ✅（已实现）

**功能**：合并多个CSV文件
**输入**：`C:\Users\32941\Downloads\合并\*_broad-match_*.csv`
**输出**：`data/merged_keywords_all.csv`

**当前状态**：✅ 已实现，功能正常

---

#### 1.2 step_A3_clustering.py 🔧（需优化）

**功能**：对短语进行语义聚类

**当前实现的功能**：
- ✅ Embedding生成
- ✅ HDBSCAN聚类
- ✅ 基础输出（cluster_id）

**需要新增的功能**：
```python
# A3.1 预处理
- [ ] 文本标准化（小写、去空格）
- [ ] 去重合并（保留frequency）
- [ ] 标记无效数据（is_invalid）

# A3.2 基础特征
- [ ] word_count（单词数）
- [ ] phrase_length（字符长度）

# A3.3 动态参数
- [ ] 根据数据量计算 min_cluster_size
- [ ] calculate_cluster_params(phrase_count)

# A3.4 输出增强
- [ ] cluster_size 回填到短语表
- [ ] is_noise 标志
- [ ] example_phrases 在summary表中
- [ ] 字段改名：cluster_id → cluster_id_A
```

**优先级**：🔥 立即实现（本周）

**验证标准**：
```bash
# 运行后应该看到：
- stageA_clusters.csv 包含 cluster_id_A, cluster_size, is_noise
- cluster_summary_A3.csv 包含 example_phrases
- 簇数量在 60-100 个之间（6565条短语）
- 噪音点比例 < 25%
```

---

#### 1.3 validation.py ⏳（新建）

**功能**：验证输出文件格式

```python
# validation.py
import pandas as pd

REQUIRED_COLUMNS = {
    "stageA_clusters.csv": [
        "phrase", "seed_word", "cluster_id_A", "cluster_size", "is_noise"
    ],
    "cluster_summary_A3.csv": [
        "cluster_id_A", "cluster_size", "example_phrases",
        "seed_words_in_cluster", "total_frequency"
    ],
}

def validate_csv_columns(file_path, file_type):
    """验证CSV文件的列名是否符合规范"""
    df = pd.read_csv(file_path)
    required = set(REQUIRED_COLUMNS[file_type])
    actual = set(df.columns)
    missing = required - actual

    if missing:
        print(f"❌ {file_type} 缺失字段: {missing}")
        return False
    print(f"✅ {file_type} 验证通过")
    return True

if __name__ == "__main__":
    # 验证所有文件
    files = [
        ("data/stageA_clusters.csv", "stageA_clusters.csv"),
        ("data/cluster_summary_A3.csv", "cluster_summary_A3.csv"),
    ]
    for file_path, file_type in files:
        validate_csv_columns(file_path, file_type)
```

**优先级**：⚠️ 重要（本周）

---

#### 1.4 cluster_stats.py ⏳（新建）

**功能**：聚类结果统计分析

```python
# cluster_stats.py
import pandas as pd

def analyze_clustering_result(clusters_file, summary_file):
    """分析聚类结果质量"""
    clusters = pd.read_csv(clusters_file)
    summary = pd.read_csv(summary_file)

    # 基础统计
    total_phrases = len(clusters)
    noise_count = len(clusters[clusters['cluster_id_A'] == -1])
    noise_ratio = noise_count / total_phrases * 100

    valid_clusters = summary[summary['cluster_id_A'] != -1]
    cluster_count = len(valid_clusters)

    # 簇大小分布
    avg_size = valid_clusters['cluster_size'].mean()
    median_size = valid_clusters['cluster_size'].median()
    max_size = valid_clusters['cluster_size'].max()
    min_size = valid_clusters['cluster_size'].min()

    # 输出报告
    print(f"""
    ========== 聚类结果分析 ==========
    总短语数: {total_phrases}
    有效簇数: {cluster_count}
    噪音点数: {noise_count} ({noise_ratio:.1f}%)

    簇大小统计:
    - 平均: {avg_size:.1f}
    - 中位数: {median_size:.1f}
    - 最大: {max_size}
    - 最小: {min_size}

    质量评估:
    {"✅ 簇数量合理 (60-100)" if 60 <= cluster_count <= 100 else "❌ 簇数量不合理"}
    {"✅ 噪音比例合理 (<25%)" if noise_ratio < 25 else "❌ 噪音比例过高"}
    {"✅ 簇大小合理 (50-150)" if 50 <= avg_size <= 150 else "⚠️ 簇大小需调整"}
    ==================================
    """)

if __name__ == "__main__":
    analyze_clustering_result(
        "data/stageA_clusters.csv",
        "data/cluster_summary_A3.csv"
    )
```

**优先级**：⚠️ 重要（本周）

---

### ✅ Phase 1 完成标准

- [ ] step_A3_clustering.py 完成所有优化
- [ ] validation.py 能验证所有输出文件
- [ ] cluster_stats.py 能生成质量报告
- [ ] 实际运行结果：60-100个簇，噪音<25%
- [ ] 人工查看 example_phrases 字段，能快速理解每个簇

**预计工作量**：1-2天

---

## 📦 Phase 2：方向分析（重要）

### 🎯 目标
能够对选定的方向进行深度分析，生成需求洞察

### 📝 核心脚本

#### 2.1 manual_direction_selector.py ⏳（新建）

**功能**：人工筛选方向的辅助工具

```python
# manual_direction_selector.py
import pandas as pd

def interactive_select_directions(summary_file, output_file):
    """交互式筛选方向"""
    summary = pd.read_csv(summary_file)

    # 显示所有簇
    valid_clusters = summary[summary['cluster_id_A'] != -1]
    valid_clusters = valid_clusters.sort_values('total_frequency', ascending=False)

    print("===== 簇列表（按频次排序）=====")
    for idx, row in valid_clusters.iterrows():
        print(f"\n[{row['cluster_id_A']}] 大小:{row['cluster_size']} 频次:{row['total_frequency']}")
        print(f"种子词: {row['seed_words_in_cluster']}")
        print(f"示例短语: {row['example_phrases'][:200]}...")

    # 让用户输入选择
    selected_ids = input("\n请输入要保留的簇ID（逗号分隔）: ")
    selected_ids = [int(x.strip()) for x in selected_ids.split(',')]

    # 为每个簇生成方向关键词
    directions = []
    for cid in selected_ids:
        row = summary[summary['cluster_id_A'] == cid].iloc[0]
        keyword = input(f"\n簇{cid}的方向关键词（如'compress pdf'）: ")

        directions.append({
            'direction_keyword': keyword,
            'from_cluster_id_A': cid,
            'cluster_size': row['cluster_size'],
            'total_frequency': row['total_frequency'],
            'seed_words_in_cluster': row['seed_words_in_cluster'],
            'is_non_digital_scenario': False,
            'review_decision': '保留',
        })

    # 保存
    pd.DataFrame(directions).to_csv(output_file, index=False)
    print(f"\n✅ 已保存 {len(directions)} 个方向到 {output_file}")

if __name__ == "__main__":
    interactive_select_directions(
        "data/cluster_summary_A3.csv",
        "data/direction_keywords.csv"
    )
```

**优先级**：⚠️ 重要（本周）

---

#### 2.2 step_B1_expand_direction.py ⏳（新建）

**功能**：对选定方向再次扩展短语

**输入**：`direction_keywords.csv`
**输出**：`raw_phrases_stageB.csv`

**说明**：这个脚本需要调用影刀RPA或手动输入数据，Phase 2暂时可以用手动方式

**优先级**：📝 可选（可以先手动准备数据）

---

#### 2.3 step_B3_cluster_stageB.py ⏳（新建）

**功能**：对每个方向内部再次聚类

**核心逻辑**：
```python
# step_B3_cluster_stageB.py
import pandas as pd
from sentence_transformers import SentenceTransformer
import hdbscan

def cluster_by_direction(phrases_file, directions_file, output_file):
    """对每个方向内部聚类"""
    phrases = pd.read_csv(phrases_file)
    directions = pd.read_csv(directions_file)

    model = SentenceTransformer('all-MiniLM-L6-v2')
    all_results = []

    for _, direction in directions.iterrows():
        keyword = direction['direction_keyword']
        print(f"\n处理方向: {keyword}")

        # 筛选该方向的短语
        direction_phrases = phrases[phrases['direction_keyword'] == keyword]

        if len(direction_phrases) < 10:
            print(f"⚠️ 短语数量不足，跳过")
            continue

        # 聚类
        texts = direction_phrases['phrase'].tolist()
        embeddings = model.encode(texts)

        # 动态计算参数
        min_cluster_size = max(5, len(texts) // 20)
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=2
        )
        labels = clusterer.fit_predict(embeddings)

        # 添加结果
        direction_phrases = direction_phrases.copy()
        direction_phrases['cluster_id_B'] = labels
        all_results.append(direction_phrases)

    # 合并保存
    result_df = pd.concat(all_results, ignore_index=True)
    result_df.to_csv(output_file, index=False)
    print(f"\n✅ B3聚类完成，保存到 {output_file}")

if __name__ == "__main__":
    cluster_by_direction(
        "data/raw_phrases_stageB.csv",
        "data/direction_keywords.csv",
        "data/stageB_clusters.csv"
    )
```

**优先级**：⚠️ 重要（下周）

---

#### 2.4 plot_clusters.py ⏳（新建）

**功能**：简易可视化

```python
# plot_clusters.py
import pandas as pd
import matplotlib.pyplot as plt

def plot_cluster_distribution(summary_file):
    """可视化簇大小分布"""
    summary = pd.read_csv(summary_file)
    valid = summary[summary['cluster_id_A'] != -1]

    plt.figure(figsize=(12, 6))

    # 簇大小分布
    plt.subplot(1, 2, 1)
    plt.hist(valid['cluster_size'], bins=20, edgecolor='black')
    plt.xlabel('Cluster Size')
    plt.ylabel('Count')
    plt.title('Cluster Size Distribution')

    # 频次分布（Top 20）
    plt.subplot(1, 2, 2)
    top20 = valid.nlargest(20, 'total_frequency')
    plt.barh(range(len(top20)), top20['total_frequency'])
    plt.yticks(range(len(top20)), [f"C{x}" for x in top20['cluster_id_A']])
    plt.xlabel('Total Frequency')
    plt.title('Top 20 Clusters by Frequency')

    plt.tight_layout()
    plt.savefig('output/cluster_distribution.png')
    print("✅ 图表已保存到 output/cluster_distribution.png")

if __name__ == "__main__":
    plot_cluster_distribution("data/cluster_summary_A3.csv")
```

**优先级**：📝 可选（可视化辅助）

---

### ✅ Phase 2 完成标准

- [ ] 能够人工筛选出5-10个方向
- [ ] direction_keywords.csv 生成成功
- [ ] 至少对1个方向完成B3聚类
- [ ] 能够看到该方向内部的需求分组

**预计工作量**：2-3天

---

## 📦 Phase 3：完整功能（可选）

### 🎯 目标
实现大模型分析、Trends验证、需求库等完整功能

### 📝 扩展脚本

#### 3.1 step_A4_llm_insights.py ⏳（大模型）

**功能**：对每个簇调用大模型生成洞察

**说明**：
- 成本较高，只在确认pipeline有价值后实现
- 需要配置API密钥
- 支持 OpenAI / Claude / DeepSeek

**优先级**：📝 可选（完整版）

---

#### 3.2 step_A5_trends.py ⏳（Trends）

**功能**：获取方向级关键词的趋势数据

**说明**：
- 需要影刀RPA或手动查询
- 可以先用手动方式填入数据

**优先级**：📝 可选（完整版）

---

#### 3.3 step_B5_full_labeling.py ⏳（全量映射）

**功能**：将簇标签映射到所有短语

**说明**：
- 实现复杂度较高
- 需要embedding相似度计算
- 轻量版可以跳过

**优先级**：📝 可选（规模化时）

---

#### 3.4 需求库管理 ⏳（C阶段）

**功能**：Notion / Airtable 集成

**说明**：
- 需要API集成
- 可以先用Excel/CSV管理

**优先级**：📝 可选（长期维护）

---

## 📅 实施时间表

### 本周（2025-12-15 至 2025-12-21）

**Day 1-2：Phase 1核心优化**
- [ ] 优化 step_A3_clustering.py（新增功能）
- [ ] 创建 validation.py
- [ ] 创建 cluster_stats.py
- [ ] 重新运行，验证结果改善

**Day 3-4：Phase 1验证**
- [ ] 人工查看 example_phrases
- [ ] 确认簇数量合理（60-100个）
- [ ] 确认噪音比例合理（<25%）

**Day 5：Phase 2准备**
- [ ] 创建 manual_direction_selector.py
- [ ] 人工筛选5-10个方向
- [ ] 生成 direction_keywords.csv

---

### 下周（2025-12-22 至 2025-12-28）

**Phase 2：方向分析**
- [ ] 为1-2个方向手动准备B1数据
- [ ] 实现 step_B3_cluster_stageB.py
- [ ] 运行B3聚类
- [ ] 人工查看结果

---

### 第三周及以后

**Phase 3（可选）**：
- 根据实际需要决定是否实现
- 优先级：A4 > B2 > A5 > B5 > C

---

## 🎯 关键里程碑

### Milestone 1：核心Pipeline可用（Phase 1完成）
```
输入：merged_keywords_all.csv（6565条）
输出：cluster_summary_A3.csv（60-100个簇，带example_phrases）
人工操作：从summary中筛选5-10个方向
成功标准：人工感觉"好处理多了"
```

### Milestone 2：方向分析可用（Phase 2完成）
```
输入：direction_keywords.csv（5-10个方向）
输出：stageB_clusters.csv（每个方向的内部聚类）
人工操作：查看需求分组，挑选2-3个值得做MVP的
成功标准：能够说出"这个方向的用户要解决什么问题"
```

### Milestone 3：完整功能（Phase 3完成）
```
所有步骤自动化
大模型辅助分析
Trends数据验证
需求库持续积累
```

---

## 📝 注意事项

### 1. 不要贪多

```
❌ 错误做法：
同时实现 A3优化 + A4大模型 + B3聚类 + B5映射
→ 结果：都做了一半，都不能用

✅ 正确做法：
先把 A3 优化到完全可用
→ 验证通过后，再做 manual_direction_selector
→ 再做 B3聚类
```

### 2. 每步都要验证

```
每完成一个脚本，必须：
1. 运行一次
2. 查看输出
3. 人工检查几条数据
4. 用 validation.py 验证
5. 确认无误后，再进行下一步
```

### 3. 保留旧版本

```
# 文件命名示例
step_A3_clustering_v1.py（旧版本，保留）
step_A3_clustering.py（新版本，当前使用）

如果新版本有问题，可以随时回退
```

---

## ✅ 总结

### 核心原则

> **先做少，做精，做通，再扩展**

### 优先级排序

1. 🔥 Phase 1（必须）：核心聚类 + 验证工具
2. ⚠️ Phase 2（重要）：方向筛选 + 内部聚类
3. 📝 Phase 3（可选）：大模型 + Trends + 需求库

### 成功标准

```
Phase 1成功 = 簇数量合理 + 人工能快速理解
Phase 2成功 = 能挑出2-3个值得做MVP的方向
Phase 3成功 = 全流程自动化 + 知识积累
```

---

**立即开始：优化 step_A3_clustering.py**
