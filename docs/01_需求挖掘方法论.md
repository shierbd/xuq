# 需求挖掘流程（单词种子版 · 最终版）

> 目标：从「一批英文单词种子」出发，通过**有限迭代 + 语义聚类 + 大模型分析**，筛出少量**真实可做的需求方向**，而不是堆几万条杂乱短语。

---

## 0. 总体原则

- **统一起点：单词种子**
  - 初始输入是「**单个英文单词**」，例如：`compress` / `convert` / `generate` / `track` / `optimize` 等。
  - 不从短语起步，短语是在后续“单词 → 方向”的过程中自动生成的。
- **两阶段流程**

  - **阶段 A：从单词到方向**
    - 单词种子 → 自动扩展短语 → 语义聚类 → 方向级关键词（如 `compress pdf`、`compress mp3`）。
  - **阶段 B：从方向到需求 & MVP**
    - 方向级关键词 → 深度需求拆解 → 竞品分析 → 模式聚类 → 选方向 → MVP 验证。

- **有限迭代**
  - 「扩展 → 拆解 → 分析」整体最多 **1–2 轮**；
  - 每一轮开始前，先写一句话：这轮是为了**选方向 / 看场景 / 看人群 / 看属性**中的哪一个。
- **分工明确**
  - **影刀 RPA**：负责 Google 搜索、Google Trends、下拉词、相关搜索、SERP 竞品结果的自动采集。
  - **Python**：负责数据清洗、去重、频次统计、embedding 计算、聚类、可视化草图、报表导出。
  - **大模型（GPT / Claude / Gemini / DeepSeek 等）**：
    - 负责对「代表样本 / 聚类簇 / SERP 结果 / 访谈记录」做深度理解与结构化总结；
    - 不负责扫全量，只做“理解 + 抽象 + 模式发现”。
  - **Power BI / Python 可视化**：
    - 用来做趋势/模式的**探索式分析**，帮助你“看清方向”，而不是自动做决策。

- **语义聚类为主，规则为辅**
  - 主体：使用 embedding + 聚类 + 大模型 summary 来概括模式；
  - 仅保留少量"硬规则"（如 `for whatsapp` / `for tiktok` / `online free` 等）作为补充标签，而不是依赖大规模规则引擎。

---

## 📋 实施优先级指南

**重要提醒**：本文档描述的是**完整版流程**，适合长期维护的成熟项目。如果是**第一次使用**，强烈建议从**轻量版**起步，避免被复杂度压垮。

### 🚀 轻量版执行路径（推荐首次使用）

**第一轮：验证"单词种子 → 方向"链路**

```
选择：
- 5–10 个单词种子
- 每个种子最多抓 100 条短语（A2）

只做：
- A2：raw_phrases_stageA.csv
- A3：短语聚类 + cluster_summary_A3.csv
- 跳过 A4 全簇大模型
- A5：不跑 Trends，先人工看 cluster_summary_A3
      手动写 5–10 个方向级关键词（相当于自己做了简化版 A4+A5）

得到：direction_keywords.csv（手写也可以）
```

**第二轮：直接进入 B1 + B3 + B6**

```
- B1：针对每个方向再扩 100–200 条短语
- B3：在每个方向内部聚类 + 5 维需求标注
- B6：用简单的 Python 图/表看 pattern 热度，不一定非要 Power BI

得到：
- 一批从"单词"自然长出来的方向
- 每个方向下若干"what/why/who"的结构化洞察
- 至少能挑出 2–3 个你愿意做 MVP 的方向
```

**等轻量版跑通了，再逐步补上**：
- SERP 深度分析（B2、B7）
- Trends 指标（A5、B6）
- 全量标注（B5）
- 需求库（C1/C2）

### 📚 完整版执行路径（长期维护时）

**阶段 A 完整版**：
- A1 → A2 → A3 → **A4**（大模型过所有簇）→ **A5**（Trends 验证）
- 适用场景：这套 pipeline 已被证明有价值，需要规模化和自动化

**阶段 B 完整版**：
- B1 → **B2**（SERP 竞品）→ B3 → B4 → **B5**（全量映射）→ B6 → **B7**（访谈）→ **B8**（MVP）

**关键差异**：
- 完整版会对所有簇调用大模型（A4），成本较高
- 完整版会做多层聚类（A3 + B3 + B5），实现复杂度高
- 完整版会做深度竞品和用户访谈（B2、B7），时间投入大

### ⚠️ 复杂度警告

**阶段 A 的复杂度**：
- 如果你有 6,565 条短语、248 个簇（参数未优化前）
- A4 要对每个簇调用大模型做 JSON 总结
- A5 还要对每个方向跑 Trends
- **成本 + 时间都会很感人**

**建议**：
- ✅ Pilot 版：只执行到 A3，人工过 cluster_summary_A3，选少量簇，直接跳 B1
- ✅ 完整版：只在确认这套 pipeline 值得长期维护时再开

---

## 阶段 A｜从单词到方向（Word → Direction）

### 步骤 A1｜准备单词种子

**目标**：确定一批“能发散出很多数字产品场景”的英文单词。

**输入形式**

- 单个英文单词（动词/名词均可），例如：
  - 动词：`compress`、`convert`、`edit`、`generate`、`track`、`optimize`、`analyze` …
  - 名词：`audio`、`video`、`image`、`pdf`、`resume` …

**操作**

1. 根据经验/直觉，列出 **20–50 个单词种子**。
2. 统一格式：
   - 全部小写；
   - 去掉无意义符号；
   - 保留对含义重要的核心词。
3. 在表格中为每个单词添加两个辅助字段：
   - `word_type`：`verb` / `noun` / `other`；
   - `note`：你对这个词的简单直觉说明（比如“和压缩/优化相关”）。

**输出文件**

- `seed_words.csv`
  - 示例字段：
    - `seed_word`
    - `word_type`
    - `note`

---

### 步骤 A2｜单词扩展：从词到短语

**目标**：让每个单词“长出”大量真实搜索短语，为后续聚类提供原料。

**工具**

- Google 搜索 / Google Trends
- 影刀 RPA：自动完成输入、滚动、抓取下拉和相关搜索
- Python：合并、去重、统计

**操作**

1. 使用影刀，对每个 `seed_word` 执行：
   - 在 Google 搜索框输入该单词；
   - 捕获**下拉 Autocomplete** 列表；
   - 进入搜索结果页，抓取底部 **Related Searches**；
   - 如需要，也可在 Trends 的相关查询中抓短语。
2. 写入 CSV，记录来源：
   - `source_type` 可取：
     - `autocomplete_main`
     - `related_searches`
     - `trends_related`（如使用）
3. 用 Python 合并所有 `seed_word` 的短语：
   - 统一转小写；
   - 去掉重复、无意义符号；
   - 统计每条短语的出现次数 `frequency`（跨种子词/来源出现多次就累加）。

**数据量控制（重要）**：
- **每个种子词抓取上限**：建议 `max_phrases_per_seed = 100–200` 条
- **首次运行建议**：选择 5–10 个种子词，每个抓取 100 条，避免数据量过大导致后续聚类成本过高
- **参数配置**：在 `config.py` 中设置 `A2_CONFIG = {"max_phrases_per_seed": 150}`

**输出文件**

- `raw_phrases_stageA.csv`
  - 示例字段：
    - `phrase`
    - `seed_word`
    - `source_type`
    - `frequency`

---

### 步骤 A3｜短语聚类：统一格式 + 聚类 + 打标签（不做提前取舍）

> **说明**：
> A1、A2 都是人工/影刀操作，把所有拓出来的下拉词、相关词最终合并成一个表。
> 从 A3 开始，才进入「Python + 模型」阶段，对这些短语做**格式统一 + 向量化 + 聚类 + 打标签**。
> A3 **只负责"整理和分桶"，不负责"删掉哪一桶"** —— 留还是不留，后面由人工决定。

**目标**：把从单词扩展出来的一大堆短语，通过预处理、向量化、聚类，变成带标签的「全量分组视图」，供后续人工筛选。

**工具**

- Python：
  - 文本预处理（小写、去重、标准化）；
  - embedding（sentence-transformers / OpenAI embedding）；
  - 聚类（HDBSCAN 推荐，可选 KMeans）。

---

#### A3.1 预处理：统一格式，但不丢行

**操作**

1. **文本标准化**（不删除任何短语）：
   - 全部转小写；
   - 去掉首尾空格；
   - 合并连续空格为一个空格。
2. **去重合并**：
   - 如果完全相同的 `phrase` 出现多次，合并为一行；
   - 累加 `frequency` 字段；
   - 其它来源信息可以合并成列表（如 `source_type_list`、`seed_word_list`）。
3. **标记无效数据**（保留但标记）：
   - 对于空字符串、只有标点等明显无效的记录；
   - 不删除，而是添加字段 `is_invalid = True`。

**关键原则**：
- ✅ 可以修改文本格式（小写、空格）；
- ✅ 可以合并完全重复的短语（保留频次信息）；
- ❌ 不允许因为语义判断去删行（如"看起来像工作岗位就删掉"——不在 A3 做）。

---

#### A3.2 向量化（Embedding）与基础特征

**操作**

1. **生成 Embedding**：
   - 对 `phrase` 列使用同一套 embedding 模型（如 sentence-transformers）；
   - 得到 `embedding_vector`（内部使用，可单独保存为 pkl/h5）。

2. **计算辅助特征**（只是标签，不决定删留）：
   - `word_count`：按空格分词的单词数；
   - `phrase_length`：字符长度；
   - `has_question_word`（可选）：是否包含 what/how/why 等；
   - `has_best_or_top`（可选）：是否包含 best/top/how-to 等修饰词。

---

#### A3.3 聚类策略（HDBSCAN 或其他）

**目标**：基于 embedding，把语义相近的短语归到同一个 cluster_id 下；噪音点统一标记为 `cluster_id = -1`，**依旧保留在表里**。

**操作**

1. **执行聚类**（推荐 HDBSCAN）：
   - `min_cluster_size`：10–20（对于 6,565 条数据，从 10–15 起步）；
   - `min_samples`：2–3。
2. **聚类结果**：
   - 每条短语得到一个 `cluster_id`（整数，-1 表示噪音）；
   - 对应 cluster 的 `cluster_size` 在 post-process 时算出。

**重要说明**：
- `cluster_id = -1` 只是 HDBSCAN 的"噪音标签"，**A3 不会删这些行**；
- 后面可以：
  - 单独查看噪音短语；
  - 决定是再做二次聚类，还是人工判定"这一块全部砍掉"。

---

#### A3.4 聚类结果整理 & 打标签（仍然不删，只是方便后面选择）

**短语级表整理**

对每条短语：
- 写入 `cluster_id`；
- 计算并回填 `cluster_size`（按 cluster_id 分组后回填）；
- 添加 `is_noise` 标志（cluster_id == -1）。

可选自动标签（举例）：
- `query_type`：
  - if 以 "how to" 开头 → `"tutorial"`
  - if 开头含 "best" / "top" → `"best_list"`
  - 否则 → `"normal"`
- `is_question_like`：是否为问句（包含 what/how/why 等）

**簇级 summary 表整理**

按 `cluster_id` 分组，生成 `cluster_summary_A3.csv`：
- `cluster_id`
- `cluster_size`
- `example_phrases`：该簇里 5–10 条代表短语（频次高/中/低混着取）
- `seed_words_in_cluster`：用逗号拼接该簇中出现过的 seed_word 去重集合
- `avg_search_volume` / `total_search_volume` / `total_frequency`（如果有数据）
- `noise_flag`（cluster_id == -1）
- `auto_comment`（可选：由简单规则或模型给出简短描述，但不做决策）
- `notes_for_human_review`：留给人工备注的空列

> **后面的"取舍动作"**，全部基于这张簇级 summary 表来做：
> 比如：
> - "这个簇全是 job/degree 相关，我整个簇都暂时不深入"；
> - "这个簇全是工具/生成器相关，我下轮重点拆"。
>
> 这些决策不属于 A3 的职责，A3 只负责给你这个"地图"。

---

**A3 的边界（人工决策的地方）**

- A3 **不做**：
  - 不按照"搜索量/频次/类型"等直接删除短语；
  - 不按照"job / 教育 / 信息型"自动丢弃任何一类；
  - 不根据"是不是你感兴趣的方向"自动过滤，只负责给你视图。

- A3 **只做**：
  - 把 A2 合并出来的所有短语 →
    **统一格式 → 向量化 → 聚类 → 每条贴上 cluster_id + 辅助标签 → 生成簇级 summary**。

> 换句话说：
> A3 给你的是一个"全量分组视图 + 若干辅助标签"，
> **真正的"留/弃、深挖/不挖"都是后续人工决策，不提前替你做选择。**

---

**输出文件**

1. **短语级表**：`phrases_with_clusters.csv`
   - 示例字段：
     - `phrase`
     - `seed_word`
     - `source_type`
     - `search_volume`（如有）
     - `frequency`
     - `cluster_id`：聚类编号（-1 表示噪音簇）
     - `cluster_size`：该 cluster 内短语数量
     - `is_noise`：是否为噪音点（cluster_id == -1）
     - `phrase_length`：字符长度
     - `word_count`：单词数量
     - `query_type`（可选）：tutorial / best_list / normal
     - `is_question_like`（可选）：是否问句

2. **簇级汇总表**：`cluster_summary_A3.csv`
   - 示例字段：
     - `cluster_id`
     - `cluster_size`
     - `example_phrases`：该簇里 5–10 个代表短语
     - `seed_words_in_cluster`：该簇中涉及到的种子词集合
     - `avg_search_volume` / `total_search_volume`（如果有）
     - `avg_frequency` / `total_frequency`
     - `noise_flag`：是否为噪音簇（通常 -1 专用）
     - `notes_for_human_review`：留空，后面人工填写（如"这簇是 job 类，可以弃用"）

---

### 步骤 A4｜簇级理解：用大模型给“簇”命名与归纳

**目标**：把每个 `cluster_id` 变成一个人类可读的“方向描述”，而不是仅仅是一堆短语和向量。

**工具**

- 大模型（GPT / Claude / Gemini / DeepSeek 等）
- Python：循环调用，将结果整合回 CSV

**操作**

1. 对每个 `cluster_id`：
   - 把该簇内的**全部短语**（或抽样 50–100 条代表性的）发送给大模型；
   - 要求大模型输出一个 JSON，例如：

   ```json
   {
     "cluster_id": "xxx",
     "cluster_title": "PDF 文件压缩与体积减小",
     "short_label": "compress_pdf",
     "scope_description": "主要是用户希望压缩 PDF 文档体积，用于上传、分享、邮件发送等场景。",
     "exclude_reasons": ["与医学压迫疗法无关", "与物理压缩空气/弹簧无关"]
   }

	2.	将所有簇的结果汇总成：
	•	cluster_insights_stageA.csv

**重要说明（理解大模型输出的边界）**：
- ⚠️ **大模型输出是"假设"而非"事实"**：大模型会基于搜索短语推测用户意图，但这只是一种合理的假设，并非经过验证的真实场景
- ⚠️ **需要人工验证**：对于关键决策（如选择哪个方向做MVP），不能仅依赖模型的 `scope_description`，必须结合：
  - SERP 竞品分析（B2）
  - 实际用户访谈（B7）
  - 你自己对该领域的理解
- ✅ **最佳使用方式**：把 A4 的输出当作"快速理解簇语义的辅助视图"，而不是"确定的需求定义"

输出文件
	•	cluster_insights_stageA.csv
	•	示例字段：
	•	cluster_id
	•	cluster_title（人类可读的方向名）
	•	short_label（简短标签，如 compress_pdf、compress_audio）
	•	scope_description
	•	seed_words_in_cluster
	•	total_frequency
	•	cluster_size

⸻

步骤 A5｜方向筛选：从簇到"方向级关键词"

目标：从若干簇中挑出数量有限、值得继续深挖的"方向级关键词"。

工具
	•	Google Trends（用于验证方向级关键词的趋势）
	•	影刀 RPA：自动查询 Trends
	•	Python：汇总指标

操作
	1.	为每个簇生成 1–2 个代表性的"方向级关键词"：
	•	如簇标题是"PDF 文件压缩"，则代表词可为：
	•	compress pdf
	•	pdf compressor
	2.	使用影刀 + Trends，对每个方向级关键词获取趋势数据：
	•	统一时间范围与地区；
	•	计算 avg_score_12m、trend_direction 等。
	3.	合并信息：
	•	将方向级关键词与 cluster_insights_stageA.csv 合并；
	•	得到每个方向的：
	•	频次规模；
	•	趋势强度；
	•	大致语义范围。
	4.	标注与排序（示例维度）：
	•	按 total_frequency 排序，标注热度等级；
	•	avg_score_12m 不为 0；
	•	trend_direction != 'down'；
	•	**标记为低优先级方向**：对明显"非数字产品场景"的簇（例如纯医学、纯物理），在 `is_non_digital_scenario` 字段标记为 `True`，但**不删除**，由人工在 `review_decision` 列决定是否暂缓/保留/丢弃。

**A5 的重要原则**：
- ✅ A5 可以做：标注、排序、计算指标、提供多维度视图
- ❌ A5 不做：自动删除任何簇或方向，所有取舍决策由人工完成
- 所有方向都保留在输出文件中，通过标签字段（如 `is_non_digital_scenario`）和人工决策列（`review_decision`）来管理

输出文件
	•	direction_keywords.csv
	•	示例字段：
	•	direction_keyword
	•	from_cluster_id
	•	cluster_title
	•	short_label
	•	avg_score_12m
	•	trend_direction
	•	total_frequency
	•	cluster_size
	•	is_non_digital_scenario（新增）：是否为非数字产品场景
	•	review_decision（新增）：人工决策列（空/暂缓/保留/丢弃）

阶段 A 的结果：
从“单词种子”变成 10–20 个“方向级关键词”（direction_keyword），
下一阶段所有深挖工作都围绕这些方向展开。

⸻

阶段 B｜从方向到需求 & MVP（Direction → Needs & MVP）

阶段 B 的流程基于“方向级关键词”，并使用简化的 5 维需求框架。

⸻

步骤 B1｜针对方向级关键词再扩展短语

目标：在每个确定的方向内部，做一次更聚焦的短语扩展，作为需求挖掘主体数据。

工具
	•	Google 搜索 / Trends
	•	影刀 RPA
	•	Python

操作
	1.	对 direction_keywords.csv 中每个 direction_keyword：
	•	再执行一次下拉 + Related 搜索扩展；
	•	每个方向收集 100–300 条相关短语。
	2.	清洗、去重、统计频次：
	•	写为 raw_phrases_stageB.csv。

输出文件
	•	raw_phrases_stageB.csv
	•	示例字段：
	•	phrase
	•	direction_keyword
	•	source_type
	•	frequency

⸻

步骤 B2｜竞品快照（SERP 快速扫描）

目标：了解每个方向上现有解决方案的类型、优缺点和市场拥挤程度。

工具
	•	Google SERP（前 10–20 个结果）
	•	影刀 RPA：抓取标题、URL、摘要、评分等
	•	大模型：总结竞品情况与共同缺陷

操作
	1.	对每个 direction_keyword：
	•	抓取 SERP 前 10–20 条结果：
	•	title、url、snippet，如有 rating、review_count 也记录。
	2.	把同一方向的 SERP 结果丢给大模型，要求输出 JSON：

{
  "direction_keyword": "compress pdf",
  "solution_types": ["online web tool", "desktop app"],
  "common_strengths": ["免费", "无需注册", "操作简单"],
  "common_weaknesses": ["广告多", "压缩质量不稳定", "文件大小有限制"],
  "market_maturity": "normal"
}


	3.	汇总所有方向的竞品数据为 serp_summary.csv。

输出文件
	•	serp_snapshot_stageB.csv
	•	direction_keyword
	•	rank
	•	title
	•	url
	•	snippet
	•	rating（可选）
	•	review_count（可选）
	•	serp_summary.csv
	•	direction_keyword
	•	solution_types
	•	common_strengths
	•	common_weaknesses
	•	market_maturity（overcrowded / normal / room_to_play）

⸻

步骤 B3｜代表短语聚类 + 5 维需求框架标注

目标：在每个方向中，用简化的 5 维框架，理解“用户真正想干嘛”。

5 维需求框架

对每个簇，输出一条 JSON：

{
  "cluster_id": "xxx",
  "cluster_title": "给这个簇起的直观名字",
  "who": "用户角色（developer / marketer / student / creator / seller / 普通用户…）",
  "what": "他们想完成的核心任务（用一句话描述）",
  "why": "现有方案的问题或痛点（为什么不满意现在的做法）",
  "how_now": "现在通常是如何解决的（用什么工具、流程、变通方案）",
  "quality_bar": ["他们在意的属性：快速 / 无损 / 批量 / 免费 / 自动化 / 安全 / 无水印…"],
  "example_phrases": ["该簇代表性的搜索短语1", "搜索短语2", "..."],
  "possible_monetization": ["SaaS", "info_product", "plugin", "service"]
}

工具
	•	Python：对 raw_phrases_stageB 做 embedding + 聚类；
	•	大模型：对每个 cluster 做 5 维需求框架总结。

操作
	1.	对 raw_phrases_stageB.csv：
	•	按 direction_keyword 分组；
	•	每个方向内部计算 embedding → 聚类，得到 cluster_id。
	2.	对每个 cluster_id：
	•	把该簇内短语（或抽样）丢给大模型；
	•	按上述 5 维需求框架生成 JSON。
	3.	将结果写为 cluster_insights_stageB.csv。

输出文件
	•	cluster_insights_stageB.csv
	•	direction_keyword
	•	cluster_id
	•	cluster_title
	•	who
	•	what
	•	why
	•	how_now
	•	quality_bar
	•	example_phrases
	•	possible_monetization
	•	cluster_size
	•	total_frequency

**重要说明（理解 5 维框架的边界）**：
- ⚠️ **who / why / how_now 是推测而非确认**：仅凭搜索短语，大模型对"谁在搜""为什么搜""现在怎么解决"的判断往往是合理推测，但可能与真实用户场景有偏差
- ⚠️ **possible_monetization 是参考而非强标签**：变现方式高度依赖你的资源、能力和对赛道的理解，模型给出的只是"可能的匹配方式"
- ✅ **决策优先级**：真正做产品决策时，优先级应该是：
  1. SERP 竞品分析（B2）+ 用户访谈（B7）
  2. BI 图表（B6）看量级和趋势
  3. 你自己的经验与资源评估
  4. 模型的 5 维标注（作为辅助参考）

⸻

步骤 B4｜模式抽象：从簇到“需求模式”

目标：在多个方向之间抽象出更高一层的“需求模式”，例如“无损压缩”“跨平台格式转换”“AI 代写文案”等。

工具
	•	大模型：对所有 cluster_insights_stageB 做二次归纳。
	•	Obsidian / Notion：保存“模式字典”。

操作
	1.	将 cluster_insights_stageB.csv 交给大模型：
	•	按 what + why + quality_bar + who 维度，归纳出 10–30 个模式。
	2.	为每个模式输出：

{
  "pattern_id": "P01",
  "pattern_title": "无损压缩与体积减小",
  "typical_users": ["普通用户", "office工作人员"],
  "core_pains": ["文件过大无法上传", "压缩后质量严重下降"],
  "quality_focus": ["无损或低损", "稳定", "支持大文件"]
}


	3.	为每个 cluster_id 关联一个 pattern_id。

输出文件
	•	pattern_dictionary.csv
	•	pattern_id
	•	pattern_title
	•	typical_users
	•	core_pains
	•	quality_focus
	•	更新版 cluster_insights_stageB.csv：
	•	增加字段：
	•	pattern_id
	•	pattern_title

⸻

步骤 B5｜全量短语标注：模式迁移 + 少量硬规则

目标：不对每条短语再问大模型，而是将“簇标签（pattern）”映射到所有短语上。

工具
	•	Python：
	•	embedding；
	•	最近簇匹配；
	•	简单规则补充标签（平台/是否免费等）。

操作
	1.	对 raw_phrases_stageB.csv 中的全部 phrase：
	•	计算 embedding；
	•	基于向量相似度，将每条短语映射到一个最近的 cluster_id；
	•	同时记下 similarity_score。
	2.	从该 cluster_id 继承：
	•	who / what / why / how_now / quality_bar；
	•	pattern_id / pattern_title；
	•	possible_monetization。
	3.	额外使用少量“硬规则”识别：
	•	平台类：
	•	包含 whatsapp / wechat / tiktok / youtube 等；
	•	属性类：
	•	free / online / no login / without watermark 等；
	•	将这些规则的结果合并到 quality_bar 或单独的 platform_tag 字段。
	4.	增加置信度字段：
	•	mapping_confidence：根据 similarity_score + cluster_size + frequency 等，粗分为 high / medium / low。
	5.	对 mapping_confidence = low 且 frequency 较高的短语：
	•	单独输出一个“疑难样本列表”，后续可以再用大模型人工处理。

输出文件
	•	full_labeled_phrases.csv
	•	示例字段：
	•	phrase
	•	direction_keyword
	•	frequency
	•	cluster_id
	•	cluster_title
	•	pattern_id
	•	pattern_title
	•	who
	•	what
	•	why
	•	how_now
	•	quality_bar
	•	possible_monetization
	•	platform_tag（可选）
	•	similarity_score
	•	mapping_confidence

⸻

步骤 B6｜需求地图（Power BI / Python 可视化）

目标：通过图表“看全局”，但决策仍然由「图表 + 竞品 + 直觉」共同完成。

工具
	•	Power BI Desktop 或 Python 可视化
	•	数据源：
	•	full_labeled_phrases.csv
	•	direction_keywords.csv
	•	serp_summary.csv

操作
	1.	导入并关联数据（按 direction_keyword 关联）。
	2.	设定字段：
	•	维度：
	•	pattern_title
	•	direction_keyword
	•	who
	•	possible_monetization
	•	market_maturity
	•	数值：
	•	frequency
	•	avg_score_12m
	•	similarity_score
	3.	建立几个视图（示例）：
	•	模式热度矩阵
	•	行：pattern_title
	•	列：possible_monetization
	•	值：SUM(frequency) 或 SUM(frequency * avg_score_12m)
	•	趋势 vs 需求量 散点图
	•	X：方向的 avg_score_12m
	•	Y：按 pattern_id 聚合后的 SUM(frequency)
	•	颜色：market_maturity（overcrowded / normal / room_to_play）
	•	用户角色视图
	•	切片器：who
	•	显示不同角色下的主要 pattern_title 和 direction_keyword。
	4.	利用这些视图 + serp_summary 中的竞品结论：
	•	选出 3–5 个候选方向，记录：
	•	模式名称；
	•	核心场景；
	•	核心痛点；
	•	市场拥挤度；
	•	初步变现方式判断。

输出
	•	一组 BI 报表页面（或 Python 图）；
	•	一份「候选方向清单」，用于进入 MVP 设计。

⸻

步骤 B7｜可选：小规模用户访谈与深度竞品分析

目标：对最有潜力的 2–3 个方向多做一点“现实世界验证”。

工具
	•	手动访谈 / 问卷
	•	大模型：整理访谈记录
	•	竞品网站、应用商店、社区（评论/帖子）

操作
	1.	为每个候选方向整理一页“问题说明”：
	•	谁在用（who）；
	•	要做什么（what）；
	•	为何不爽（why）；
	•	现在怎么解决（how_now）；
	•	期望标准（quality_bar）。
	2.	找 5–10 个符合画像的真实用户：
	•	了解他们现在用哪些工具；
	•	何时会愿意换工具或付费；
	•	你的设想是否有吸引力。
	3.	把访谈记录交给大模型，总结：
	•	需求强度（强/中/弱）；
	•	付费意愿；
	•	功能优先级建议。
	4.	再做一轮目标竞品功能/价格/差评分析，整理出：
	•	我们可以差异化的地方；
	•	哪些点千万不能比竞品更差。

输出
	•	每个方向一份《访谈 + 竞品深度报告》：
	•	用户反馈结论；
	•	需求强度；
	•	付费意愿；
	•	产品机会点。

⸻

步骤 B8｜MVP 实验与决策闭环

目标：把“理论上可做的需求方向”变成“经过实测的机会或坑”。

工具
	•	简易网站 / 工具页 / 原型
	•	GA / 埋点统计 / 后端日志
	•	大模型：分析实验数据与用户反馈

操作
	1.	为每个最终要测的方向写一份 MVP 实验卡：
	•	方向名称
	•	目标用户（who）
	•	核心任务（what）
	•	核心痛点（why）
	•	当前解决方式（how_now）
	•	我们的 MVP 能力边界
	•	质量标准（quality_bar 中最关键的 2–3 条）
	•	关键指标：
	•	访问量；
	•	完整使用次数；
	•	留邮箱 / 注册；
	•	付费/打赏/预约等信号。
	2.	按“最小可行”的原则构建 MVP：
	•	功能越少越好，但必须能完成一次完整任务；
	•	可以用 no-code / low-code / 简易脚本实现。
	3.	上线并收集数据：
	•	各环节转化率；
	•	用户停留、流失节点；
	•	用户主动反馈（邮件、表单、留言）。
	4.	把数据 + 用户反馈交给大模型整理成一份简报：
	•	总结这个方向是“值得继续”还是“暂缓/放弃”；
	•	给出下一步改进建议（功能、定位或定价）。

输出
	•	每个方向一份《MVP 实验报告》：
	•	实验数据；
	•	结论（继续 / 暂缓 / 放弃）；
	•	下一步建议。

⸻

阶段 C｜需求知识库 & 方法论迭代

这部分保证：你做的每一次挖掘和实验，都不会白费；
变成一个可复用、可学习的「需求资产库」。

⸻

步骤 C1｜建立“需求库”

目标：把从“单词 → 方向 → 模式 → MVP”全过程中的关键信息，全部放进一个统一数据库中。

工具
	•	Notion / Obsidian / Airtable / 飞书多维表格等

建议字段结构
	•	基本信息：
	•	需求ID
	•	pattern_id
	•	pattern_title
	•	direction_keyword
	•	seed_words（来源单词集合）
	•	需求画像：
	•	who
	•	what
	•	why
	•	how_now
	•	quality_bar
	•	possible_monetization
	•	市场与竞品：
	•	avg_score_12m
	•	trend_direction
	•	market_maturity
	•	serp_summary_link
	•	实验与状态：
	•	status：
	•	发现中 / 验证中 / 已验证可行 / 已放弃 / 暂停观察
	•	mvp_experiment_links（指向实验报告）
	•	key_learnings（关键经验）

⸻

步骤 C2｜定期复盘 & 方法论更新

目标：让这套“从词到需求”的流程，随着实践不断升级，而不是一成不变。

工具
	•	大模型：总结需求库中已积累的信息
	•	你的主观判断与方向规划

操作
	1.	每隔 1–3 个月，将需求库中最新数据导出给大模型：
	•	让它总结：
	•	哪类 pattern 成功率更高；
	•	哪类 pattern 一再失败；
	•	哪些阈值设得太高/太低（例如 Trends 阈值、频次阈值）。
	2.	综合这些结论，更新本文件：
	•	记录为《需求挖掘流程 – v2 / v3 / …》；
	•	标明更新点（调整了哪些阈值、删减了哪些步骤、加强了哪些分析）。

输出
	•	一个带版本号的《需求挖掘流程》系列文档；
	•	一套越来越贴合你实际赚钱目标的“需求挖掘 Playbook”。

⸻


