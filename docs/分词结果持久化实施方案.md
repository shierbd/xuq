# 分词结果持久化功能实施方案

## 用户需求

用户希望分词结果能够自动保存到数据库，避免每次都需要重新点击"开始分词"按钮。

**核心要求：**
1. 分词结果（单词+短语）保存到数据库
2. 页面打开时自动加载已有的分词结果并显示
3. 只有当数据库有新数据时，才提示用户"建议重新分词"
4. 用户可以选择"重新分词"（全量）或"增量分词"（只处理新数据）

## 实施方案

### Phase 1: 数据库结构扩展 ✅

**已完成修改：**
1. `storage/models.py` - WordSegment模型
   - 扩展 `word` 字段：VARCHAR(100) → VARCHAR(255)
   - 新增 `word_count` 字段：INT（1=单词，>1=短语）
   - 添加索引：`ix_word_segments_word_count`

2. `scripts/migrate_add_word_count.py` - 数据库迁移脚本
   - 已成功执行，更新了4617条记录

### Phase 2: Repository扩展 ✅

**已完成修改：**
1. `storage/word_segment_repository.py`

**新增方法：**
```python
def load_segmentation_results(
    self,
    min_word_frequency: int = 1,
    min_ngram_frequency: int = 1
) -> Tuple[Counter, Counter, Dict, Dict, Dict, SegmentationBatch]:
    """
    从数据库加载完整的分词结果（单词+短语）

    Returns:
        (
            word_counter: {word: frequency},
            ngram_counter: {ngram: frequency},
            pos_tags: {word: (pos_tag, pos_category, pos_chinese)},
            translations: {word: translation},
            ngram_translations: {ngram: translation},
            latest_batch: SegmentationBatch对象（最新批次信息）
        )
    """
```

**修改方法：**
```python
def save_word_segments(
    self,
    word_counter: Counter,
    pos_tags: Optional[Dict[str, Tuple[str, str, str]]] = None,
    translations: Optional[Dict[str, str]] = None,
    batch_id: Optional[int] = None,
    ngram_counter: Optional[Counter] = None,  # 新增
    ngram_translations: Optional[Dict[str, str]] = None  # 新增
) -> Tuple[int, int]:  # 返回 (新增单词数, 新增短语数)
    """
    保存或更新分词结果（支持单词和短语）
    """
```

### Phase 3: UI修改（进行中）

#### 3.1 自动加载已有分词结果

**修改位置：** `ui/pages/phase0_expansion.py` - `render_segmentation_tab()` 函数

**实施步骤：**

1. 在Session State初始化时，尝试从数据库加载已有分词结果
2. 如果数据库中有分词结果，自动加载并显示
3. 显示"上次分词时间"和"分词数据统计"

**代码框架：**
```python
# 在Session State初始化之后添加
if 'segmentation_loaded' not in st.session_state:
    st.session_state.segmentation_loaded = False

# 如果还没加载过，尝试从数据库加载
if not st.session_state.segmentation_loaded:
    with WordSegmentRepository() as ws_repo:
        # 检查数据库是否有分词结果
        stats = ws_repo.get_statistics()
        if stats['total_words'] > 0:
            # 加载分词结果
            (
                word_counter,
                ngram_counter,
                pos_tags,
                translations,
                ngram_translations,
                latest_batch
            ) = ws_repo.load_segmentation_results()

            # 更新Session State
            st.session_state.word_counter = word_counter
            st.session_state.ngram_counter = ngram_counter
            st.session_state.pos_tags = pos_tags
            st.session_state.translations = translations
            st.session_state.ngram_translations = ngram_translations
            st.session_state.segmentation_loaded = True

            # 显示加载信息
            st.success(f"✓ 已加载上次分词结果：{len(word_counter)} 个单词 + {len(ngram_counter)} 个短语")
            if latest_batch:
                st.info(f"📅 上次分词时间: {latest_batch.batch_date}")
```

#### 3.2 检测新数据提示

**实施逻辑：**
- 对比数据库中的 `phrases` 表总数 vs 上次分词时处理的短语数
- 如果有新增关键词，显示提示框："检测到 XXX 条新关键词，建议重新分词"

**代码框架：**
```python
# 在加载分词结果后，检查是否有新数据
if st.session_state.segmentation_loaded and latest_batch:
    with PhraseRepository() as phrase_repo:
        current_phrase_count = phrase_repo.get_statistics()['total_count']
        last_phrase_count = latest_batch.phrase_count

        if current_phrase_count > last_phrase_count:
            new_count = current_phrase_count - last_phrase_count
            st.warning(f"⚠️ 检测到 {new_count} 条新关键词，建议重新分词以获取最新结果")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("🔄 全量重新分词"):
                    # 清空session state，触发重新分词
                    st.session_state.word_counter = None
                    st.session_state.ngram_counter = None
                    st.session_state.segmentation_loaded = False
                    st.rerun()
            with col2:
                if st.button("➕ 增量分词（仅新数据）"):
                    # TODO: 实现增量分词
                    st.info("增量分词功能开发中...")
```

#### 3.3 分词完成后自动保存

**修改位置：** 分词按钮的回调逻辑（第429-483行）

**实施步骤：**
1. 分词完成后，调用 `ws_repo.save_word_segments()`
2. 创建或更新批次记录（`SegmentationBatch`）
3. 显示保存成功提示

**代码框架：**
```python
if st.button("🚀 开始分词", type="primary"):
    import time
    start_time = time.time()

    with st.spinner("正在分词..."):
        # ... 现有的分词逻辑 ...

        st.session_state.word_counter = word_counter
        st.session_state.ngram_counter = ngram_counter
        # ... 其他状态更新 ...

    # 保存到数据库
    with st.spinner("正在保存分词结果到数据库..."):
        with WordSegmentRepository() as ws_repo:
            # 创建批次记录
            batch_id = ws_repo.create_batch(
                phrase_count=len(keywords_cleaned),
                notes=f"Phase0分词 - {len(word_counter)}词 + {len(ngram_counter)}短语"
            )

            # 保存分词结果
            new_words, new_ngrams = ws_repo.save_word_segments(
                word_counter=word_counter,
                pos_tags=st.session_state.pos_tags if enable_pos_tagging else None,
                translations=st.session_state.translations,
                batch_id=batch_id,
                ngram_counter=ngram_counter if extract_ngrams else None,
                ngram_translations=st.session_state.ngram_translations
            )

            # 更新批次记录
            duration = int(time.time() - start_time)
            ws_repo.complete_batch(
                batch_id=batch_id,
                word_count=len(word_counter) + len(ngram_counter),
                new_word_count=new_words + new_ngrams,
                duration_seconds=duration
            )

    st.success(f"✓ 分词完成并已保存！新增 {new_words} 个单词 + {new_ngrams} 个短语")
    st.session_state.segmentation_loaded = True
```

## 用户体验流程

### 场景1：首次使用
1. 用户打开Phase 0页面
2. 页面显示："数据库中没有分词结果，请点击'开始分词'"
3. 用户点击"开始分词"
4. 分词完成后自动保存到数据库
5. 显示："✓ 分词完成并已保存！"

### 场景2：再次打开页面
1. 用户打开Phase 0页面
2. 页面自动加载上次的分词结果
3. 显示："✓ 已加载上次分词结果：XXX 个单词 + XXX 个短语"
4. 显示："📅 上次分词时间: 2025-12-25 12:00"
5. 用户可以直接查看、筛选、导出结果，**无需重新分词**

### 场景3：有新数据导入
1. 用户在Phase 1中导入了新的关键词
2. 返回Phase 0页面
3. 页面自动加载上次的分词结果（旧数据）
4. 同时显示提示："⚠️ 检测到 500 条新关键词，建议重新分词"
5. 用户点击"全量重新分词"
6. 系统重新分词所有数据（旧+新）并更新数据库

### 场景4：刷新浏览器
1. 用户刷新浏览器（或关闭后重新打开）
2. Session State清空，但数据库数据仍在
3. 页面自动从数据库加载分词结果
4. **用户可以继续之前的工作，无需重新分词**

## 技术细节

### 数据持久化策略
- **单词和短语统一存储**：`word_segments` 表通过 `word_count` 字段区分
- **累加式更新**：重新分词时频次累加，不覆盖
- **批次追踪**：`segmentation_batches` 表记录每次分词的元数据

### 性能优化
- **按需加载**：只在首次渲染时从数据库加载，后续使用Session State
- **分页显示**：如果分词结果过多，可以考虑分页加载（未来优化）
- **索引优化**：在 `word_count` 和 `frequency` 上创建索引，加速查询

### 数据一致性
- **事务保证**：保存分词结果时使用数据库事务
- **回滚机制**：如果保存失败，自动回滚，不影响现有数据
- **批次记录**：即使保存失败，批次记录会标记为"failed"

## 未来扩展

1. **增量分词**：只对新增关键词进行分词，合并到现有结果
2. **分词历史**：查看每次分词的详细信息和对比
3. **导出/导入分词结果**：支持团队成员共享分词结果
4. **自动化触发**：数据导入后自动触发增量分词

## 测试计划

1. **首次分词测试**：确保保存成功
2. **加载测试**：刷新页面后能正确加载
3. **新数据检测测试**：导入新数据后显示提示
4. **重新分词测试**：重新分词后数据正确更新
5. **边界情况测试**：空数据、超大数据集等

## 回滚方案

如果新功能有问题，可以：
1. 注释掉自动加载代码，恢复原有的手动分词流程
2. 数据库表结构不影响现有功能，可以保留
3. 用户数据不会丢失，存储在数据库中

## 总结

这个功能极大地改善了用户体验：
- ✅ 避免重复劳动（不需要每次都点"开始分词"）
- ✅ 数据持久化（关闭浏览器后数据不丢失）
- ✅ 智能提示（检测新数据并提示重新分词）
- ✅ 灵活性（可以选择全量或增量分词）
