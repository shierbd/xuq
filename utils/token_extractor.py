"""
Tokenæå–è¾…åŠ©å·¥å…·
ä»çŸ­è¯­ä¸­æå–å€™é€‰tokenså¹¶ç»Ÿè®¡é¢‘æ¬¡
"""
import re
from typing import List, Dict, Set
from collections import Counter


# åœç”¨è¯åˆ—è¡¨ï¼ˆè‹±æ–‡å¸¸è§åœç”¨è¯ï¼‰
STOP_WORDS = {
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
    'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
    'to', 'was', 'will', 'with', 'the', 'this', 'but', 'they', 'have',
    'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how'
}

# å¸¸è§ä»‹è¯å’Œå† è¯
FUNCTION_WORDS = {
    'about', 'after', 'all', 'also', 'any', 'back', 'because', 'but',
    'can', 'come', 'could', 'day', 'do', 'even', 'first', 'get',
    'give', 'go', 'good', 'her', 'him', 'his', 'i', 'if', 'into',
    'just', 'know', 'like', 'look', 'make', 'me', 'more', 'my',
    'new', 'no', 'not', 'now', 'one', 'only', 'or', 'other', 'our',
    'out', 'over', 'people', 'say', 'see', 'she', 'so', 'some',
    'take', 'than', 'them', 'then', 'there', 'these', 'think', 'time',
    'two', 'up', 'use', 'very', 'want', 'way', 'we', 'well', 'were',
    'what', 'when', 'which', 'who', 'will', 'would', 'year', 'you', 'your'
}

# åˆå¹¶åœç”¨è¯
ALL_STOP_WORDS = STOP_WORDS | FUNCTION_WORDS


def clean_token(text: str) -> str:
    """
    æ¸…ç†tokenæ–‡æœ¬

    Args:
        text: åŸå§‹æ–‡æœ¬

    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    # è½¬å°å†™
    text = text.lower().strip()

    # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^\w\s-]', '', text)

    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def is_valid_token(token: str, min_length: int = 2, max_length: int = 30) -> bool:
    """
    æ£€æŸ¥tokenæ˜¯å¦æœ‰æ•ˆ

    Args:
        token: å¾…æ£€æŸ¥çš„token
        min_length: æœ€å°é•¿åº¦
        max_length: æœ€å¤§é•¿åº¦

    Returns:
        æ˜¯å¦æœ‰æ•ˆ
    """
    # é•¿åº¦æ£€æŸ¥
    if len(token) < min_length or len(token) > max_length:
        return False

    # åœç”¨è¯æ£€æŸ¥
    if token.lower() in ALL_STOP_WORDS:
        return False

    # çº¯æ•°å­—æ£€æŸ¥
    if token.isdigit():
        return False

    # åŒ…å«å­—æ¯æ£€æŸ¥ï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ªå­—æ¯ï¼‰
    if not any(c.isalpha() for c in token):
        return False

    return True


def extract_tokens_from_phrase(phrase: str) -> List[str]:
    """
    ä»å•ä¸ªçŸ­è¯­ä¸­æå–tokens

    Args:
        phrase: çŸ­è¯­æ–‡æœ¬

    Returns:
        tokenåˆ—è¡¨
    """
    # æ¸…ç†çŸ­è¯­
    cleaned = clean_token(phrase)

    # åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œè¿å­—ç¬¦ï¼‰
    # ä¿ç•™è¿å­—ç¬¦ç»„åˆï¼Œä½†ä¹Ÿåˆ†åˆ«æå–
    tokens = []

    # æŒ‰ç©ºæ ¼åˆ†è¯
    words = cleaned.split()

    for word in words:
        # æ·»åŠ å®Œæ•´å•è¯
        if is_valid_token(word):
            tokens.append(word)

        # å¦‚æœåŒ…å«è¿å­—ç¬¦ï¼Œä¹Ÿåˆ†åˆ«æå–
        if '-' in word:
            parts = word.split('-')
            for part in parts:
                if is_valid_token(part):
                    tokens.append(part)

    return tokens


def suggest_frequency_threshold(token_counter: Counter,
                               target_tokens: int = 1000,
                               show_distribution: bool = True) -> int:
    """
    æ ¹æ®tokené¢‘æ¬¡åˆ†å¸ƒè‡ªåŠ¨å»ºè®®é˜ˆå€¼

    Args:
        token_counter: tokené¢‘æ¬¡ç»Ÿè®¡
        target_tokens: ç›®æ ‡ä¿ç•™çš„tokenæ•°é‡
        show_distribution: æ˜¯å¦æ˜¾ç¤ºåˆ†å¸ƒç»Ÿè®¡

    Returns:
        å»ºè®®çš„æœ€å°é¢‘æ¬¡é˜ˆå€¼
    """
    freq_distribution = sorted(token_counter.values(), reverse=True)

    if show_distribution:
        print("\n  ğŸ“Š Tokené¢‘æ¬¡åˆ†å¸ƒç»Ÿè®¡:")
        thresholds = [3, 5, 8, 10, 15, 20, 30]
        for threshold in thresholds:
            count = sum(1 for freq in freq_distribution if freq >= threshold)
            print(f"     é¢‘æ¬¡ >= {threshold:2d}: {count:5d} ä¸ªtoken")

    # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡æ•°é‡çš„é˜ˆå€¼
    thresholds_to_test = [3, 5, 8, 10, 15, 20, 30, 50]
    suggested = 8  # é»˜è®¤å»ºè®®å€¼

    for threshold in thresholds_to_test:
        count = sum(1 for freq in freq_distribution if freq >= threshold)
        if count <= target_tokens:
            suggested = threshold
            break

    print(f"\n  ğŸ’¡ å»ºè®®é˜ˆå€¼: >= {suggested} (ä¿ç•™çº¦ {sum(1 for f in freq_distribution if f >= suggested)} ä¸ªtoken)")
    print(f"     ç†ç”±: æ•°æ®é‡ {len(freq_distribution)} ä¸ªå”¯ä¸€tokenï¼Œå»ºè®®ä¿ç•™é«˜é¢‘æ ¸å¿ƒè¯")

    return suggested


def extract_tokens(phrases: List[str],
                  min_frequency: int = 8,
                  auto_suggest: bool = False) -> List[Dict]:
    """
    ä»çŸ­è¯­åˆ—è¡¨ä¸­æå–å€™é€‰tokenså¹¶ç»Ÿè®¡é¢‘æ¬¡

    Args:
        phrases: çŸ­è¯­åˆ—è¡¨
        min_frequency: æœ€å°é¢‘æ¬¡ï¼ˆä½äºæ­¤é¢‘æ¬¡çš„tokenä¼šè¢«è¿‡æ»¤ï¼‰ï¼Œé»˜è®¤8
        auto_suggest: æ˜¯å¦è‡ªåŠ¨å»ºè®®é˜ˆå€¼

    Returns:
        å€™é€‰tokenåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'text': ..., 'frequency': ...}
    """
    print(f"\nğŸ” ä» {len(phrases)} ä¸ªçŸ­è¯­ä¸­æå–tokens...")

    # ç»Ÿè®¡æ‰€æœ‰tokens
    token_counter = Counter()

    for phrase in phrases:
        tokens = extract_tokens_from_phrase(phrase)
        token_counter.update(tokens)

    print(f"  âœ“ æå–åˆ° {len(token_counter)} ä¸ªå”¯ä¸€tokens")

    # è‡ªåŠ¨å»ºè®®é˜ˆå€¼
    if auto_suggest:
        suggested_threshold = suggest_frequency_threshold(token_counter)
        print(f"  â„¹ï¸  å½“å‰ä½¿ç”¨é˜ˆå€¼: {min_frequency}")
        if min_frequency != suggested_threshold:
            print(f"  âš ï¸  å»ºè®®è°ƒæ•´ä¸º: {suggested_threshold} (ä½¿ç”¨ --min-frequency {suggested_threshold})")

    # è¿‡æ»¤ä½é¢‘tokens
    filtered_tokens = [
        {'text': token, 'frequency': count}
        for token, count in token_counter.items()
        if count >= min_frequency
    ]

    # æŒ‰é¢‘æ¬¡æ’åº
    filtered_tokens.sort(key=lambda x: x['frequency'], reverse=True)

    print(f"  âœ“ è¿‡æ»¤åä¿ç•™ {len(filtered_tokens)} ä¸ªtokens (é¢‘æ¬¡>={min_frequency})")

    return filtered_tokens


def extract_bigrams(phrases: List[str], min_frequency: int = 3) -> List[Dict]:
    """
    æå–äºŒå…ƒè¯ç»„ï¼ˆbigramsï¼‰

    **å·²åºŸå¼ƒ**: è¯·ä½¿ç”¨ extract_ngrams() æ›¿ä»£

    Args:
        phrases: çŸ­è¯­åˆ—è¡¨
        min_frequency: æœ€å°é¢‘æ¬¡

    Returns:
        bigramåˆ—è¡¨
    """
    print(f"\nğŸ” æå–äºŒå…ƒè¯ç»„...")

    bigram_counter = Counter()

    for phrase in phrases:
        tokens = extract_tokens_from_phrase(phrase)

        # ç”Ÿæˆbigrams
        for i in range(len(tokens) - 1):
            bigram = f"{tokens[i]} {tokens[i+1]}"
            bigram_counter[bigram] += 1

    print(f"  âœ“ æå–åˆ° {len(bigram_counter)} ä¸ªå”¯ä¸€bigrams")

    # è¿‡æ»¤ä½é¢‘bigrams
    filtered_bigrams = [
        {'text': bigram, 'frequency': count}
        for bigram, count in bigram_counter.items()
        if count >= min_frequency
    ]

    filtered_bigrams.sort(key=lambda x: x['frequency'], reverse=True)

    print(f"  âœ“ è¿‡æ»¤åä¿ç•™ {len(filtered_bigrams)} ä¸ªbigrams (é¢‘æ¬¡>={min_frequency})")

    return filtered_bigrams


def extract_ngrams(phrases: List[str],
                  max_gram_size: int = 4,
                  min_frequency: int = 5,
                  priority_mode: bool = True) -> List[Dict]:
    """
    æå–N-gramçŸ­è¯­ï¼ˆä¼˜å…ˆçº§æå–æ¨¡å¼ï¼‰

    **ä¼˜å…ˆçº§ç­–ç•¥**:
    - ä¼˜å…ˆçº§1: N-gram (2-4è¯ç»„åˆï¼ŒåŸç”Ÿè¯ç»„)
    - ä¼˜å…ˆçº§2: å•token (ç”¨äºè¡¥å……)

    Args:
        phrases: çŸ­è¯­åˆ—è¡¨
        max_gram_size: æœ€å¤§Nå€¼ï¼ˆ1=unigram, 2=bigram, 3=trigram, 4=4-gramï¼‰
        min_frequency: æœ€å°é¢‘æ¬¡é˜ˆå€¼
        priority_mode: True=ä¼˜å…ˆæ¨¡å¼(é«˜é¢‘n-gram+ä½é¢‘å•token), False=å…¨éƒ¨æå–

    Returns:
        n-gramåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
        {
            'text': str,           # n-gramæ–‡æœ¬
            'frequency': int,      # å‡ºç°é¢‘æ¬¡
            'gram_size': int,      # Nå€¼ (1=å•è¯, 2=äºŒå…ƒ, etc.)
            'priority': int        # ä¼˜å…ˆçº§ (1=æœ€é«˜)
        }
    """
    print(f"\nğŸ” æå–N-gramè¯ç»„ (max_gram={max_gram_size})...")

    # æŒ‰gram_sizeåˆ†åˆ«ç»Ÿè®¡
    ngram_counters = {i: Counter() for i in range(1, max_gram_size + 1)}

    for phrase in phrases:
        tokens = extract_tokens_from_phrase(phrase)

        # æå–ä¸åŒé•¿åº¦çš„n-grams
        for n in range(1, min(max_gram_size + 1, len(tokens) + 1)):
            for i in range(len(tokens) - n + 1):
                ngram = " ".join(tokens[i:i+n])
                ngram_counters[n][ngram] += 1

    # ç»Ÿè®¡
    for n in range(1, max_gram_size + 1):
        print(f"  âœ“ {n}-gram: {len(ngram_counters[n])} ä¸ªå”¯ä¸€ç»„åˆ")

    # ä¼˜å…ˆçº§è¿‡æ»¤ç­–ç•¥
    all_ngrams = []

    if priority_mode:
        print(f"\n  ğŸ“Š ä¼˜å…ˆçº§è¿‡æ»¤æ¨¡å¼:")

        # ä¼˜å…ˆçº§1: é«˜é¢‘N-gram (n >= 2)
        for n in range(max_gram_size, 1, -1):  # ä»å¤§åˆ°å°ï¼Œä¼˜å…ˆä¿ç•™é•¿çŸ­è¯­
            high_freq_threshold = min_frequency * (1 + (n - 2) * 0.5)  # é•¿çŸ­è¯­å…è®¸æ›´ä½é¢‘æ¬¡

            for ngram, count in ngram_counters[n].items():
                if count >= high_freq_threshold:
                    all_ngrams.append({
                        'text': ngram,
                        'frequency': count,
                        'gram_size': n,
                        'priority': 1  # æœ€é«˜ä¼˜å…ˆçº§
                    })

            print(f"    - ä¼˜å…ˆçº§1: {n}-gram â‰¥ {high_freq_threshold:.0f}é¢‘æ¬¡ â†’ {len([x for x in all_ngrams if x['gram_size']==n])} ä¸ª")

        # ä¼˜å…ˆçº§2: å•token (è¡¥å……è¯æ±‡)
        # é™ä½å•tokençš„é¢‘æ¬¡è¦æ±‚ï¼Œä½†åªä¿ç•™æœªåœ¨n-gramä¸­å‡ºç°çš„
        ngram_tokens_set = set()
        for item in all_ngrams:
            ngram_tokens_set.update(item['text'].split())

        for token, count in ngram_counters[1].items():
            # å¦‚æœtokenå·²ç»åœ¨é«˜ä¼˜å…ˆçº§n-gramä¸­å‡ºç°ï¼Œæé«˜é¢‘æ¬¡è¦æ±‚
            if token in ngram_tokens_set:
                threshold = min_frequency * 2  # æ›´é«˜é—¨æ§›
            else:
                threshold = min_frequency  # æ ‡å‡†é—¨æ§›

            if count >= threshold:
                all_ngrams.append({
                    'text': token,
                    'frequency': count,
                    'gram_size': 1,
                    'priority': 2  # æ¬¡è¦ä¼˜å…ˆçº§
                })

        unigram_count = len([x for x in all_ngrams if x['gram_size'] == 1])
        print(f"    - ä¼˜å…ˆçº§2: 1-gram(å•token) â‰¥ {min_frequency}é¢‘æ¬¡ â†’ {unigram_count} ä¸ª")

    else:
        # å…¨éƒ¨æå–æ¨¡å¼ï¼ˆæ— ä¼˜å…ˆçº§åŒºåˆ†ï¼‰
        print(f"\n  ğŸ“Š å…¨éƒ¨æå–æ¨¡å¼ (é¢‘æ¬¡>={min_frequency}):")

        for n in range(1, max_gram_size + 1):
            for ngram, count in ngram_counters[n].items():
                if count >= min_frequency:
                    all_ngrams.append({
                        'text': ngram,
                        'frequency': count,
                        'gram_size': n,
                        'priority': 1
                    })

            count_at_n = len([x for x in all_ngrams if x['gram_size'] == n])
            print(f"    - {n}-gram: {count_at_n} ä¸ª")

    # æ’åºï¼šä¼˜å…ˆçº§ > gram_size(é™åº) > é¢‘æ¬¡(é™åº)
    all_ngrams.sort(key=lambda x: (x['priority'], -x['gram_size'], -x['frequency']))

    print(f"\n  âœ… æ€»è®¡æå–: {len(all_ngrams)} ä¸ªè¯ç»„")
    print(f"     - ä¼˜å…ˆçº§1 (n-gram): {len([x for x in all_ngrams if x['priority']==1])} ä¸ª")
    print(f"     - ä¼˜å…ˆçº§2 (å•token): {len([x for x in all_ngrams if x['priority']==2])} ä¸ª")

    return all_ngrams


def extract_demand_patterns(phrases: List[str],
                           tokens_classified: Dict[str, str],
                           min_frequency: int = 5) -> List[Dict]:
    """
    ä»çŸ­è¯­ä¸­æå–éœ€æ±‚æ¨¡å¼ï¼ˆæ¡†æ¶ï¼‰

    Args:
        phrases: çŸ­è¯­åˆ—è¡¨
        tokens_classified: tokenåˆ†ç±»å­—å…¸ {token_text: token_type}
        min_frequency: æ¨¡å¼æœ€å°é¢‘æ¬¡

    Returns:
        éœ€æ±‚æ¨¡å¼åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'pattern': ..., 'frequency': ..., 'examples': [...]}
    """
    print(f"\nğŸ” ä» {len(phrases)} ä¸ªçŸ­è¯­ä¸­æå–éœ€æ±‚æ¨¡å¼...")

    pattern_counter = Counter()
    pattern_examples = {}  # å­˜å‚¨æ¯ä¸ªæ¨¡å¼çš„ç¤ºä¾‹çŸ­è¯­

    for phrase in phrases:
        phrase_tokens = extract_tokens_from_phrase(phrase)

        # æ„å»ºæ¨¡å¼
        pattern_parts = []
        for token in phrase_tokens:
            token_type = tokens_classified.get(token, 'other')
            pattern_parts.append(f"[{token_type}]")

        pattern = " ".join(pattern_parts)
        pattern_counter[pattern] += 1

        # è®°å½•ç¤ºä¾‹çŸ­è¯­ï¼ˆæ¯ä¸ªæ¨¡å¼æœ€å¤šä¿ç•™5ä¸ªç¤ºä¾‹ï¼‰
        if pattern not in pattern_examples:
            pattern_examples[pattern] = []
        if len(pattern_examples[pattern]) < 5:
            pattern_examples[pattern].append(phrase)

    print(f"  âœ“ å‘ç° {len(pattern_counter)} ç§å”¯ä¸€éœ€æ±‚æ¨¡å¼")

    # è¿‡æ»¤ä½é¢‘æ¨¡å¼
    filtered_patterns = [
        {
            'pattern': pattern,
            'frequency': count,
            'examples': pattern_examples.get(pattern, [])[:3]  # åªä¿ç•™å‰3ä¸ªç¤ºä¾‹
        }
        for pattern, count in pattern_counter.items()
        if count >= min_frequency
    ]

    # æŒ‰é¢‘æ¬¡æ’åº
    filtered_patterns.sort(key=lambda x: x['frequency'], reverse=True)

    print(f"  âœ“ è¿‡æ»¤åä¿ç•™ {len(filtered_patterns)} ç§é«˜é¢‘æ¨¡å¼ (é¢‘æ¬¡>={min_frequency})")

    return filtered_patterns


def analyze_token_framework(tokens_with_types: List[Dict],
                            patterns: List[Dict] = None) -> Dict:
    """
    åˆ†æéœ€æ±‚æ¡†æ¶è¯åº“çš„ç»Ÿè®¡ä¿¡æ¯

    Args:
        tokens_with_types: åˆ†ç±»åçš„tokenåˆ—è¡¨
        patterns: éœ€æ±‚æ¨¡å¼åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        æ¡†æ¶åˆ†æç»Ÿè®¡å­—å…¸
    """
    print("\nğŸ“Š åˆ†æéœ€æ±‚æ¡†æ¶è¯åº“...")

    # æŒ‰ç±»å‹åˆ†ç»„ç»Ÿè®¡
    type_stats = {}
    for token in tokens_with_types:
        token_type = token['token_type']
        if token_type not in type_stats:
            type_stats[token_type] = {
                'count': 0,
                'total_frequency': 0,
                'top_tokens': []
            }

        type_stats[token_type]['count'] += 1
        type_stats[token_type]['total_frequency'] += token.get('in_phrase_count', 0)

    # ä¸ºæ¯ä¸ªç±»å‹æ‰¾å‡ºTop 10é«˜é¢‘token
    for token_type in type_stats:
        tokens_of_type = [t for t in tokens_with_types if t['token_type'] == token_type]
        sorted_tokens = sorted(tokens_of_type,
                              key=lambda x: x.get('in_phrase_count', 0),
                              reverse=True)
        type_stats[token_type]['top_tokens'] = sorted_tokens[:10]

    analysis = {
        'total_tokens': len(tokens_with_types),
        'type_stats': type_stats,
        'patterns': patterns or []
    }

    print(f"  âœ“ åˆ†æå®Œæˆ")

    return analysis


def group_tokens_by_prefix(tokens: List[Dict], max_groups: int = 100) -> Dict[str, List[Dict]]:
    """
    æŒ‰å‰ç¼€åˆ†ç»„tokensï¼ˆç”¨äºæ‰¹é‡LLMåˆ†ç±»æ—¶å‡å°‘é‡å¤ï¼‰

    Args:
        tokens: tokenåˆ—è¡¨
        max_groups: æœ€å¤§åˆ†ç»„æ•°

    Returns:
        åˆ†ç»„å­—å…¸ {prefix: [tokens]}
    """
    groups = {}

    for token in tokens:
        text = token['text']

        # å–å‰3ä¸ªå­—ç¬¦ä½œä¸ºå‰ç¼€
        prefix = text[:3] if len(text) >= 3 else text

        if prefix not in groups:
            groups[prefix] = []

        groups[prefix].append(token)

    # å¦‚æœåˆ†ç»„å¤ªå¤šï¼Œåˆå¹¶å°åˆ†ç»„
    if len(groups) > max_groups:
        sorted_groups = sorted(groups.items(), key=lambda x: len(x[1]), reverse=True)
        groups = dict(sorted_groups[:max_groups])

    return groups


def test_token_extraction():
    """æµ‹è¯•tokenæå–åŠŸèƒ½"""
    print("\n" + "="*70)
    print("æµ‹è¯•Tokenæå–")
    print("="*70)

    # æµ‹è¯•çŸ­è¯­
    test_phrases = [
        "best running shoes for women",
        "top rated running shoes",
        "comfortable running shoes",
        "affordable running shoes",
        "running shoes for men",
        "how to choose running shoes",
        "running shoe sizing guide",
        "best budget running shoes",
        "nike running shoes",
        "adidas running shoes",
    ]

    # æå–tokens
    tokens = extract_tokens(test_phrases, min_frequency=2)

    print(f"\næå–ç»“æœ (Top 10):")
    for i, token in enumerate(tokens[:10], 1):
        print(f"  {i}. '{token['text']}' - å‡ºç°{token['frequency']}æ¬¡")

    # æå–bigrams
    bigrams = extract_bigrams(test_phrases, min_frequency=2)

    print(f"\näºŒå…ƒè¯ç»„ (Top 5):")
    for i, bigram in enumerate(bigrams[:5], 1):
        print(f"  {i}. '{bigram['text']}' - å‡ºç°{bigram['frequency']}æ¬¡")

    print("\nâœ… Tokenæå–æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_token_extraction()
