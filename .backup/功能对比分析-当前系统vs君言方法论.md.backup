# åŠŸèƒ½å¯¹æ¯”åˆ†æï¼šå½“å‰ç³»ç»Ÿ vs å›è¨€å…³é”®è¯å¤„ç†æ–¹æ³•è®º

> **å¯¹æ¯”ç›®çš„**ï¼šå…¨é¢è¯„ä¼°ä¸¤å¥—ç³»ç»Ÿçš„åŠŸèƒ½å·®å¼‚ï¼Œè¯†åˆ«ä¼˜åŒ–æœºä¼šå’Œå®æ–½è·¯å¾„
>
> **å¯¹æ¯”èŒƒå›´**ï¼šä»æ•°æ®é‡‡é›†ã€æ¸…æ´—ã€èšç±»åˆ°éœ€æ±‚æå–çš„å…¨æµç¨‹å¯¹æ¯”
>
> **åˆ›å»ºæ—¥æœŸ**ï¼š2025-12-23

---

## ğŸ“‘ ç›®å½•

1. [æ€»ä½“æ¶æ„å¯¹æ¯”](#æ€»ä½“æ¶æ„å¯¹æ¯”)
2. [è¯¦ç»†åŠŸèƒ½å¯¹æ¯”](#è¯¦ç»†åŠŸèƒ½å¯¹æ¯”)
3. [æ ¸å¿ƒç®—æ³•å¯¹æ¯”](#æ ¸å¿ƒç®—æ³•å¯¹æ¯”)
4. [æ•°æ®åº“è®¾è®¡å¯¹æ¯”](#æ•°æ®åº“è®¾è®¡å¯¹æ¯”)
5. [æ€§èƒ½ä¸è§„æ¨¡å¯¹æ¯”](#æ€§èƒ½ä¸è§„æ¨¡å¯¹æ¯”)
6. [ä¼˜åŠ¿ä¸åŠ£åŠ¿åˆ†æ](#ä¼˜åŠ¿ä¸åŠ£åŠ¿åˆ†æ)
7. [å®æ–½ä¼˜å…ˆçº§å»ºè®®](#å®æ–½ä¼˜å…ˆçº§å»ºè®®)

---

## æ€»ä½“æ¶æ„å¯¹æ¯”

### ç³»ç»Ÿå®šä½

| ç»´åº¦ | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º |
|------|----------|-----------|
| **åº”ç”¨åœºæ™¯** | è‹±æ–‡å•è¯éœ€æ±‚æŒ–æ˜ | ä¸­æ–‡å…³é”®è¯éœ€æ±‚åˆ†æ |
| **ç›®æ ‡é¢†åŸŸ** | äº§å“æœºä¼šå‘ç°ï¼ˆSaaS/å·¥å…·ç±»ï¼‰ | SEO/SEM/å†…å®¹è¿è¥ |
| **æ•°æ®è§„æ¨¡** | 5-10ä¸‡çŸ­è¯­ | 1.6äº¿ â†’ 5000ä¸‡ â†’ 4000ä¸‡ |
| **æ ¸å¿ƒè¾“å‡º** | éœ€æ±‚å¡ç‰‡ + è¯­ä¹‰ç°‡ | éœ€æ±‚ç±»åˆ« + ç‰¹å¾å˜é‡ + æœç´¢æ¨¡æ¿ |
| **æŠ€æœ¯æ ˆ** | Python + HDBSCAN + LLM | Python + è‡ªç ”èšç±» + äººå·¥å®¡æ ¸ |

### å·¥ä½œæµç¨‹å¯¹æ¯”å›¾

```
ã€å½“å‰ç³»ç»Ÿ - MVPç‰ˆæœ¬ã€‘
Phase 1: æ•°æ®å¯¼å…¥
   â†“
Phase 2: å¤§ç»„èšç±»ï¼ˆHDBSCAN, 60-100ç°‡ï¼‰
   â†“
Phase 3: äººå·¥ç­›é€‰ï¼ˆ10-15ä¸ªå¤§ç»„ï¼‰
   â†“
Phase 4: å°ç»„èšç±» + LLMç”Ÿæˆéœ€æ±‚å¡ç‰‡
   â†“
Phase 5: Tokenæå– + LLMåˆ†ç±»
   â†“
è¾“å‡ºï¼šéœ€æ±‚å¡ç‰‡åº“ + Tokenè¯åº“
```

```
ã€å›è¨€æ–¹æ³•è®º - å®Œæ•´æµç¨‹ã€‘
Phase 1: æ•°æ®æŒ–æ˜
   â”œâ”€ é«˜é¢‘è¯ç»„åˆç­–ç•¥
   â””â”€ ä¼˜å…ˆå¤§è¯æ’åº
   â†“ (1.6äº¿ â†’ 5000ä¸‡, -68.75%)
Phase 2: æ•°æ®æ¸…æ´—
   â”œâ”€ æ–‡å­—æ’åºå»é‡
   â””â”€ åœç”¨è¯è¿‡æ»¤
   â†“ (5000ä¸‡ â†’ 4000ä¸‡, -20%)
Phase 3: åˆ†æ‰¹èšç±»
   â”œâ”€ 20æ‰¹ Ã— 200ä¸‡
   â””â”€ å…¨å±€åˆå¹¶
   â†“ (180ä¸‡æ ‡è¯†)
Phase 4: ç‰¹å¾ç‰‡æ®µæå– â­æ ¸å¿ƒåˆ›æ–°
   â”œâ”€ N-gramç»Ÿè®¡ï¼ˆ3-5å­—ï¼‰
   â””â”€ Top 1ä¸‡ç‰‡æ®µæ˜ å°„
   â†“ (180ä¸‡ â†’ 2ä¸‡æ ·æœ¬)
Phase 5: é‡æ–°èšç±»ï¼ˆ2ä¸‡æ ·æœ¬ï¼‰
   â†“
Phase 6: éœ€æ±‚åˆ†ç±»ï¼ˆ6å¤§ç±»åˆ«ï¼‰
   â”œâ”€ å¯»æ‰¾ç±»ï¼ˆ95%ï¼‰
   â”œâ”€ æ“ä½œç±»ï¼ˆ<2%ï¼‰
   â””â”€ å…¶ä»–ç±»
   â†“
Phase 7: ç‰¹å¾å˜é‡æå– â­æ ¸å¿ƒåˆ›æ–°
   â”œâ”€ æ¨¡æ¿-å˜é‡è¿­ä»£
   â”œâ”€ åŠŸèƒ½ç±»ï¼ˆå‡ åƒä¸ªï¼‰
   â”œâ”€ å¯¹è±¡ç±»ï¼ˆå‡ åƒä¸ªï¼‰
   â”œâ”€ æ¸ é“ç±»ï¼ˆ8000+ï¼‰
   â””â”€ ç¾¤ä½“ç±»ï¼ˆå‡ ç™¾ä¸ªï¼‰
   â†“
Phase 8: æœç´¢ç»“æ„è¯†åˆ«
   â””â”€ é«˜é¢‘æ¨¡æ¿æå–
   â†“
è¾“å‡ºï¼šéœ€æ±‚æ¡†æ¶ + ç‰¹å¾è¯åº“ + æœç´¢æ¨¡æ¿
```

---

## è¯¦ç»†åŠŸèƒ½å¯¹æ¯”

### 1. æ•°æ®é‡‡é›†ä¸æŒ–æ˜

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **æ•°æ®æº** | SEMRUSH + ä¸‹æ‹‰è¯ + ç›¸å…³æœç´¢ | 5118ï¼ˆè½¯ä»¶é¢†åŸŸï¼‰ | ğŸŸ¢ ç›¸åŒæ€è·¯ |
| **é‡‡é›†ç­–ç•¥** | âŒ ç›´æ¥å¯¼å…¥åŸå§‹æ•°æ® | âœ… é«˜é¢‘è¯ç»„åˆ + ä¼˜å…ˆå¤§è¯ | ğŸ”´ **ç¼ºå¤±æ ¸å¿ƒç­–ç•¥** |
| **æ•°æ®å‡å‹** | âŒ ä»…åŸºæœ¬å»é‡ | âœ… ä¸»åŠ¨å‡å‹68.75%ï¼ˆç­–ç•¥æ€§ï¼‰ | ğŸ”´ **ç¼ºå¤±** |
| **æˆæœ¬ä¼˜åŒ–** | âŒ æ— ä¼˜åŒ– | âœ… ä¸‹è½½æ¬¡æ•°æœ€å°åŒ– | ğŸ”´ **ç¼ºå¤±** |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`core/data_integration.py` (åŸºç¡€æ¸…æ´—)
- å›è¨€æ–¹æ³•è®ºï¼šé«˜é¢‘è¯ç»„åˆç®—æ³•ï¼ˆæœªå®ç°ï¼‰

**å·®è·è¯´æ˜**ï¼š
- âŒ å½“å‰ç³»ç»Ÿç›´æ¥å¯¼å…¥åŸå§‹æ•°æ®ï¼Œæ²¡æœ‰é¢„å…ˆè¿‡æ»¤å†—ä½™
- âŒ ç¼ºå°‘"ä¼˜å…ˆå¤§è¯"ç­–ç•¥ï¼ˆæŒ‰é•¿å°¾è¯æ•°é‡é™åºä¸‹è½½ï¼‰
- âŒ ç¼ºå°‘é«˜é¢‘è¯ç»„åˆç­–ç•¥ï¼ˆå¸•ç´¯æ‰˜80%è¦†ç›–ï¼‰

**ä¼˜å…ˆçº§**ï¼šâ­â­ ä¸­ï¼ˆè‹±æ–‡åœºæ™¯æ•°æ®é‡è¾ƒå°ï¼Œæš‚ä¸ç´§è¿«ï¼‰

---

### 2. æ•°æ®æ¸…æ´—

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **å»é‡æ–¹å¼** | ğŸŸ¡ ç›´æ¥å­—ç¬¦ä¸²å»é‡ | âœ… æ–‡å­—æ’åº + åœç”¨è¯è¿‡æ»¤ | ğŸŸ¡ **éƒ¨åˆ†ç¼ºå¤±** |
| **åœç”¨è¯åº“** | ğŸŸ¡ è‹±æ–‡åœç”¨è¯ï¼ˆåŸºç¡€ï¼‰ | âœ… ä¸­æ–‡åœç”¨è¯ï¼ˆå®Œå–„ï¼‰ | ğŸŸ¢ å·²æœ‰åŸºç¡€ |
| **å†—ä½™è¯†åˆ«** | âŒ æ— æ³•è¯†åˆ«è¡¨è¾¾ä¸åŒçš„ç›¸åŒéœ€æ±‚ | âœ… æ‹¼éŸ³æ’åºè¯†åˆ«åŒä¹‰è¯ | ğŸ”´ **ç¼ºå¤±æ ¸å¿ƒç®—æ³•** |
| **å»é‡æ•ˆæœ** | åŸºæœ¬å»é‡ | é¢å¤–å»é™¤20%å†—ä½™ | ğŸ”´ **æ•ˆæœå·®è·å¤§** |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`core/data_integration.py` (line 117-149)
- å›è¨€æ–¹æ³•è®ºï¼šæ–‡å­—æ’åºç®—æ³•ï¼ˆæœªå®ç°ï¼‰

**å½“å‰ç³»ç»Ÿä»£ç ç¤ºä¾‹**ï¼š
```python
# core/data_integration.py:117-149
def clean_phrase(self, phrase: str) -> Optional[str]:
    if pd.isna(phrase) or not isinstance(phrase, str):
        return None
    phrase = phrase.lower().strip()
    phrase = re.sub(r'\s+', ' ', phrase)
    # ... åŸºç¡€æ¸…æ´—ï¼Œæ— æ’åºå»é‡
```

**å›è¨€æ–¹æ³•è®ºç®—æ³•**ï¼š
```python
def create_unique_identifier(keyword, stop_words):
    """
    åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
    æ­¥éª¤ï¼š
    1. å»é™¤åœç”¨è¯
    2. å»é™¤ç¬¦å·å’Œç©ºæ ¼
    3. æŒ‰æ‹¼éŸ³æ’åº
    """
    # å»é™¤åœç”¨è¯
    for stop_word in stop_words:
        keyword = keyword.replace(stop_word, '')
    # å»é™¤ç¬¦å·
    keyword = re.sub(r'[^\w]', '', keyword)
    # æŒ‰æ‹¼éŸ³æ’åº
    chars = list(keyword)
    chars.sort(key=lambda x: pypinyin.lazy_pinyin(x))
    return ''.join(chars)

# ç¤ºä¾‹æ•ˆæœï¼š
# "å›¾ç‰‡å‹ç¼©" â†’ "å›¾å‹ç‰‡ç¼©"
# "å‹ç¼©å›¾ç‰‡" â†’ "å›¾å‹ç‰‡ç¼©"  âœ“ è¯†åˆ«ä¸ºåŒä¸€éœ€æ±‚
```

**å·®è·è¯´æ˜**ï¼š
- âŒ å½“å‰ç³»ç»Ÿæ— æ³•è¯†åˆ« "best calculator" å’Œ "calculator best" ä¸ºåŒä¸€éœ€æ±‚
- âŒ ç¼ºå°‘è¯­ä¹‰ç­‰ä»·å»é‡èƒ½åŠ›
- ğŸŸ¡ è‹±æ–‡åœç”¨è¯åº“åŸºç¡€ï¼Œä½†å¯è¿›ä¸€æ­¥å®Œå–„

**ä¼˜å…ˆçº§**ï¼šâ­â­â­ é«˜ï¼ˆç«‹å³å¯å®æ–½ï¼Œæ•ˆæœæ˜æ˜¾ï¼‰

---

### 3. èšç±»ç®—æ³•

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **èšç±»ç®—æ³•** | âœ… HDBSCAN | âœ… è‡ªç ”èšç±»ï¼ˆä»¥ç©ºé—´æ¢æ—¶é—´ï¼‰ | ğŸŸ¢ ç®—æ³•å„æœ‰ä¼˜åŠ¿ |
| **å¤§ç»„èšç±»** | âœ… ä¸€æ¬¡æ€§èšç±»ï¼ˆå…¨é‡ï¼‰ | âœ… åˆ†æ‰¹èšç±» + å…¨å±€åˆå¹¶ | ğŸŸ¡ **æ‰©å±•æ€§ä¸åŒ** |
| **å°ç»„èšç±»** | âœ… å¯¹é€‰ä¸­å¤§ç»„å†èšç±» | âš ï¸ é€šè¿‡ç‰‡æ®µæ˜ å°„é¿å… | ğŸŸ¢ æ€è·¯ä¸åŒä½†éƒ½æœ‰æ•ˆ |
| **å†…å­˜ä¼˜åŒ–** | ğŸŸ¡ å•æ¬¡èšç±»å—é™ï¼ˆ16Gå†…å­˜ï¼‰ | âœ… åˆ†æ‰¹å¤„ç†ï¼ˆ200ä¸‡/æ‰¹ï¼‰ | ğŸŸ¡ **å¤§è§„æ¨¡æ•°æ®å—é™** |
| **èšç±»ç»“æœ** | 60-100ä¸ªå¤§ç»„ | 180ä¸‡æ ‡è¯† | ğŸ”´ **é—®é¢˜è§„æ¨¡ä¸åŒ** |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`core/clustering.py` (HDBSCAN)
- å›è¨€æ–¹æ³•è®ºï¼šè‡ªç ”èšç±»ç®—æ³•ï¼ˆæœªå…¬å¼€å®Œæ•´ä»£ç ï¼‰

**å½“å‰ç³»ç»Ÿæ ¸å¿ƒä»£ç **ï¼š
```python
# core/clustering.py:39-94
def fit_predict(self, embeddings: np.ndarray):
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=self.config['min_cluster_size'],
        min_samples=self.config['min_samples'],
        metric='cosine',
        cluster_selection_method='eom',
        prediction_data=True
    )
    labels = clusterer.fit_predict(embeddings_normalized)
    # ä¸€æ¬¡æ€§èšç±»å…¨éƒ¨æ•°æ®
```

**å›è¨€åˆ†æ‰¹èšç±»é€»è¾‘**ï¼š
```python
# åˆ†æ‰¹èšç±»ç­–ç•¥
def batch_clustering(keywords, batch_size=2000000):
    num_batches = math.ceil(len(keywords) / batch_size)
    batch_results = []

    # æ­¥éª¤1-2: åˆ†æ‰¹èšç±»
    for i in range(num_batches):
        batch = keywords[start:end]
        cluster_result = keyword_clustering(batch)
        batch_results.append(cluster_result)

    # æ­¥éª¤3: å…¨å±€åˆå¹¶
    global_clusters = {}
    for batch_result in batch_results:
        for label, kw_list in batch_result.items():
            if label not in global_clusters:
                global_clusters[label] = []
            global_clusters[label].extend(kw_list)

    return global_clusters
```

**å·®è·è¯´æ˜**ï¼š
- ğŸŸ¢ å½“å‰ç³»ç»Ÿä½¿ç”¨æˆç†Ÿçš„HDBSCANç®—æ³•ï¼Œæ•ˆæœå¯é 
- ğŸŸ¡ å½“å‰ç³»ç»Ÿä¸€æ¬¡æ€§èšç±»ï¼Œ5-10ä¸‡æ•°æ®æ— å‹åŠ›
- ğŸ”´ å›è¨€é¢å¯¹4000ä¸‡æ•°æ®éœ€åˆ†æ‰¹å¤„ç†ï¼Œå½“å‰ç³»ç»Ÿä¸é€‚ç”¨äºè¶…å¤§è§„æ¨¡
- ğŸŸ¢ ä½†è‹±æ–‡åœºæ™¯æ•°æ®é‡è¿œå°äºä¸­æ–‡ï¼Œå½“å‰ç®—æ³•è¶³å¤Ÿ

**ä¼˜å…ˆçº§**ï¼šâ­ ä½ï¼ˆå½“å‰ç®—æ³•æ»¡è¶³éœ€æ±‚ï¼‰

---

### 4. ç‰¹å¾æå–ï¼ˆæ ¸å¿ƒå·®è·ï¼‰

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **N-gramæå–** | âœ… å·²å®ç°ï¼ˆ1-4 gramï¼‰ | âœ… å·²å®ç°ï¼ˆ3-5 gramï¼‰ | ğŸŸ¢ å·²å¯¹é½ |
| **ç‰¹å¾ç‰‡æ®µåˆ†æ** | âŒ æœªå®ç° | âœ… **æ ¸å¿ƒåˆ›æ–°**ï¼šç‰‡æ®µæ˜ å°„å…¨å±€ | ğŸ”´ **æœ€å¤§å·®è·** |
| **æ ·æœ¬é™ç»´** | âŒ æ— é™ç»´ç­–ç•¥ | âœ… 180ä¸‡æ ‡è¯† â†’ 2ä¸‡æ ·æœ¬ | ğŸ”´ **ç¼ºå¤±å…³é”®åŠŸèƒ½** |
| **å¯å®¡æ ¸æ€§** | ğŸŸ¡ éœ€å®¡æ ¸å…¨éƒ¨èšç±»ç»“æœ | âœ… ä»…éœ€å®¡æ ¸2ä¸‡æ ·æœ¬ï¼ˆ<2å°æ—¶ï¼‰ | ğŸ”´ **æ•ˆç‡å·®è·å·¨å¤§** |
| **Tokenåˆ†ç±»** | âœ… LLMåˆ†ç±»ï¼ˆ4ç±»ï¼‰ | âœ… äººå·¥+æ¨¡æ¿ï¼ˆ4å¤§ç±»ï¼‰ | ğŸŸ¢ æ€è·¯ç›¸ä¼¼ |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`utils/token_extractor.py` (å·²å®ç°N-gram)
- å›è¨€æ–¹æ³•è®ºï¼šç‰¹å¾ç‰‡æ®µç®—æ³• + æ¨¡æ¿å˜é‡æå–ï¼ˆ**æœªå®ç°**ï¼‰

**å½“å‰ç³»ç»ŸN-gramå®ç°**ï¼ˆâœ…å·²æœ‰ï¼‰ï¼š
```python
# utils/token_extractor.py:250-363
def extract_ngrams(phrases, max_gram_size=4, min_frequency=5):
    """
    æå–N-gramè¯ç»„
    ä¼˜å…ˆçº§1: 2-4è¯çš„åŸç”Ÿn-gram
    ä¼˜å…ˆçº§2: å•è¯tokenï¼ˆè¡¥å……ï¼‰
    """
    # å·²å®ç°åŸºç¡€N-gramæå–
    # âœ“ èƒ½æå– "area code", "promo code"
    # âœ“ èƒ½æŒ‰é¢‘æ¬¡è¿‡æ»¤
    # âœ“ èƒ½åŒºåˆ†ä¼˜å…ˆçº§
```

**å›è¨€ç‰¹å¾ç‰‡æ®µç®—æ³•**ï¼ˆâŒæœªå®ç°ï¼‰ï¼š
```python
def ngram_segment_statistics(keywords, min_n=3, max_n=5, top_k=10000):
    """
    N-gramç‰‡æ®µç»Ÿè®¡ - å›è¨€æ ¸å¿ƒåˆ›æ–°

    æ­¥éª¤ï¼š
    1. å¯¹æ¯ä¸ªå…³é”®è¯æå–æ‰€æœ‰n-gram
    2. å…¨å±€ç»Ÿè®¡é¢‘æ¬¡
    3. è¿”å›Top-Ké«˜é¢‘ç‰‡æ®µ
    """
    ngram_counter = Counter()

    for keyword in keywords:
        for n in range(min_n, max_n + 1):
            for i in range(len(keyword) - n + 1):
                segment = keyword[i:i+n]
                ngram_counter[segment] += 1

    # Top-Kç‰‡æ®µ
    return ngram_counter.most_common(top_k)

# å…³é”®ä»·å€¼ï¼š
# Top 1ä¸‡ç‰‡æ®µ â†’ æå–2ä¸‡æ ·æœ¬è¯ â†’ æ˜ å°„å…¨å±€éœ€æ±‚
# ä»"ä¸å¯èƒ½å®¡æ ¸"ï¼ˆ180ä¸‡ï¼‰åˆ°"2å°æ—¶å®Œæˆ"ï¼ˆ2ä¸‡ï¼‰
```

**ç‰¹å¾ç‰‡æ®µçš„åº”ç”¨æµç¨‹**ï¼ˆâŒæœªå®ç°ï¼‰ï¼š
```python
# Phase 4.5: ç‰¹å¾ç‰‡æ®µåˆ†æï¼ˆæ–°å¢ï¼‰
def run_phase4_5_segments(sample_size=10000, min_frequency=8):
    # 1. åŠ è½½æ‰€æœ‰çŸ­è¯­
    phrases = load_all_phrases()

    # 2. N-gramç»Ÿè®¡
    ngram_stats = extract_ngrams_global(phrases, min_n=3, max_n=5)

    # 3. é€‰æ‹©Topç‰‡æ®µ
    top_segments = ngram_stats.most_common(10000)

    # 4. æå–æ ·æœ¬è¯
    sample_keywords = []
    for segment, freq in top_segments:
        mothers = find_mother_keywords(segment, count=2)
        sample_keywords.extend(mothers)

    sample_keywords = list(set(sample_keywords))  # å»é‡ï¼Œçº¦2ä¸‡

    # 5. å¯¹æ ·æœ¬è¯èšç±»ï¼ˆå¿«é€Ÿï¼ï¼‰
    sample_clusters = clustering(sample_keywords)

    # 6. äººå·¥å®¡æ ¸ï¼ˆä»…2ä¸‡è¯ï¼Œ<2å°æ—¶ï¼‰
    return sample_clusters
```

**å·®è·è¯´æ˜**ï¼š
- âœ… å½“å‰ç³»ç»Ÿå·²å®ç°N-gramæå–ï¼ˆPhase 5ï¼‰
- âŒ **ç¼ºå°‘ç‰¹å¾ç‰‡æ®µæ˜ å°„åŠŸèƒ½**ï¼ˆPhase 4.5ï¼‰
- âŒ æ— æ³•å°†180ä¸‡èšç±»ç»“æœé™ç»´åˆ°2ä¸‡å¯å®¡æ ¸æ ·æœ¬
- âŒ ç¼ºå°‘"ä»ç‰‡æ®µæ‰¾æ¯è¯"çš„åŠŸèƒ½

**ä¼˜å…ˆçº§**ï¼šâ­â­â­â­â­ **æœ€é«˜**ï¼ˆæ ¸å¿ƒä»·å€¼åŠŸèƒ½ï¼‰

---

### 5. æ¨¡æ¿-å˜é‡æå–

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **å˜é‡æå–** | ğŸŸ¡ ä»…æå–å•ä¸ªtoken | âœ… æ¨¡æ¿-å˜é‡è¿­ä»£æå– | ğŸ”´ **ç¼ºå°‘è¿­ä»£æœºåˆ¶** |
| **åˆ†ç±»ç»´åº¦** | ğŸŸ¡ 4ç±»ï¼ˆintent/action/object/otherï¼‰ | âœ… 4å¤§ç±»ï¼ˆåŠŸèƒ½/å¯¹è±¡/æ¸ é“/ç¾¤ä½“ï¼‰ | ğŸŸ¢ æ€è·¯ç›¸ä¼¼ |
| **è¯åº“è§„æ¨¡** | ğŸŸ¡ 26ä¸ªn-gramsï¼ˆæµ‹è¯•æ•°æ®ï¼‰ | âœ… æ¸ é“8000+ï¼ŒåŠŸèƒ½/å¯¹è±¡å‡ åƒ | ğŸ”´ **è§„æ¨¡å·®è·å·¨å¤§** |
| **è´¨é‡ä¿è¯** | ğŸŸ¡ LLMåˆ†ç±» | âœ… å˜é‡éœ€é€‚é…â‰¥3ä¸ªæ¨¡æ¿ | ğŸŸ¡ **ç­–ç•¥ä¸åŒ** |
| **è¿­ä»£æ‰©å±•** | âŒ æ— è¿­ä»£æœºåˆ¶ | âœ… 3è½®è¿­ä»£æ”¶æ•› | ğŸ”´ **ç¼ºå¤±** |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`scripts/run_phase5_tokens.py` (Tokenæå–)
- å›è¨€æ–¹æ³•è®ºï¼šæ¨¡æ¿-å˜é‡è¿­ä»£ç®—æ³•ï¼ˆ**æœªå®ç°**ï¼‰

**å½“å‰ç³»ç»Ÿå®ç°**ï¼ˆğŸŸ¡æœ‰åŸºç¡€ä½†ä¸å®Œæ•´ï¼‰ï¼š
```python
# scripts/run_phase5_tokens.py:180-250
# âœ“ æå–token
candidate_tokens = extract_tokens(phrases, min_frequency)

# âœ“ LLMåˆ†ç±»
tokens_with_types = classify_tokens_batch(candidate_tokens)

# âœ“ ä¿å­˜åˆ°æ•°æ®åº“
for token in tokens_with_types:
    token_repo.create_or_update_token(
        token_text=token['token_text'],
        token_type=token['token_type'],
        ...
    )

# âŒ ç¼ºå°‘ï¼š
# 1. æ¨¡æ¿æå–é€»è¾‘
# 2. è¿­ä»£æ‰©å±•æœºåˆ¶
# 3. è´¨é‡è¿‡æ»¤ï¼ˆé€‚é…æ¨¡æ¿æ•°â‰¥3ï¼‰
```

**å›è¨€æ¨¡æ¿-å˜é‡è¿­ä»£ç®—æ³•**ï¼ˆâŒæœªå®ç°ï¼‰ï¼š
```python
def template_variable_extraction(keywords, seed_variables, max_iterations=3):
    """
    æ¨¡æ¿-å˜é‡è¿­ä»£æå–ç®—æ³•
    æ ¸å¿ƒåˆ›æ–°ï¼šå˜é‡ â†” æ¨¡æ¿ åŒå‘è¿­ä»£
    """
    all_templates = set()
    all_variables = set(seed_variables)

    for iteration in range(max_iterations):
        print(f"\n=== Iteration {iteration + 1} ===")

        # Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿
        template_counter = Counter()
        for keyword in keywords:
            for var in all_variables:
                if var in keyword:
                    template = keyword.replace(var, '[X]')
                    template_counter[template] += 1

        # è¿‡æ»¤ï¼šé¢‘æ¬¡ >= 5
        new_templates = [t for t, freq in template_counter.items() if freq >= 5]
        all_templates.update(new_templates)
        print(f"  Templates discovered: {len(new_templates)}")

        # Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡
        variable_freq = Counter()
        variable_templates = defaultdict(set)

        for template in new_templates:
            pattern = template.replace('[X]', '(.+?)')
            for keyword in keywords:
                match = re.match(pattern, keyword)
                if match:
                    var = match.group(1)
                    variable_freq[var] += 1
                    variable_templates[var].add(template)

        # è´¨é‡è¿‡æ»¤ï¼šé€‚é…æ¨¡æ¿æ•° >= 3 ä¸” é¢‘æ¬¡ >= 5
        new_variables = [
            var for var, freq in variable_freq.items()
            if freq >= 5 and len(variable_templates[var]) >= 3
        ]

        before_count = len(all_variables)
        all_variables.update(new_variables)
        after_count = len(all_variables)
        print(f"  Variables discovered: {after_count - before_count}")

        # æ”¶æ•›åˆ¤æ–­
        if after_count == before_count:
            print("  Converged!")
            break

    return list(all_templates), list(all_variables)

# å®é™…æˆæœï¼ˆè½¯ä»¶é¢†åŸŸï¼‰ï¼š
# - æ¸ é“ç±»ï¼š8000+ ä¸ªï¼ˆå¾®ä¿¡ã€æŠ–éŸ³ã€æ·˜å®...ï¼‰
# - åŠŸèƒ½ç±»ï¼šå‡ åƒä¸ªï¼ˆæ¸…ç†ã€å‹ç¼©ã€æ‹ç…§...ï¼‰
# - å¯¹è±¡ç±»ï¼šå‡ åƒä¸ªï¼ˆå›¾ç‰‡ã€è§†é¢‘ã€æ–‡æ¡£...ï¼‰
# - ç¾¤ä½“ç±»ï¼šå‡ ç™¾ä¸ªï¼ˆå­¦ç”Ÿã€è€äººã€ç¨‹åºå‘˜...ï¼‰
```

**å·®è·è¯´æ˜**ï¼š
- ğŸŸ¡ å½“å‰ç³»ç»Ÿæœ‰Tokenæå–åŸºç¡€
- âŒ **ç¼ºå°‘æ¨¡æ¿æå–åŠŸèƒ½**
- âŒ **ç¼ºå°‘è¿­ä»£æ‰©å±•æœºåˆ¶**
- âŒ **ç¼ºå°‘è´¨é‡è¿‡æ»¤ï¼ˆå˜é‡é€‚é…å¤šæ¨¡æ¿ï¼‰**
- ğŸ”´ è¯åº“è§„æ¨¡å·®è·ï¼š26ä¸ª vs å‡ åƒåˆ°8000+

**ä¼˜å…ˆçº§**ï¼šâ­â­â­â­ **æé«˜**ï¼ˆæ„å»ºå®Œæ•´è¯åº“çš„å…³é”®ï¼‰

---

### 6. éœ€æ±‚åˆ†ç±»ä½“ç³»

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **åˆ†ç±»æ¡†æ¶** | âŒ æ— æ˜ç¡®åˆ†ç±»ä½“ç³» | âœ… 6å¤§éœ€æ±‚ç±»åˆ« | ğŸ”´ **ç¼ºå¤±** |
| **éœ€æ±‚ç±»å‹** | ğŸŸ¡ 5ç±»ï¼ˆtool/content/service/education/otherï¼‰ | âœ… å¯»æ‰¾/æ“ä½œ/é—®é¢˜/è¯¢ä»·/æ•™ç¨‹/å…¶ä»– | ğŸŸ¡ **å®šä½ä¸åŒ** |
| **æ¯”ä¾‹åˆ†æ** | âŒ æ— æ¯”ä¾‹ç»Ÿè®¡ | âœ… å¯»æ‰¾ç±»95%ï¼Œå…¶ä»–<5% | ğŸ”´ **ç¼ºå¤±æ´å¯Ÿ** |
| **è‡ªåŠ¨åˆ†ç±»** | ğŸŸ¡ LLMè‡ªåŠ¨ç”Ÿæˆ | ğŸŸ¡ äººå·¥æ ‡æ³¨ä¸ºä¸» | ğŸŸ¢ å„æœ‰ä¼˜åŠ¿ |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`storage/models.py` (demand_typeæšä¸¾)
- å›è¨€æ–¹æ³•è®ºï¼š6å¤§ç±»åˆ«ä½“ç³»ï¼ˆæ–‡æ¡£ä¸­ï¼‰

**å½“å‰ç³»ç»Ÿåˆ†ç±»**ï¼š
```python
# storage/models.py:éœ€æ±‚ç±»å‹
demand_type = Column(
    Enum('tool', 'content', 'service', 'education', 'other'),
    nullable=False
)
# è¯´æ˜ï¼š
# - é¢å‘äº§å“ç±»å‹åˆ†ç±»
# - æ²¡æœ‰æŒ‰æœç´¢æ„å›¾åˆ†ç±»
```

**å›è¨€éœ€æ±‚åˆ†ç±»ä½“ç³»**ï¼š
```python
DEMAND_CATEGORIES = {
    'search': {  # å¯»æ‰¾ç±»ï¼ˆæœ€é‡è¦ï¼Œ95%+ï¼‰
        'download': ['ä¸‹è½½', 'å®‰è£…åŒ…', 'apk'],
        'recommend': ['æ¨è', 'å“ªä¸ªå¥½', 'æ’è¡Œ'],
        'compare': ['å¯¹æ¯”', 'æœ€å¥½çš„', 'vs'],
        'free': ['å…è´¹', 'ç ´è§£ç‰ˆ', 'ç»¿è‰²ç‰ˆ']
    },
    'operation': {  # æ“ä½œç±»ï¼ˆ<2%ï¼‰
        'install': ['æ€ä¹ˆå®‰è£…', 'å®‰è£…æ•™ç¨‹'],
        'use': ['æ€ä¹ˆç”¨', 'ä½¿ç”¨æ–¹æ³•']
    },
    'problem': {  # é—®é¢˜ç±»ï¼ˆ<1%ï¼‰
        'error': ['æ‰“ä¸å¼€', 'é—ªé€€', 'æŠ¥é”™']
    },
    'price': {  # è¯¢ä»·ç±»ï¼ˆæå°‘ï¼‰
        'cost': ['å¤šå°‘é’±', 'ä»·æ ¼', 'æ”¶è´¹']
    },
    'tutorial': {  # æ•™ç¨‹ç±»ï¼ˆ<1%ï¼‰
        'guide': ['æ•™ç¨‹', 'å…¥é—¨', 'å­¦ä¹ ']
    },
    'other': {}  # å…¶ä»–ç±»ï¼ˆ<1%ï¼‰
}

# æ ¸å¿ƒå‘ç°ï¼š
# è½¯ä»¶é¢†åŸŸ 95%+ çš„æœç´¢ = å¯»æ‰¾æŸä¸ªè½¯ä»¶
# è¿™æŒ‡å¯¼SEO/SEMç­–ç•¥ï¼šèšç„¦è½¯ä»¶è¯¦æƒ…é¡µ+èšåˆé¡µ
```

**å·®è·è¯´æ˜**ï¼š
- ğŸ”´ å½“å‰ç³»ç»Ÿæ— éœ€æ±‚åˆ†ç±»æ¡†æ¶ï¼ˆæŒ‰äº§å“ç±»å‹ï¼Œéæœç´¢æ„å›¾ï¼‰
- ğŸ”´ ç¼ºå°‘æ¯”ä¾‹åˆ†æï¼ˆä¸çŸ¥é“å“ªç±»éœ€æ±‚å ä¸»å¯¼ï¼‰
- ğŸ”´ ç¼ºå°‘æœç´¢æ„å›¾ç»´åº¦çš„åˆ†ç±»

**ä¼˜å…ˆçº§**ï¼šâ­â­â­ é«˜ï¼ˆå»ºç«‹åˆ†ç±»ä½“ç³»ï¼ŒæŒ‡å¯¼åç»­åˆ†æï¼‰

---

### 7. æœç´¢ç»“æ„è¯†åˆ«

| åŠŸèƒ½æ¨¡å— | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è·è¯„ä¼° |
|---------|----------|-----------|---------|
| **éœ€æ±‚æ¨¡å¼** | âœ… æå–éœ€æ±‚æ¨¡å¼ï¼ˆintent+action+objectï¼‰ | âœ… æå–æœç´¢ç»“æ„æ¨¡æ¿ | ğŸŸ¢ æ€è·¯ç›¸ä¼¼ |
| **æ¨¡æ¿åº“** | ğŸŸ¡ æ¨¡å¼ç»Ÿè®¡ï¼ˆ14ç§ï¼‰ | âœ… é«˜é¢‘æœç´¢æ¨¡æ¿åº“ | ğŸŸ¡ **è§„æ¨¡ä¸åŒ** |
| **åº”ç”¨ä»·å€¼** | ğŸŸ¡ ç†è§£éœ€æ±‚ç»“æ„ | âœ… TDK/SEM/å†…å®¹æŒ‡å¯¼ | ğŸŸ¡ **åº”ç”¨æ·±åº¦ä¸åŒ** |

**å®ç°æ–‡ä»¶**ï¼š
- å½“å‰ç³»ç»Ÿï¼š`scripts/run_phase5_tokens.py` (éœ€æ±‚æ¨¡å¼åˆ†æ)
- å›è¨€æ–¹æ³•è®ºï¼šé«˜é¢‘ç‰‡æ®µ â†’ æœç´¢ç»“æ„æå–

**å½“å‰ç³»ç»Ÿéœ€æ±‚æ¨¡å¼**ï¼ˆâœ…å·²æœ‰åŸºç¡€ï¼‰ï¼š
```python
# scripts/run_phase5_tokens.py:250-290
# æå–éœ€æ±‚æ¨¡å¼
demand_patterns = extract_demand_patterns(phrases, tokens_with_types)

# è¾“å‡ºç¤ºä¾‹ï¼ˆphase5_framework_report.txtï¼‰ï¼š
# [object] [object]               - 181æ¬¡
# [other] [other]                 - 65æ¬¡
# [other] [other] [other]         - 56æ¬¡
# ...
```

**å›è¨€æœç´¢ç»“æ„æå–**ï¼ˆâŒæœªæ·±åº¦å®ç°ï¼‰ï¼š
```python
# ä»é«˜é¢‘ç‰‡æ®µä¸­æå–æœç´¢ç»“æ„
def extract_search_patterns(high_freq_segments):
    """
    æå–æœç´¢ç»“æ„
    è¿‡æ»¤æ ‡å‡†ï¼š
    1. ä¸æ˜¯æ˜æ˜¾ç¼ºå­—çš„ç‰‡æ®µ
    2. æœ‰å®Œæ•´è¯­ä¹‰çš„ç»“æ„
    3. é¢‘æ¬¡è¶³å¤Ÿé«˜ï¼ˆä»£è¡¨æ€§å¼ºï¼‰
    """
    search_patterns = []

    for segment, freq in high_freq_segments:
        if is_complete_structure(segment):
            search_patterns.append({
                'pattern': segment,
                'frequency': freq,
                'variables': extract_variable_positions(segment)
            })

    return search_patterns

# å…¸å‹æœç´¢ç»“æ„ï¼ˆè½¯ä»¶é¢†åŸŸï¼‰ï¼š
# 1. ä¸‹è½½ç±»ï¼š
#    - [X]è½¯ä»¶ä¸‹è½½
#    - [X]è½¯ä»¶å®‰è£…åŒ…
#    - ä¸‹è½½[X]è½¯ä»¶
#
# 2. æ¨èç±»ï¼š
#    - [X]è½¯ä»¶å“ªä¸ªå¥½
#    - æœ€å¥½çš„[X]è½¯ä»¶
#    - å¥½ç”¨çš„[X]è½¯ä»¶
#
# åº”ç”¨ä»·å€¼ï¼š
# - SEO: TDKæ¨¡æ¿è®¾è®¡å‚è€ƒ
# - SEM: è´¦æˆ·ç»“æ„åˆ’åˆ†ä¾æ®
# - å†…å®¹: æ ‡é¢˜å…¬å¼
```

**å·®è·è¯´æ˜**ï¼š
- âœ… å½“å‰ç³»ç»Ÿå·²æœ‰éœ€æ±‚æ¨¡å¼æå–åŸºç¡€
- ğŸŸ¡ ä½†æœªæ·±å…¥æå–å…·ä½“æœç´¢ç»“æ„æ¨¡æ¿
- ğŸŸ¡ æœªå°†æ¨¡æ¿åº”ç”¨åˆ°SEO/SEMæŒ‡å¯¼

**ä¼˜å…ˆçº§**ï¼šâ­â­ ä¸­ï¼ˆåœ¨æœ‰æ¨¡æ¿-å˜é‡åŠŸèƒ½åå®æ–½ï¼‰

---

## æ ¸å¿ƒç®—æ³•å¯¹æ¯”

### ç®—æ³•1ï¼šæ–‡å­—æ’åºå»é‡

**å½“å‰ç³»ç»Ÿ**ï¼šâŒ æœªå®ç°

**å›è¨€æ–¹æ³•è®º**ï¼šâœ… å·²å®ç°
```python
def create_unique_identifier(keyword, stop_words):
    # 1. å»é™¤åœç”¨è¯
    for stop_word in stop_words:
        keyword = keyword.replace(stop_word, '')
    # 2. å»é™¤ç¬¦å·
    keyword = re.sub(r'[^\w]', '', keyword)
    # 3. æŒ‰æ‹¼éŸ³æ’åº
    chars = list(keyword)
    chars.sort(key=lambda x: pypinyin.lazy_pinyin(x))
    return ''.join(chars)

# æ•ˆæœï¼š
# "å›¾ç‰‡å‹ç¼©" â†’ "å›¾å‹ç‰‡ç¼©"
# "å‹ç¼©å›¾ç‰‡" â†’ "å›¾å‹ç‰‡ç¼©"  âœ“ åˆå¹¶
# "å›¾ç‰‡æ€ä¹ˆå‹ç¼©" â†’ "å›¾å‹ç‰‡ç¼©"  âœ“ åˆå¹¶
```

**å®æ–½å»ºè®®**ï¼š
- ä¸ºè‹±æ–‡åœºæ™¯æ”¹é€ ï¼ˆç”¨å­—æ¯æ’åºæ›¿ä»£æ‹¼éŸ³ï¼‰
- å®Œå–„è‹±æ–‡åœç”¨è¯åº“

---

### ç®—æ³•2ï¼šN-gramç‰‡æ®µç»Ÿè®¡

**å½“å‰ç³»ç»Ÿ**ï¼šâœ… å·²å®ç°åŸºç¡€åŠŸèƒ½
```python
# utils/token_extractor.py å·²å®ç°
def extract_ngrams(phrases, max_gram_size=4, min_frequency=5):
    # âœ“ å·²æœ‰N-gramæå–
    # âœ“ å·²æœ‰é¢‘æ¬¡è¿‡æ»¤
```

**å›è¨€æ–¹æ³•è®º**ï¼šâœ… å®Œæ•´å®ç°
```python
def ngram_segment_statistics(keywords, min_n=3, max_n=5, top_k=10000):
    ngram_counter = Counter()

    for keyword in keywords:
        for n in range(min_n, max_n + 1):
            for i in range(len(keyword) - n + 1):
                segment = keyword[i:i+n]
                ngram_counter[segment] += 1

    return ngram_counter.most_common(top_k)
```

**å·®è·**ï¼š
- âœ… å½“å‰ç³»ç»Ÿå·²æœ‰åŸºç¡€
- âŒ **ç¼ºå°‘å…¨å±€ç»Ÿè®¡åçš„"ç‰‡æ®µæ˜ å°„"åº”ç”¨**

---

### ç®—æ³•3ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£æå–

**å½“å‰ç³»ç»Ÿ**ï¼šâŒ å®Œå…¨æœªå®ç°

**å›è¨€æ–¹æ³•è®º**ï¼šâœ… æ ¸å¿ƒåˆ›æ–°ç®—æ³•
```python
def template_variable_extraction(keywords, seed_variables, max_iterations=3):
    all_templates = set()
    all_variables = set(seed_variables)

    for iteration in range(max_iterations):
        # Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿
        template_counter = Counter()
        for keyword in keywords:
            for var in all_variables:
                if var in keyword:
                    template = keyword.replace(var, '[X]')
                    template_counter[template] += 1

        new_templates = [t for t, freq in template_counter.items() if freq >= 5]
        all_templates.update(new_templates)

        # Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡
        variable_freq = Counter()
        variable_templates = defaultdict(set)

        for template in new_templates:
            pattern = template.replace('[X]', '(.+?)')
            for keyword in keywords:
                match = re.match(pattern, keyword)
                if match:
                    var = match.group(1)
                    variable_freq[var] += 1
                    variable_templates[var].add(template)

        # è´¨é‡è¿‡æ»¤ï¼šé€‚é…æ¨¡æ¿æ•° >= 3
        new_variables = [
            var for var, freq in variable_freq.items()
            if freq >= 5 and len(variable_templates[var]) >= 3
        ]

        all_variables.update(new_variables)

        if len(all_variables) == before_count:
            break  # æ”¶æ•›

    return list(all_templates), list(all_variables)
```

**ä»·å€¼**ï¼š
- ä»å‡ ä¸ªç§å­å˜é‡æ‰©å±•åˆ°å‡ åƒä¸ªå˜é‡
- è´¨é‡é«˜ï¼ˆæ¯ä¸ªå˜é‡é€‚é…å¤šä¸ªæ¨¡æ¿ï¼‰
- è‡ªåŠ¨å‘ç°éšè—çš„æœç´¢æ¨¡å¼

**å®æ–½å»ºè®®**ï¼šâ­â­â­â­â­ æœ€é«˜ä¼˜å…ˆçº§

---

### ç®—æ³•4ï¼šåˆ†æ‰¹èšç±»ä¸åˆå¹¶

**å½“å‰ç³»ç»Ÿ**ï¼šğŸŸ¡ å•æ¬¡èšç±»ï¼ˆ16Gå†…å­˜é™åˆ¶ï¼‰
```python
# core/clustering.py
def fit_predict(self, embeddings: np.ndarray):
    clusterer = hdbscan.HDBSCAN(...)
    labels = clusterer.fit_predict(embeddings)
    # ä¸€æ¬¡æ€§èšç±»
```

**å›è¨€æ–¹æ³•è®º**ï¼šâœ… åˆ†æ‰¹å¤„ç†
```python
def batch_clustering(keywords, batch_size=2000000):
    num_batches = math.ceil(len(keywords) / batch_size)
    batch_results = []

    # åˆ†æ‰¹èšç±»
    for i in range(num_batches):
        batch = keywords[start:end]
        cluster_result = keyword_clustering(batch)
        batch_results.append(cluster_result)

    # å…¨å±€åˆå¹¶
    global_clusters = {}
    for batch_result in batch_results:
        for label, kw_list in batch_result.items():
            if label not in global_clusters:
                global_clusters[label] = []
            global_clusters[label].extend(kw_list)

    return global_clusters
```

**å®æ–½å»ºè®®**ï¼šâ­ ä½ï¼ˆå½“å‰åœºæ™¯ä¸éœ€è¦ï¼‰

---

## æ•°æ®åº“è®¾è®¡å¯¹æ¯”

### è¡¨ç»“æ„å¯¹æ¯”

| è¡¨å | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è· |
|------|----------|-----------|------|
| **phrases** | âœ… å·²æœ‰ | âœ… å¯¹åº” raw_keywords | ğŸŸ¢ åŸºæœ¬ä¸€è‡´ |
| **cluster_meta** | âœ… å·²æœ‰ | âœ… å¯¹åº” cluster_labels | ğŸŸ¢ å·²æœ‰ |
| **demands** | âœ… å·²æœ‰ | âŒ æ— ï¼ˆè¾“å‡ºæŠ¥å‘Šï¼‰ | ğŸŸ¢ å®šä½ä¸åŒ |
| **tokens** | âœ… å·²æœ‰ | âœ… å¯¹åº” feature_variables | ğŸŸ¢ å·²æœ‰åŸºç¡€ |
| **cleaned_keywords** | âŒ æ—  | âœ… å”¯ä¸€æ ‡è¯†å»é‡è¡¨ | ğŸ”´ ç¼ºå¤± |
| **ngram_segments** | âŒ æ—  | âœ… N-gramç‰‡æ®µè¡¨ | ğŸ”´ ç¼ºå¤± |
| **search_templates** | âŒ æ—  | âœ… æœç´¢æ¨¡æ¿è¡¨ | ğŸ”´ ç¼ºå¤± |
| **demand_categories** | âŒ æ—  | âœ… éœ€æ±‚åˆ†ç±»è¡¨ | ğŸ”´ ç¼ºå¤± |

### å½“å‰ç³»ç»Ÿè¡¨ç»“æ„
```sql
-- 1. phrasesï¼ˆçŸ­è¯­æ€»åº“ï¼‰âœ…
CREATE TABLE phrases (
    phrase_id INT PRIMARY KEY,
    phrase VARCHAR(255) UNIQUE,
    cluster_id_A INT,
    cluster_id_B INT,
    mapped_demand_id INT,
    processed_status ENUM('unseen','reviewed','assigned','archived')
);

-- 2. demandsï¼ˆéœ€æ±‚å¡ç‰‡ï¼‰âœ…
CREATE TABLE demands (
    demand_id INT PRIMARY KEY,
    title VARCHAR(255),
    description TEXT,
    demand_type ENUM('tool','content','service','education','other')
);

-- 3. tokensï¼ˆè¯åº“ï¼‰âœ…
CREATE TABLE tokens (
    token_id INT PRIMARY KEY,
    token_text VARCHAR(100) UNIQUE,
    token_type ENUM('intent','action','object','other'),
    in_phrase_count INT
);

-- 4. cluster_metaï¼ˆèšç±»å…ƒæ•°æ®ï¼‰âœ…
CREATE TABLE cluster_meta (
    cluster_id INT PRIMARY KEY,
    cluster_level ENUM('A','B'),
    size INT,
    main_theme VARCHAR(255)
);
```

### å›è¨€æ–¹æ³•è®ºè¡¨ç»“æ„ï¼ˆå»ºè®®æ–°å¢ï¼‰

```sql
-- 5. cleaned_keywordsï¼ˆå”¯ä¸€æ ‡è¯†å»é‡è¡¨ï¼‰âŒæ–°å¢
CREATE TABLE cleaned_keywords (
    unique_id TEXT PRIMARY KEY,   -- å”¯ä¸€æ ‡è¯†ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
    original TEXT NOT NULL,        -- åŸå§‹é•¿å°¾è¯
    created_at TIMESTAMP
);

-- 6. ngram_segmentsï¼ˆN-gramç‰‡æ®µè¡¨ï¼‰âŒæ–°å¢
CREATE TABLE ngram_segments (
    id INT PRIMARY KEY,
    segment TEXT UNIQUE,
    frequency INT,
    gram_size INT,  -- 3-gram, 4-gram, 5-gram
    INDEX idx_frequency (frequency DESC)
);

-- 7. search_templatesï¼ˆæ¨¡æ¿è¡¨ï¼‰âŒæ–°å¢
CREATE TABLE search_templates (
    id INT PRIMARY KEY,
    template TEXT UNIQUE,     -- å¦‚"[X]è½¯ä»¶ä¸‹è½½"
    frequency INT,
    variable_count INT
);

-- 8. feature_variablesï¼ˆæ‰©å±•tokensè¡¨ï¼‰ğŸŸ¡å®Œå–„
ALTER TABLE tokens ADD COLUMN category VARCHAR(20);  -- åŠŸèƒ½/å¯¹è±¡/æ¸ é“/ç¾¤ä½“
ALTER TABLE tokens ADD COLUMN template_count INT;     -- é€‚é…çš„æ¨¡æ¿æ•°
ALTER TABLE tokens ADD COLUMN verified BOOLEAN;       -- æ˜¯å¦äººå·¥éªŒè¯

-- 9. demand_categoriesï¼ˆéœ€æ±‚åˆ†ç±»è¡¨ï¼‰âŒæ–°å¢
CREATE TABLE demand_categories (
    id INT PRIMARY KEY,
    category VARCHAR(50),     -- å¦‚"å¯»æ‰¾-ä¸‹è½½ç±»"
    keyword_count INT,
    percentage DECIMAL(5,2)   -- å æ¯”
);
```

---

## æ€§èƒ½ä¸è§„æ¨¡å¯¹æ¯”

### æ•°æ®è§„æ¨¡å¯¹æ¯”

| æŒ‡æ ‡ | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º | å·®è· |
|------|----------|-----------|------|
| **è¾“å…¥æ•°æ®** | 5-10ä¸‡çŸ­è¯­ | 1.6äº¿ â†’ 5000ä¸‡ | ğŸ”´ 500-1000å€ |
| **æ¸…æ´—å** | 5-10ä¸‡ | 4000ä¸‡ | ğŸ”´ 400-800å€ |
| **èšç±»ç»“æœ** | 60-100ç°‡ | 180ä¸‡æ ‡è¯† | ğŸ”´ 18000å€ |
| **å¯å®¡æ ¸æ ·æœ¬** | 60-100ç°‡ï¼ˆç›´æ¥å®¡æ ¸ï¼‰ | 2ä¸‡æ ·æœ¬ï¼ˆç‰‡æ®µæ˜ å°„ï¼‰ | ğŸŸ¢ æ•°é‡çº§ç›¸è¿‘ |
| **Tokenè¯åº“** | 26ä¸ªï¼ˆæµ‹è¯•ï¼‰ | 8000+ï¼ˆæ¸ é“ç±»ï¼‰ | ğŸ”´ 300å€+ |

### æ—¶é—´æˆæœ¬å¯¹æ¯”

| é˜¶æ®µ | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º |
|------|----------|-----------|
| **æ•°æ®å¯¼å…¥** | 1å°æ—¶ | æ•°å¤©ï¼ˆåˆ†æ‰¹ä¸‹è½½ï¼‰ |
| **æ•°æ®æ¸…æ´—** | 1å°æ—¶ | æ•°å°æ—¶ï¼ˆ4000ä¸‡ï¼‰ |
| **å¤§ç»„èšç±»** | 1å°æ—¶ | æ•°å°æ—¶ï¼ˆ20æ‰¹ï¼‰ |
| **äººå·¥å®¡æ ¸** | 2å°æ—¶ï¼ˆ100ç°‡ï¼‰ | 2å°æ—¶ï¼ˆ2ä¸‡æ ·æœ¬ï¼‰ |
| **Tokenæå–** | 1å°æ—¶ | æ•°å¤©ï¼ˆè¿­ä»£æ‰©å±•ï¼‰ |
| **æ€»è®¡** | 1å¤© | 1-2å‘¨ |

### å†…å­˜å ç”¨å¯¹æ¯”

| æ“ä½œ | å½“å‰ç³»ç»Ÿ | å›è¨€æ–¹æ³•è®º |
|------|----------|-----------|
| **Embeddingè®¡ç®—** | 2-4GB | éœ€åˆ†æ‰¹ |
| **èšç±»è¿ç®—** | 4-8GB | éœ€åˆ†æ‰¹ï¼ˆ200ä¸‡/æ‰¹ï¼‰ |
| **N-gramç»Ÿè®¡** | <1GB | éœ€ä¼˜åŒ–ï¼ˆTrieæ ‘ï¼‰ |

---

## ä¼˜åŠ¿ä¸åŠ£åŠ¿åˆ†æ

### å½“å‰ç³»ç»Ÿä¼˜åŠ¿

âœ… **æŠ€æœ¯æ ˆæˆç†Ÿ**
- ä½¿ç”¨HDBSCANï¼ˆæˆç†Ÿç®—æ³•ï¼‰
- ä½¿ç”¨LLMï¼ˆè‡ªåŠ¨åŒ–é«˜ï¼‰
- ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“ç»´æŠ¤

âœ… **é€‚ç”¨åœºæ™¯æ˜ç¡®**
- è‹±æ–‡å…³é”®è¯ï¼ˆ5-10ä¸‡çº§åˆ«ï¼‰
- äº§å“éœ€æ±‚æŒ–æ˜
- MVPå¿«é€ŸéªŒè¯

âœ… **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜**
- LLMè‡ªåŠ¨ç”Ÿæˆéœ€æ±‚å¡ç‰‡
- LLMè‡ªåŠ¨åˆ†ç±»Token
- å‡å°‘äººå·¥å·¥ä½œé‡

âœ… **Web UIå‹å¥½**
- Streamlitç•Œé¢
- å®æ—¶æ—¥å¿—æŸ¥çœ‹
- å‚æ•°é…ç½®ç›´è§‚

### å½“å‰ç³»ç»ŸåŠ£åŠ¿

âŒ **æ•°æ®æ¸…æ´—ä¸å¤Ÿæ·±åº¦**
- æ— æ³•è¯†åˆ«åŒä¹‰è¡¨è¾¾ï¼ˆ"best calculator" vs "calculator best"ï¼‰
- ç¼ºå°‘æ™ºèƒ½å»é‡ï¼ˆä»…åŸºç¡€å»é‡ï¼‰
- åœç”¨è¯åº“å¯æ‰©å±•

âŒ **ç¼ºå°‘ç‰¹å¾ç‰‡æ®µåˆ†æ**
- æ— æ³•ä»180ä¸‡æ ‡è¯†é™ç»´åˆ°2ä¸‡å¯å®¡æ ¸æ ·æœ¬
- ç¼ºå°‘"ç‰‡æ®µæ˜ å°„å…¨å±€"çš„æ ¸å¿ƒåˆ›æ–°
- äººå·¥å®¡æ ¸å·¥ä½œé‡å¤§ï¼ˆå¦‚æœèšç±»ç»“æœå¤šï¼‰

âŒ **ç¼ºå°‘æ¨¡æ¿-å˜é‡è¿­ä»£**
- Tokenæå–æ— è¿­ä»£æ‰©å±•æœºåˆ¶
- è¯åº“è§„æ¨¡å°ï¼ˆ26ä¸ª vs å‡ åƒåˆ°8000+ï¼‰
- æ— è´¨é‡è¿‡æ»¤ï¼ˆå˜é‡é€‚é…å¤šæ¨¡æ¿ï¼‰

âŒ **ç¼ºå°‘éœ€æ±‚åˆ†ç±»ä½“ç³»**
- æ— æœç´¢æ„å›¾ç»´åº¦åˆ†ç±»
- æ— æ¯”ä¾‹åˆ†æï¼ˆä¸çŸ¥ä¸»å¯¼éœ€æ±‚ï¼‰
- ç¼ºå°‘SEO/SEMåº”ç”¨æŒ‡å¯¼

âŒ **æ‰©å±•æ€§å—é™**
- å•æ¬¡èšç±»ï¼ˆå†…å­˜é™åˆ¶ï¼‰
- ä¸é€‚ç”¨äºè¶…å¤§è§„æ¨¡æ•°æ®ï¼ˆåƒä¸‡çº§ï¼‰

### å›è¨€æ–¹æ³•è®ºä¼˜åŠ¿

âœ… **æ ¸å¿ƒåˆ›æ–°ç®—æ³•**
- ç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼š180ä¸‡ â†’ 2ä¸‡ï¼ˆå¯å®¡æ ¸ï¼‰
- æ¨¡æ¿-å˜é‡è¿­ä»£ï¼šè‡ªåŠ¨æ‰©å±•è¯åº“
- æ–‡å­—æ’åºå»é‡ï¼šè¯†åˆ«åŒä¹‰è¡¨è¾¾

âœ… **å®Œæ•´æ–¹æ³•è®ºä½“ç³»**
- 6å¤§éœ€æ±‚ç±»åˆ«æ¡†æ¶
- 4å¤§ç‰¹å¾å˜é‡ç»´åº¦
- æœç´¢ç»“æ„æ¨¡æ¿åº“

âœ… **é€‚ç”¨è¶…å¤§è§„æ¨¡**
- å¤„ç†1.6äº¿æ•°æ®
- åˆ†æ‰¹èšç±»ï¼ˆ200ä¸‡/æ‰¹ï¼‰
- å†…å­˜ä¼˜åŒ–ç­–ç•¥

âœ… **æ·±åº¦ä¸šåŠ¡æ´å¯Ÿ**
- å‘ç°95%æœç´¢=å¯»æ‰¾éœ€æ±‚
- æŒ‡å¯¼SEO/SEMç­–ç•¥
- æä¾›æœç´¢ç»“æ„æ¨¡æ¿

### å›è¨€æ–¹æ³•è®ºåŠ£åŠ¿

âŒ **äººå·¥å·¥ä½œé‡å¤§**
- éœ€äººå·¥å®¡æ ¸2ä¸‡æ ·æœ¬
- éœ€äººå·¥ç­›é€‰å¤§ç»„
- æ¨¡æ¿-å˜é‡éœ€äººå·¥éªŒè¯

âŒ **æŠ€æœ¯ç»†èŠ‚æœªå…¬å¼€**
- è‡ªç ”èšç±»ç®—æ³•æœªå¼€æº
- å…·ä½“å®ç°ç»†èŠ‚ç¼ºå¤±
- éœ€è‡ªè¡Œç ”å‘

âŒ **ä¸­æ–‡åœºæ™¯ä¸ºä¸»**
- æ‹¼éŸ³æ’åºï¼ˆè‹±æ–‡éœ€æ”¹é€ ï¼‰
- ä¸­æ–‡åœç”¨è¯ï¼ˆè‹±æ–‡éœ€æ›¿æ¢ï¼‰
- ç¤ºä¾‹æ•°æ®ä¸ºä¸­æ–‡

---

## å®æ–½ä¼˜å…ˆçº§å»ºè®®

### ğŸ”´ Priority 1 - ç«‹å³å®æ–½ï¼ˆ1-2å‘¨ï¼‰

#### 1.1 æ–‡å­—æ’åºå»é‡ï¼ˆè‹±æ–‡æ”¹é€ ç‰ˆï¼‰

**ç›®æ ‡**ï¼šè¯†åˆ«åŒä¹‰è¡¨è¾¾ï¼Œé¢å¤–å»é‡10-20%

**å®æ–½æ­¥éª¤**ï¼š
```python
# æ–°å¢æ–‡ä»¶: utils/deduplication.py

def create_unique_identifier_en(phrase: str, stop_words: set) -> str:
    """
    è‹±æ–‡ç‰ˆå”¯ä¸€æ ‡è¯†ç”Ÿæˆ

    æ­¥éª¤ï¼š
    1. å»é™¤åœç”¨è¯
    2. å»é™¤ç¬¦å·å’Œç©ºæ ¼
    3. æŒ‰å­—æ¯æ’åº
    """
    # è½¬å°å†™
    phrase = phrase.lower()

    # å»é™¤åœç”¨è¯
    words = phrase.split()
    words = [w for w in words if w not in stop_words]

    # å»é™¤ç¬¦å·
    text = ''.join(words)
    text = re.sub(r'[^a-z0-9]', '', text)

    # æŒ‰å­—æ¯æ’åº
    chars = sorted(list(text))

    return ''.join(chars)

# é›†æˆåˆ° core/data_integration.py
def merge_and_clean(self, round_id: int = 1):
    # ... åŸæœ‰é€»è¾‘

    # æ–°å¢ï¼šå”¯ä¸€æ ‡è¯†å»é‡
    df['unique_id'] = df['phrase'].apply(
        lambda x: create_unique_identifier_en(x, ALL_STOP_WORDS)
    )
    df = df.drop_duplicates(subset=['unique_id'])

    # ...
```

**é¢„æœŸæ•ˆæœ**ï¼š
- "best calculator" å’Œ "calculator best" è¯†åˆ«ä¸ºåŒä¸€éœ€æ±‚
- é¢å¤–å»é‡10-20%æ•°æ®
- æ¸…æ´—æ•ˆæœæå‡

**å·¥ä½œé‡**ï¼š2-3å¤©

---

#### 1.2 å®Œå–„åœç”¨è¯åº“

**ç›®æ ‡**ï¼šæ‰©å±•è‹±æ–‡åœç”¨è¯ï¼Œæå‡æ¸…æ´—è´¨é‡

**å®æ–½æ­¥éª¤**ï¼š
```python
# æ›´æ–° utils/token_extractor.py

STOP_WORDS_EXTENDED = {
    # åŸæœ‰åœç”¨è¯
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',

    # æ–°å¢ï¼šç–‘é—®è¯
    'what', 'when', 'where', 'who', 'which', 'why', 'how',

    # æ–°å¢ï¼šä»‹è¯
    'about', 'after', 'before', 'between', 'during', 'under', 'over',

    # æ–°å¢ï¼šåŠ¨è¯
    'can', 'could', 'do', 'does', 'did', 'get', 'have', 'has', 'had',

    # æ–°å¢ï¼šå½¢å®¹è¯
    'good', 'best', 'better', 'new', 'old', 'free', 'online',

    # æ–°å¢ï¼šå…¶ä»–
    'top', 'near', 'me', 'my', 'you', 'your'
}
```

**å·¥ä½œé‡**ï¼š1å¤©

---

### ğŸŸ  Priority 2 - æ ¸å¿ƒåŠŸèƒ½ï¼ˆ1ä¸ªæœˆï¼‰

#### 2.1 å®ç°Phase 4.5ï¼šç‰¹å¾ç‰‡æ®µåˆ†æ â­â­â­â­â­

**ç›®æ ‡**ï¼šä»N-gramç»Ÿè®¡ä¸­æå–Topç‰‡æ®µï¼Œæ˜ å°„å…¨å±€éœ€æ±‚

**å®æ–½æ­¥éª¤**ï¼š

**Step 1ï¼šåˆ›å»ºN-gramç‰‡æ®µè¡¨**
```sql
-- storage/models.py æ–°å¢æ¨¡å‹
CREATE TABLE ngram_segments (
    id INT PRIMARY KEY,
    segment TEXT UNIQUE,
    frequency INT,
    gram_size INT,
    created_at TIMESTAMP
);
CREATE INDEX idx_frequency ON ngram_segments(frequency DESC);
```

**Step 2ï¼šå®ç°å…¨å±€N-gramç»Ÿè®¡**
```python
# æ–°å¢æ–‡ä»¶: core/segment_analysis.py

def extract_ngrams_global(phrases: List[str],
                          min_n=3, max_n=5) -> Counter:
    """
    å…¨å±€N-gramç‰‡æ®µç»Ÿè®¡
    """
    from collections import Counter
    ngram_counter = Counter()

    for phrase in tqdm(phrases, desc="Extracting N-grams"):
        for n in range(min_n, max_n + 1):
            words = phrase.split()
            # æå–è¿ç»­nä¸ªè¯çš„ç»„åˆ
            for i in range(len(words) - n + 1):
                segment = ' '.join(words[i:i+n])
                ngram_counter[segment] += 1

    return ngram_counter

def find_mother_keywords(segment: str,
                         phrases: List[str],
                         count=2) -> List[str]:
    """
    æ‰¾åˆ°åŒ…å«è¯¥ç‰‡æ®µçš„æ¯è¯ï¼ˆä»£è¡¨çŸ­è¯­ï¼‰
    """
    mothers = []
    for phrase in phrases:
        if segment in phrase:
            mothers.append(phrase)
            if len(mothers) >= count:
                break
    return mothers
```

**Step 3ï¼šåˆ›å»ºPhase 4.5è„šæœ¬**
```python
# æ–°å¢æ–‡ä»¶: scripts/run_phase4_5_segments.py

def run_phase4_5_segments(sample_size=10000, min_frequency=8):
    """
    Phase 4.5: ç‰¹å¾ç‰‡æ®µåˆ†æ

    æµç¨‹ï¼š
    1. åŠ è½½æ‰€æœ‰çŸ­è¯­
    2. N-gramå…¨å±€ç»Ÿè®¡
    3. é€‰æ‹©Topç‰‡æ®µ
    4. æå–æ ·æœ¬è¯
    5. å¯¹æ ·æœ¬è¯èšç±»
    6. ç”Ÿæˆå¯å®¡æ ¸æŠ¥å‘Š
    """
    print("\nã€Phase 4.5ã€‘ç‰¹å¾ç‰‡æ®µåˆ†æ")

    # 1. åŠ è½½çŸ­è¯­
    with PhraseRepository() as repo:
        phrases_db = repo.session.query(Phrase).all()
        phrases = [p.phrase for p in phrases_db]

    print(f"âœ“ åŠ è½½ {len(phrases)} æ¡çŸ­è¯­")

    # 2. N-gramç»Ÿè®¡
    print("\næ‰§è¡ŒN-gramå…¨å±€ç»Ÿè®¡...")
    ngram_stats = extract_ngrams_global(phrases, min_n=3, max_n=5)

    print(f"âœ“ ç»Ÿè®¡åˆ° {len(ngram_stats)} ä¸ªä¸åŒç‰‡æ®µ")

    # 3. é€‰æ‹©Topç‰‡æ®µ
    top_segments = ngram_stats.most_common(sample_size)

    print(f"âœ“ é€‰æ‹©Top {sample_size} ä¸ªé«˜é¢‘ç‰‡æ®µ")
    print(f"\nTop 10ç‰‡æ®µ:")
    for i, (segment, freq) in enumerate(top_segments[:10], 1):
        print(f"  {i}. '{segment}' - {freq:,}æ¬¡")

    # 4. æå–æ ·æœ¬è¯
    print("\nä»é«˜é¢‘ç‰‡æ®µä¸­æå–æ ·æœ¬è¯...")
    sample_keywords = []
    for segment, freq in tqdm(top_segments, desc="Finding mother keywords"):
        mothers = find_mother_keywords(segment, phrases, count=2)
        sample_keywords.extend(mothers)

    sample_keywords = list(set(sample_keywords))  # å»é‡
    print(f"âœ“ å¾—åˆ° {len(sample_keywords)} ä¸ªæ ·æœ¬è¯")

    # 5. å¯¹æ ·æœ¬è¯èšç±»ï¼ˆå¿«é€Ÿï¼ï¼‰
    print("\nå¯¹æ ·æœ¬è¯è¿›è¡Œèšç±»...")
    # é‡ç”¨Phase 2çš„èšç±»é€»è¾‘
    from core.embedding import EmbeddingService
    from core.clustering import cluster_phrases_large

    embedding_service = EmbeddingService(use_cache=True)
    sample_phrases = [{'phrase': kw, 'phrase_id': i}
                      for i, kw in enumerate(sample_keywords)]
    embeddings, phrase_ids = embedding_service.embed_phrases_from_db(
        sample_phrases, round_id=999  # ç‰¹æ®Šè½®æ¬¡ID
    )

    cluster_ids, cluster_info, clusterer = cluster_phrases_large(
        embeddings, sample_phrases
    )

    print(f"âœ“ èšç±»å®Œæˆï¼š{len(cluster_info)} ä¸ªç°‡")

    # 6. ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆå¯äººå·¥å®¡æ ¸ï¼‰
    print("\nç”Ÿæˆå®¡æ ¸æŠ¥å‘Š...")
    report_file = OUTPUT_DIR / 'phase4_5_segment_clusters.html'
    generate_segment_report(cluster_info, report_file)

    print(f"\nâœ… Phase 4.5å®Œæˆï¼")
    print(f"   - é«˜é¢‘ç‰‡æ®µæ•°: {len(top_segments)}")
    print(f"   - æ ·æœ¬è¯æ•°: {len(sample_keywords)}")
    print(f"   - èšç±»ç°‡æ•°: {len(cluster_info)}")
    print(f"   - å®¡æ ¸æŠ¥å‘Š: {report_file}")
    print(f"\nğŸ“Œ ä¸‹ä¸€æ­¥: äººå·¥å®¡æ ¸æŠ¥å‘Šï¼Œæ ‡è®°éœ€æ±‚ç±»åˆ«")
```

**Step 4ï¼šé›†æˆåˆ°Web UI**
```python
# ui/pages/phase4_5_segments.pyï¼ˆæ–°å¢é¡µé¢ï¼‰

import streamlit as st
import subprocess

st.title("ğŸ“Š Phase 4.5: ç‰¹å¾ç‰‡æ®µåˆ†æ")

st.markdown("""
### ğŸ¯ ç›®æ ‡
é€šè¿‡N-gramç‰‡æ®µç»Ÿè®¡ï¼Œä»å¤§é‡èšç±»ç»“æœä¸­æå–å¯å®¡æ ¸çš„æ ·æœ¬è¯ã€‚

### ğŸ“‹ æµç¨‹
1. å…¨å±€N-gramç»Ÿè®¡
2. é€‰æ‹©Topé«˜é¢‘ç‰‡æ®µ
3. æå–æ ·æœ¬è¯
4. èšç±»æ ·æœ¬è¯
5. ç”Ÿæˆå®¡æ ¸æŠ¥å‘Š

### â­ æ ¸å¿ƒä»·å€¼
- å°†180ä¸‡æ ‡è¯† â†’ 2ä¸‡æ ·æœ¬è¯
- äººå·¥å®¡æ ¸æ—¶é—´ï¼š< 2å°æ—¶
""")

sample_size = st.number_input("æ ·æœ¬ç‰‡æ®µæ•°", 1000, 50000, 10000, 1000)
min_frequency = st.number_input("æœ€å°é¢‘æ¬¡", 3, 100, 8, 1)

if st.button("å¼€å§‹åˆ†æ", type="primary"):
    cmd = [
        "python", "scripts/run_phase4_5_segments.py",
        f"--sample-size={sample_size}",
        f"--min-frequency={min_frequency}"
    ]

    # æ‰§è¡Œï¼ˆåŒPhase 5çš„æ‰§è¡Œæ–¹å¼ï¼‰
    ...
```

**é¢„æœŸæ•ˆæœ**ï¼š
- ä»å¤§é‡èšç±»ç»“æœä¸­å¿«é€Ÿæå–æ ·æœ¬
- äººå·¥å®¡æ ¸æ—¶é—´ä»"ä¸å¯èƒ½"å˜ä¸º"<2å°æ—¶"
- è¦†ç›–å…¨å±€éœ€æ±‚ï¼ˆé€šè¿‡ç‰‡æ®µæ˜ å°„ï¼‰

**å·¥ä½œé‡**ï¼š1å‘¨

---

#### 2.2 å®ç°æ¨¡æ¿-å˜é‡è¿­ä»£æå– â­â­â­â­â­

**ç›®æ ‡**ï¼šè‡ªåŠ¨æ‰©å±•è¯åº“ï¼Œæ„å»ºå®Œæ•´ç‰¹å¾å˜é‡åº“

**å®æ–½æ­¥éª¤**ï¼š

**Step 1ï¼šæ‰©å±•tokensè¡¨**
```sql
-- storage/models.py æ›´æ–°Tokenæ¨¡å‹
ALTER TABLE tokens ADD COLUMN category VARCHAR(20);      -- åŠŸèƒ½/å¯¹è±¡/æ¸ é“/ç¾¤ä½“
ALTER TABLE tokens ADD COLUMN template_count INT;        -- é€‚é…çš„æ¨¡æ¿æ•°
ALTER TABLE tokens ADD COLUMN gram_size INT DEFAULT 1;   -- n-gramå¤§å°
```

**Step 2ï¼šåˆ›å»ºsearch_templatesè¡¨**
```sql
CREATE TABLE search_templates (
    id INT PRIMARY KEY AUTO_INCREMENT,
    template TEXT UNIQUE,         -- å¦‚"best [X] for"
    frequency INT,
    variable_count INT,
    created_at TIMESTAMP
);
```

**Step 3ï¼šå®ç°è¿­ä»£æå–ç®—æ³•**
```python
# æ–°å¢æ–‡ä»¶: core/template_extraction.py

from collections import Counter, defaultdict
import re

def template_variable_extraction(
    phrases: List[str],
    seed_variables: List[str],
    max_iterations=3,
    min_template_freq=5,
    min_variable_freq=5,
    min_template_match=3
) -> Tuple[List[str], List[str]]:
    """
    æ¨¡æ¿-å˜é‡è¿­ä»£æå–ç®—æ³•

    Args:
        phrases: çŸ­è¯­åˆ—è¡¨
        seed_variables: ç§å­å˜é‡
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        min_template_freq: æ¨¡æ¿æœ€å°é¢‘æ¬¡
        min_variable_freq: å˜é‡æœ€å°é¢‘æ¬¡
        min_template_match: å˜é‡éœ€é€‚é…çš„æœ€å°æ¨¡æ¿æ•°

    Returns:
        (templates, variables)
    """
    all_templates = set()
    all_variables = set(seed_variables)

    print(f"\nå¼€å§‹æ¨¡æ¿-å˜é‡è¿­ä»£æå–ï¼ˆç§å­å˜é‡æ•°ï¼š{len(seed_variables)}ï¼‰")

    for iteration in range(max_iterations):
        print(f"\n=== Iteration {iteration + 1}/{max_iterations} ===")

        # Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿
        print("  Phase 1: ç”¨å˜é‡æå–æ¨¡æ¿...")
        template_counter = Counter()

        for phrase in phrases:
            for var in all_variables:
                # å®Œæ•´è¯åŒ¹é…ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
                if re.search(r'\b' + re.escape(var) + r'\b', phrase):
                    template = re.sub(
                        r'\b' + re.escape(var) + r'\b',
                        '[X]',
                        phrase
                    )
                    template_counter[template] += 1

        # è¿‡æ»¤ä½é¢‘æ¨¡æ¿
        new_templates = [
            t for t, freq in template_counter.items()
            if freq >= min_template_freq
        ]

        before_templates = len(all_templates)
        all_templates.update(new_templates)
        after_templates = len(all_templates)

        print(f"    å‘ç°æ–°æ¨¡æ¿: {after_templates - before_templates}")
        print(f"    ç´¯è®¡æ¨¡æ¿æ•°: {after_templates}")

        # æ˜¾ç¤ºTop 10æ–°æ¨¡æ¿
        top_new_templates = sorted(
            [(t, template_counter[t]) for t in new_templates],
            key=lambda x: x[1],
            reverse=True
        )[:10]
        for template, freq in top_new_templates:
            print(f"      '{template}' - {freq}æ¬¡")

        # Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡
        print("\n  Phase 2: ç”¨æ¨¡æ¿æå–å˜é‡...")
        variable_freq = Counter()
        variable_templates = defaultdict(set)

        for template in new_templates:
            # å°†[X]æ›¿æ¢ä¸ºæ­£åˆ™æ•è·ç»„
            pattern = template.replace('[X]', '(.+?)')
            pattern = '^' + pattern + '$'  # å®Œæ•´åŒ¹é…

            for phrase in phrases:
                match = re.match(pattern, phrase)
                if match:
                    var = match.group(1).strip()
                    # è¿‡æ»¤ï¼šå˜é‡é•¿åº¦åˆç†ï¼ˆ2-30å­—ç¬¦ï¼‰
                    if 2 <= len(var) <= 30:
                        variable_freq[var] += 1
                        variable_templates[var].add(template)

        # è´¨é‡è¿‡æ»¤ï¼šé€‚é…æ¨¡æ¿æ•° >= min_template_match ä¸” é¢‘æ¬¡ >= min_variable_freq
        new_variables = [
            var for var, freq in variable_freq.items()
            if freq >= min_variable_freq
            and len(variable_templates[var]) >= min_template_match
        ]

        before_variables = len(all_variables)
        all_variables.update(new_variables)
        after_variables = len(all_variables)

        print(f"    å‘ç°æ–°å˜é‡: {after_variables - before_variables}")
        print(f"    ç´¯è®¡å˜é‡æ•°: {after_variables}")

        # æ˜¾ç¤ºTop 10æ–°å˜é‡
        top_new_variables = sorted(
            [(v, variable_freq[v], len(variable_templates[v]))
             for v in new_variables],
            key=lambda x: x[1],
            reverse=True
        )[:10]
        for var, freq, template_count in top_new_variables:
            print(f"      '{var}' - {freq}æ¬¡, é€‚é…{template_count}ä¸ªæ¨¡æ¿")

        # æ”¶æ•›åˆ¤æ–­
        if after_variables == before_variables:
            print("\n  âœ“ å·²æ”¶æ•›ï¼ˆæ— æ–°å˜é‡ï¼‰")
            break

    print(f"\nâœ… è¿­ä»£å®Œæˆï¼")
    print(f"   - æ€»æ¨¡æ¿æ•°: {len(all_templates)}")
    print(f"   - æ€»å˜é‡æ•°: {len(all_variables)}")

    return list(all_templates), list(all_variables)
```

**Step 4ï¼šåˆ›å»ºPhase 5.5è„šæœ¬**
```python
# æ–°å¢æ–‡ä»¶: scripts/run_phase5_5_templates.py

def run_phase5_5_templates():
    """
    Phase 5.5: æ¨¡æ¿-å˜é‡è¿­ä»£æå–
    """
    print("\nã€Phase 5.5ã€‘æ¨¡æ¿-å˜é‡è¿­ä»£æå–")

    # 1. åŠ è½½çŸ­è¯­
    with PhraseRepository() as repo:
        phrases_db = repo.session.query(Phrase).all()
        phrases = [p.phrase for p in phrases_db]

    # 2. å‡†å¤‡ç§å­å˜é‡ï¼ˆä»ç°æœ‰tokensä¸­é€‰æ‹©ï¼‰
    with TokenRepository() as repo:
        seed_tokens = repo.session.query(Token).filter(
            Token.token_type.in_(['object', 'action'])
        ).all()
        seed_variables = [t.token_text for t in seed_tokens]

    print(f"âœ“ ç§å­å˜é‡æ•°: {len(seed_variables)}")

    # 3. æ‰§è¡Œè¿­ä»£æå–
    templates, variables = template_variable_extraction(
        phrases=phrases,
        seed_variables=seed_variables,
        max_iterations=3,
        min_template_freq=5,
        min_variable_freq=5,
        min_template_match=3
    )

    # 4. ä¿å­˜æ¨¡æ¿åˆ°æ•°æ®åº“
    print("\nä¿å­˜æ¨¡æ¿åˆ°æ•°æ®åº“...")
    with SearchTemplateRepository() as repo:
        for template in tqdm(templates, desc="Saving templates"):
            repo.create_or_update_template(
                template=template,
                frequency=calculate_template_frequency(template, phrases)
            )

    # 5. ä¿å­˜å˜é‡åˆ°tokensè¡¨
    print("\nä¿å­˜å˜é‡åˆ°tokensè¡¨...")
    with TokenRepository() as repo:
        for variable in tqdm(variables, desc="Saving variables"):
            # ä½¿ç”¨LLMåˆ†ç±»å˜é‡ç±»åˆ«ï¼ˆåŠŸèƒ½/å¯¹è±¡/æ¸ é“/ç¾¤ä½“ï¼‰
            category = classify_variable_category(variable)

            repo.create_or_update_token(
                token_text=variable,
                token_type='object',  # é»˜è®¤ç±»å‹
                category=category,
                in_phrase_count=count_phrase_occurrence(variable, phrases),
                template_count=count_template_matches(variable, templates),
                gram_size=len(variable.split())
            )

    # 6. ç”ŸæˆæŠ¥å‘Š
    generate_template_variable_report(templates, variables)

    print("\nâœ… Phase 5.5å®Œæˆï¼")
```

**é¢„æœŸæ•ˆæœ**ï¼š
- è¯åº“ä»26ä¸ªæ‰©å±•åˆ°æ•°ç™¾/æ•°åƒä¸ª
- æ¯ä¸ªå˜é‡è´¨é‡é«˜ï¼ˆé€‚é…å¤šä¸ªæ¨¡æ¿ï¼‰
- è‡ªåŠ¨å‘ç°éšè—çš„æœç´¢æ¨¡å¼

**å·¥ä½œé‡**ï¼š1.5å‘¨

---

#### 2.3 å»ºç«‹éœ€æ±‚åˆ†ç±»ä½“ç³»

**ç›®æ ‡**ï¼šæ„å»ºæœç´¢æ„å›¾ç»´åº¦çš„éœ€æ±‚åˆ†ç±»æ¡†æ¶

**å®æ–½æ­¥éª¤**ï¼š

**Step 1ï¼šå®šä¹‰åˆ†ç±»æ¡†æ¶**
```python
# config/demand_categories.pyï¼ˆæ–°å¢é…ç½®æ–‡ä»¶ï¼‰

DEMAND_CATEGORIES = {
    'search': {  # å¯»æ‰¾ç±»ï¼ˆä¸»å¯¼ï¼‰
        'download': {
            'keywords': ['download', 'free download', 'get', 'install'],
            'description': 'å¯»æ‰¾ä¸‹è½½èµ„æº'
        },
        'recommend': {
            'keywords': ['best', 'top', 'recommend', 'popular'],
            'description': 'å¯»æ‰¾æ¨è/å¯¹æ¯”'
        },
        'compare': {
            'keywords': ['vs', 'versus', 'compare', 'better'],
            'description': 'å¯»æ‰¾å¯¹æ¯”ä¿¡æ¯'
        },
        'free': {
            'keywords': ['free', 'open source', 'no cost'],
            'description': 'å¯»æ‰¾å…è´¹èµ„æº'
        }
    },
    'operation': {  # æ“ä½œç±»
        'how_to': {
            'keywords': ['how to', 'how do i', 'tutorial', 'guide'],
            'description': 'æ“ä½œæ•™ç¨‹'
        },
        'install': {
            'keywords': ['install', 'setup', 'configure'],
            'description': 'å®‰è£…é…ç½®'
        }
    },
    'problem': {  # é—®é¢˜ç±»
        'error': {
            'keywords': ['error', 'fix', 'not working', 'issue'],
            'description': 'é”™è¯¯ä¿®å¤'
        }
    },
    'price': {  # è¯¢ä»·ç±»
        'cost': {
            'keywords': ['price', 'cost', 'how much', 'pricing'],
            'description': 'ä»·æ ¼å’¨è¯¢'
        }
    },
    'tutorial': {  # æ•™ç¨‹ç±»
        'guide': {
            'keywords': ['tutorial', 'learn', 'course', 'training'],
            'description': 'å­¦ä¹ æ•™ç¨‹'
        }
    },
    'other': {}  # å…¶ä»–ç±»
}
```

**Step 2ï¼šå®ç°è‡ªåŠ¨åˆ†ç±»**
```python
# utils/demand_classifier.pyï¼ˆæ–°å¢å·¥å…·ï¼‰

def classify_demand(phrase: str, tokens: List[Dict]) -> Dict:
    """
    å¯¹çŸ­è¯­è¿›è¡Œéœ€æ±‚åˆ†ç±»

    Args:
        phrase: çŸ­è¯­æ–‡æœ¬
        tokens: çŸ­è¯­åŒ…å«çš„tokensï¼ˆå«ç±»å‹ï¼‰

    Returns:
        åˆ†ç±»ç»“æœ {'main_category': ..., 'sub_category': ..., 'confidence': ...}
    """
    phrase_lower = phrase.lower()

    # éå†åˆ†ç±»æ¡†æ¶
    for main_cat, sub_cats in DEMAND_CATEGORIES.items():
        if not sub_cats:  # otherç±»
            continue

        for sub_cat, config in sub_cats.items():
            keywords = config['keywords']

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            for keyword in keywords:
                if keyword in phrase_lower:
                    return {
                        'main_category': main_cat,
                        'sub_category': sub_cat,
                        'confidence': 'high',
                        'matched_keyword': keyword
                    }

    # ä½¿ç”¨tokenç±»å‹è¾…åŠ©åˆ¤æ–­
    token_types = [t['token_type'] for t in tokens]
    if 'intent' in token_types:
        # åŒ…å«intent tokenï¼Œå¯èƒ½æ˜¯searchç±»
        return {
            'main_category': 'search',
            'sub_category': 'recommend',
            'confidence': 'medium',
            'matched_keyword': None
        }

    # é»˜è®¤åˆ†ç±»
    return {
        'main_category': 'other',
        'sub_category': None,
        'confidence': 'low',
        'matched_keyword': None
    }
```

**Step 3ï¼šæ‰¹é‡åˆ†ç±»ä¸ç»Ÿè®¡**
```python
# scripts/analyze_demand_distribution.pyï¼ˆæ–°å¢åˆ†æè„šæœ¬ï¼‰

def analyze_demand_distribution():
    """
    åˆ†æéœ€æ±‚åˆ†ç±»åˆ†å¸ƒ
    """
    print("\nã€éœ€æ±‚åˆ†ç±»åˆ†å¸ƒåˆ†æã€‘")

    # 1. åŠ è½½æ‰€æœ‰çŸ­è¯­
    with PhraseRepository() as repo:
        phrases_db = repo.session.query(Phrase).all()

    # 2. æ‰¹é‡åˆ†ç±»
    print("\nå¯¹çŸ­è¯­è¿›è¡Œåˆ†ç±»...")
    category_counter = Counter()

    for phrase_db in tqdm(phrases_db, desc="Classifying"):
        phrase = phrase_db.phrase

        # è·å–tokens
        tokens = get_phrase_tokens(phrase)

        # åˆ†ç±»
        result = classify_demand(phrase, tokens)

        category_key = f"{result['main_category']}-{result['sub_category']}"
        category_counter[category_key] += 1

    # 3. ç»Ÿè®¡åˆ†æ
    print("\nã€éœ€æ±‚åˆ†ç±»ç»Ÿè®¡ã€‘")
    total = sum(category_counter.values())

    for category, count in category_counter.most_common():
        percentage = count / total * 100
        print(f"  {category:30s}: {count:6d} ({percentage:5.1f}%)")

    # 4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    generate_category_distribution_chart(category_counter)

    print("\nâœ… åˆ†æå®Œæˆï¼")
```

**é¢„æœŸæ•ˆæœ**ï¼š
- å»ºç«‹å®Œæ•´éœ€æ±‚åˆ†ç±»æ¡†æ¶
- äº†è§£å„ç±»éœ€æ±‚å æ¯”
- æŒ‡å¯¼åç»­SEO/SEMç­–ç•¥

**å·¥ä½œé‡**ï¼š3-5å¤©

---

### ğŸŸ¡ Priority 3 - å®Œå–„ç³»ç»Ÿï¼ˆ2-3ä¸ªæœˆï¼‰

#### 3.1 æ„å»ºçŸ¥è¯†å›¾è°±

**ç›®æ ‡**ï¼šå»ºç«‹éœ€æ±‚ç±»åˆ«â†’æ¨¡æ¿â†’å˜é‡â†’çŸ­è¯­çš„å…³è”å›¾è°±

```python
çŸ¥è¯†å›¾è°±ç»“æ„ï¼š

éœ€æ±‚ç±»åˆ«ï¼ˆ6å¤§ç±»ï¼‰
  â†“
æœç´¢æ¨¡æ¿ï¼ˆæ•°ç™¾ä¸ªï¼‰
  â†“
ç‰¹å¾å˜é‡ï¼ˆæ•°åƒä¸ªï¼‰
  â†“
å…·ä½“çŸ­è¯­ï¼ˆæ•°ä¸‡ä¸ªï¼‰

ç¤ºä¾‹ï¼š
  å¯»æ‰¾-ä¸‹è½½ç±» â†’
    "best [X] for [Y]" â†’
      X=calculator, Y=students â†’
        "best calculator for students"
```

**å·¥ä½œé‡**ï¼š2å‘¨

---

#### 3.2 å¼€å‘å¯è§†åŒ–å·¥å…·

**åŠŸèƒ½åˆ—è¡¨**ï¼š
- éœ€æ±‚åˆ†ç±»åˆ†å¸ƒé¥¼å›¾
- é«˜é¢‘ç‰‡æ®µè¯äº‘
- ç‰¹å¾å˜é‡åˆ†ç±»æ ‘
- æœç´¢ç»“æ„æ¨¡æ¿åº“
- å˜é‡å…³è”ç½‘ç»œå›¾

**å·¥ä½œé‡**ï¼š2å‘¨

---

#### 3.3 å»ºç«‹è‡ªåŠ¨åŒ–Pipeline

**ç›®æ ‡**ï¼šå®šæœŸè‡ªåŠ¨è¿è¡Œå®Œæ•´æµç¨‹

```python
å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹ï¼š

æ•°æ®å¯¼å…¥ï¼ˆPhase 1ï¼‰
  â†“
æ–‡å­—æ’åºå»é‡ï¼ˆPhase 2ï¼‰
  â†“
å¤§ç»„èšç±»ï¼ˆPhase 2ï¼‰
  â†“
ç‰¹å¾ç‰‡æ®µåˆ†æï¼ˆPhase 4.5ï¼‰
  â†“
æ ·æœ¬èšç±»ï¼ˆPhase 4.5ï¼‰
  â†“
éœ€æ±‚åˆ†ç±»ï¼ˆè‡ªåŠ¨ï¼‰
  â†“
æ¨¡æ¿-å˜é‡è¿­ä»£ï¼ˆPhase 5.5ï¼‰
  â†“
ç”Ÿæˆå®Œæ•´æŠ¥å‘Š

æ—¶é—´ï¼šå…¨è‡ªåŠ¨ < 24å°æ—¶ï¼ˆæ— éœ€äººå·¥ï¼‰
```

**å·¥ä½œé‡**ï¼š1å‘¨

---

## æ€»ç»“

### å…³é”®å·®è·

1. **ğŸ”´ æœ€å¤§å·®è·**ï¼šç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼ˆPhase 4.5ï¼‰
   - å›è¨€æ ¸å¿ƒåˆ›æ–°ï¼š180ä¸‡æ ‡è¯† â†’ 2ä¸‡æ ·æœ¬
   - å½“å‰ç³»ç»Ÿï¼šå®Œå…¨ç¼ºå¤±
   - **å½±å“**ï¼šå¤§è§„æ¨¡æ•°æ®æ— æ³•äººå·¥å®¡æ ¸

2. **ğŸ”´ ç¬¬äºŒå·®è·**ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£æå–ï¼ˆPhase 5.5ï¼‰
   - å›è¨€æ ¸å¿ƒåˆ›æ–°ï¼šè‡ªåŠ¨æ‰©å±•è¯åº“åˆ°æ•°åƒ/8000+
   - å½“å‰ç³»ç»Ÿï¼šä»…26ä¸ªtokenï¼ˆæµ‹è¯•æ•°æ®ï¼‰
   - **å½±å“**ï¼šè¯åº“ä¸å®Œæ•´ï¼Œæ— æ³•æ·±åº¦åˆ†æ

3. **ğŸŸ¡ ç¬¬ä¸‰å·®è·**ï¼šæ–‡å­—æ’åºå»é‡
   - å›è¨€æ–¹æ³•ï¼šé¢å¤–å»é‡20%
   - å½“å‰ç³»ç»Ÿï¼šåŸºç¡€å»é‡
   - **å½±å“**ï¼šæ•°æ®å†—ä½™ï¼Œæµªè´¹è®¡ç®—

4. **ğŸŸ¡ ç¬¬å››å·®è·**ï¼šéœ€æ±‚åˆ†ç±»ä½“ç³»
   - å›è¨€æ–¹æ³•ï¼š6å¤§ç±»åˆ«+æ¯”ä¾‹åˆ†æ
   - å½“å‰ç³»ç»Ÿï¼šæ— åˆ†ç±»ä½“ç³»
   - **å½±å“**ï¼šç¼ºå°‘ä¸šåŠ¡æ´å¯Ÿ

### å®æ–½è·¯çº¿å›¾

```
ç¬¬1-2å‘¨ï¼ˆPriority 1ï¼‰ï¼š
  âœ… æ–‡å­—æ’åºå»é‡ï¼ˆè‹±æ–‡ç‰ˆï¼‰
  âœ… å®Œå–„åœç”¨è¯åº“

ç¬¬3-4å‘¨ï¼ˆPriority 2.1ï¼‰ï¼š
  âœ… Phase 4.5ï¼šç‰¹å¾ç‰‡æ®µåˆ†æ
  âœ… é›†æˆåˆ°Web UI

ç¬¬5-6å‘¨ï¼ˆPriority 2.2ï¼‰ï¼š
  âœ… Phase 5.5ï¼šæ¨¡æ¿-å˜é‡è¿­ä»£
  âœ… æ‰©å±•æ•°æ®åº“è¡¨ç»“æ„

ç¬¬7-8å‘¨ï¼ˆPriority 2.3ï¼‰ï¼š
  âœ… å»ºç«‹éœ€æ±‚åˆ†ç±»ä½“ç³»
  âœ… æ‰¹é‡åˆ†ç±»ä¸ç»Ÿè®¡

ç¬¬9-12å‘¨ï¼ˆPriority 3ï¼‰ï¼š
  â¸ çŸ¥è¯†å›¾è°±æ„å»º
  â¸ å¯è§†åŒ–å·¥å…·å¼€å‘
  â¸ è‡ªåŠ¨åŒ–Pipeline
```

### æœ€ç»ˆç›®æ ‡

é€šè¿‡å®æ–½ä»¥ä¸Šä¼˜åŒ–ï¼Œå½“å‰ç³»ç»Ÿå°†å…·å¤‡ï¼š
1. âœ… æ·±åº¦æ•°æ®æ¸…æ´—èƒ½åŠ›ï¼ˆæ–‡å­—æ’åºå»é‡ï¼‰
2. âœ… å¤§è§„æ¨¡å¯å®¡æ ¸èƒ½åŠ›ï¼ˆç‰¹å¾ç‰‡æ®µæ˜ å°„ï¼‰
3. âœ… è‡ªåŠ¨è¯åº“æ‰©å±•èƒ½åŠ›ï¼ˆæ¨¡æ¿-å˜é‡è¿­ä»£ï¼‰
4. âœ… å®Œæ•´åˆ†ç±»ä½“ç³»ï¼ˆ6å¤§éœ€æ±‚ç±»åˆ«ï¼‰
5. âœ… ä¸šåŠ¡æ´å¯Ÿèƒ½åŠ›ï¼ˆæ¯”ä¾‹åˆ†æ+æœç´¢ç»“æ„ï¼‰

**æœ€ç»ˆæ•ˆæœ**ï¼š
- ä¿æŒMVPæ¶æ„çš„ç®€æ´æ€§
- å¸æ”¶å›è¨€æ–¹æ³•è®ºçš„æ ¸å¿ƒåˆ›æ–°
- é€‚ç”¨äºè‹±æ–‡å…³é”®è¯åœºæ™¯
- æ”¯æŒ5-10ä¸‡åˆ°æ•°ç™¾ä¸‡çº§åˆ«æ•°æ®
- æä¾›å®Œæ•´çš„éœ€æ±‚æŒ–æ˜èƒ½åŠ›

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-12-23
**ç»´æŠ¤è€…**: Claude + User

---

> ğŸ’¡ **æ ¸å¿ƒå¯ç¤º**ï¼šå½“å‰ç³»ç»Ÿå·²æœ‰åšå®åŸºç¡€ï¼ˆHDBSCAN+LLM+Web UIï¼‰ï¼Œé€šè¿‡å¸æ”¶å›è¨€æ–¹æ³•è®ºçš„æ ¸å¿ƒåˆ›æ–°ï¼ˆç‰¹å¾ç‰‡æ®µæ˜ å°„+æ¨¡æ¿å˜é‡è¿­ä»£ï¼‰ï¼Œå¯ä»¥å®ç°è´¨çš„é£è·ƒï¼Œä»"èƒ½ç”¨"æå‡åˆ°"å¼ºå¤§"ã€‚
