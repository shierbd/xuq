"""
Phase 1: æ•°æ®æ•´åˆä¸æ¸…æ´—æ¨¡å—
åŠŸèƒ½ï¼šä»åŸå§‹CSV/Excelæ–‡ä»¶è¯»å–æ•°æ®ï¼Œæ¸…æ´—åå‡†å¤‡å¯¼å…¥æ•°æ®åº“
"""
import pandas as pd
import re
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
import chardet


def detect_file_encoding(file_path: Path) -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç 

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        æ£€æµ‹åˆ°çš„ç¼–ç åç§°
    """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # è¯»å–å‰10000å­—èŠ‚
        result = chardet.detect(raw_data)
        encoding = result['encoding']

        # å¤„ç†å¸¸è§çš„ç¼–ç æ˜ å°„
        if encoding and encoding.lower() in ['gb2312', 'gbk', 'gb18030']:
            return 'gbk'
        elif encoding and 'utf' in encoding.lower():
            return 'utf-8-sig'
        else:
            return encoding or 'utf-8-sig'


class DataIntegration:
    """æ•°æ®æ•´åˆä¸æ¸…æ´—ç±»"""

    def __init__(self, raw_data_dir: Path):
        """
        åˆå§‹åŒ–æ•°æ®æ•´åˆå™¨

        Args:
            raw_data_dir: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„
        """
        self.raw_data_dir = raw_data_dir

        # è‡ªåŠ¨å‘ç°æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒçµæ´»çš„æ–‡ä»¶åï¼‰
        self.source_files = self._discover_data_files()

    def _discover_data_files(self) -> Dict[str, any]:
        """
        è‡ªåŠ¨å‘ç°åŸå§‹æ•°æ®æ–‡ä»¶

        Returns:
            æ•°æ®æºæ–‡ä»¶è·¯å¾„å­—å…¸ï¼Œå€¼å¯ä»¥æ˜¯å•ä¸ªPathæˆ–Pathåˆ—è¡¨
        """
        discovered = {}

        # SEMRUSHæ•°æ®ï¼šåœ¨ raw/semrush/ ç›®å½•ä¸‹æŸ¥æ‰¾CSVæ–‡ä»¶
        # ä¼˜å…ˆä½¿ç”¨ *_converted.csvï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ *_processed.csv
        semrush_dir = self.raw_data_dir / 'semrush'
        if semrush_dir.exists():
            # ä¼˜å…ˆæŸ¥æ‰¾convertedæ–‡ä»¶
            converted_files = list(semrush_dir.glob('*_converted.csv'))
            if converted_files:
                discovered['semrush'] = converted_files[0]  # ä½¿ç”¨è½¬æ¢åçš„æ–‡ä»¶
            else:
                # å¦åˆ™æŸ¥æ‰¾processedæ–‡ä»¶
                processed_files = list(semrush_dir.glob('*_processed.csv'))
                if processed_files:
                    discovered['semrush'] = processed_files[0]

        # ä¸‹æ‹‰è¯æ•°æ®ï¼šåœ¨ raw/dropdown/ ç›®å½•ä¸‹æŸ¥æ‰¾CSVæ–‡ä»¶
        # ä¼˜å…ˆä½¿ç”¨ *_converted.csv
        dropdown_dir = self.raw_data_dir / 'dropdown'
        if dropdown_dir.exists():
            converted_files = list(dropdown_dir.glob('*_converted.csv'))
            if converted_files:
                discovered['dropdown'] = converted_files[0]
            else:
                processed_files = list(dropdown_dir.glob('*_processed.csv'))
                if processed_files:
                    discovered['dropdown'] = processed_files[0]

        # ç›¸å…³æœç´¢æ•°æ®ï¼šåœ¨ raw/related_search/ ç›®å½•ä¸‹æŸ¥æ‰¾CSV/Excelæ–‡ä»¶
        # ä¼˜å…ˆä½¿ç”¨ *_converted.csv
        related_dir = self.raw_data_dir / 'related_search'
        if related_dir.exists():
            converted_files = list(related_dir.glob('*_converted.csv'))
            if converted_files:
                discovered['related_search'] = converted_files[0]
            else:
                processed_files = list(related_dir.glob('*_processed.csv'))
                if processed_files:
                    discovered['related_search'] = processed_files[0]

        # å¦‚æœæ²¡æœ‰å­ç›®å½•ï¼Œå°è¯•åœ¨rawç›®å½•ç›´æ¥æŸ¥æ‰¾
        if not discovered:
            for pattern in ['*semrush*.csv', '*dropdown*.csv', '*related*.csv']:
                files = list(self.raw_data_dir.glob(pattern))
                if files:
                    source_type = pattern.replace('*', '').replace('.csv', '')
                    discovered[source_type] = files if len(files) > 1 else files[0]

        return discovered

    def load_semrush_data(self) -> pd.DataFrame:
        """
        åŠ è½½SEMRUSHæ•°æ®ï¼ˆæ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶åˆå¹¶ï¼‰

        Returns:
            DataFrame with columns: phrase, seed_word, source_type, frequency, volume
        """
        if 'semrush' not in self.source_files:
            logger.warning("SEMRUSHæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return pd.DataFrame()

        file_paths = self.source_files['semrush']

        # æ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        all_dfs = []

        for file_path in file_paths:
            if not file_path.exists():
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path.name}")
                continue

            logger.info(f"åŠ è½½SEMRUSHæ•°æ®: {file_path.name}")

            # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            encoding = detect_file_encoding(file_path)
            logger.info(f"   æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")

            try:
                df = pd.read_csv(file_path, encoding=encoding)
            except Exception as e:
                logger.warning(f"   ä½¿ç”¨ {encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯• utf-8-sig...")
                try:
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                except:
                    logger.warning(f"   ä½¿ç”¨ utf-8-sig å¤±è´¥ï¼Œå°è¯• gbk...")
                    try:
                        df = pd.read_csv(file_path, encoding='gbk')
                    except Exception as e2:
                        logger.error(f"   åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ–‡ä»¶: {file_path.name}, é”™è¯¯: {e2}")
                        continue

            # é€‰æ‹©éœ€è¦çš„åˆ—
            required_cols = ['phrase', 'seed_word', 'source_type', 'frequency', 'volume']
            available_cols = [col for col in required_cols if col in df.columns]
            df = df[available_cols].copy()

            # å¡«å……ç¼ºå¤±çš„volumeï¼ˆå¦‚æœæ²¡æœ‰volumeåˆ—ï¼Œä½¿ç”¨frequencyï¼‰
            if 'volume' not in df.columns:
                df['volume'] = df.get('frequency', 0)

            # ç¡®ä¿source_typeæ­£ç¡®
            df['source_type'] = 'semrush'

            logger.info(f"   æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
            all_dfs.append(df)

        if not all_dfs:
            logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•SEMRUSHæ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_dfs, ignore_index=True)
        logger.info(f"SEMRUSHæ•°æ®åˆå¹¶å®Œæˆï¼Œæ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆæ¥è‡ª {len(all_dfs)} ä¸ªæ–‡ä»¶ï¼‰")

        return combined_df

    def load_dropdown_data(self) -> pd.DataFrame:
        """
        åŠ è½½ä¸‹æ‹‰è¯æ•°æ®ï¼ˆæ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶åˆå¹¶ï¼‰

        Returns:
            DataFrame with columns: phrase, seed_word, source_type, frequency
        """
        if 'dropdown' not in self.source_files:
            logger.warning("ä¸‹æ‹‰è¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return pd.DataFrame()

        file_paths = self.source_files['dropdown']

        # æ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        all_dfs = []

        for file_path in file_paths:
            if not file_path.exists():
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path.name}")
                continue

            logger.info(f"åŠ è½½ä¸‹æ‹‰è¯æ•°æ®: {file_path.name}")

            # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            encoding = detect_file_encoding(file_path)
            logger.info(f"   æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")

            try:
                df = pd.read_csv(file_path, encoding=encoding)
            except Exception as e:
                logger.warning(f"   ä½¿ç”¨ {encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
                try:
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                except:
                    try:
                        df = pd.read_csv(file_path, encoding='gbk')
                    except Exception as e2:
                        logger.error(f"   åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ–‡ä»¶: {file_path.name}, é”™è¯¯: {e2}")
                        continue

            # é€‰æ‹©éœ€è¦çš„åˆ—
            required_cols = ['phrase', 'seed_word', 'source_type', 'frequency']
            available_cols = [col for col in required_cols if col in df.columns]
            df = df[available_cols].copy()

            # æ·»åŠ volumeåˆ—ï¼ˆä¸‹æ‹‰è¯æ²¡æœ‰volumeï¼Œè®¾ä¸º0ï¼‰
            df['volume'] = 0

            # ç¡®ä¿source_typeæ­£ç¡®
            df['source_type'] = 'dropdown'

            logger.info(f"   æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
            all_dfs.append(df)

        if not all_dfs:
            logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ä¸‹æ‹‰è¯æ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼‰
        if len(all_dfs) > 1:
            combined_df = pd.concat(all_dfs, ignore_index=True)
            logger.info(f"ä¸‹æ‹‰è¯æ•°æ®åˆå¹¶å®Œæˆï¼Œæ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆæ¥è‡ª {len(all_dfs)} ä¸ªæ–‡ä»¶ï¼‰")
            return combined_df
        else:
            logger.info(f"æˆåŠŸåŠ è½½ {len(all_dfs[0])} æ¡ä¸‹æ‹‰è¯è®°å½•")
            return all_dfs[0]

    def load_related_search_data(self) -> pd.DataFrame:
        """
        åŠ è½½ç›¸å…³æœç´¢æ•°æ®ï¼ˆæ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶åˆå¹¶ï¼‰

        Returns:
            DataFrame with columns: phrase, seed_word, source_type, frequency
        """
        if 'related_search' not in self.source_files:
            logger.warning("ç›¸å…³æœç´¢æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return pd.DataFrame()

        file_paths = self.source_files['related_search']

        # æ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶
        if not isinstance(file_paths, list):
            file_paths = [file_paths]

        all_dfs = []

        for file_path in file_paths:
            if not file_path.exists():
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path.name}")
                continue

            logger.info(f"åŠ è½½ç›¸å…³æœç´¢æ•°æ®: {file_path.name}")

            # æ”¯æŒExcelå’ŒCSVæ ¼å¼
            if file_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            else:
                # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                encoding = detect_file_encoding(file_path)
                logger.info(f"   æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")

                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                except Exception as e:
                    logger.warning(f"   ä½¿ç”¨ {encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
                    try:
                        df = pd.read_csv(file_path, encoding='utf-8-sig')
                    except:
                        try:
                            df = pd.read_csv(file_path, encoding='gbk')
                        except Exception as e2:
                            logger.error(f"   åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ–‡ä»¶: {file_path.name}, é”™è¯¯: {e2}")
                            continue

            # é€‰æ‹©éœ€è¦çš„åˆ—
            required_cols = ['phrase', 'seed_word', 'source_type', 'frequency']
            available_cols = [col for col in required_cols if col in df.columns]
            df = df[available_cols].copy()

            # æ·»åŠ volumeåˆ—ï¼ˆç›¸å…³æœç´¢æ²¡æœ‰volumeï¼Œè®¾ä¸º0ï¼‰
            df['volume'] = 0

            # ç¡®ä¿source_typeæ­£ç¡®
            df['source_type'] = 'related_search'

            logger.info(f"   æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
            all_dfs.append(df)

        if not all_dfs:
            logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ç›¸å…³æœç´¢æ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼‰
        if len(all_dfs) > 1:
            combined_df = pd.concat(all_dfs, ignore_index=True)
            logger.info(f"ç›¸å…³æœç´¢æ•°æ®åˆå¹¶å®Œæˆï¼Œæ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆæ¥è‡ª {len(all_dfs)} ä¸ªæ–‡ä»¶ï¼‰")
            return combined_df
        else:
            logger.info(f"æˆåŠŸåŠ è½½ {len(all_dfs[0])} æ¡ç›¸å…³æœç´¢è®°å½•")
            return all_dfs[0]

    def clean_phrase(self, phrase: str) -> Optional[str]:
        """
        æ¸…æ´—å•ä¸ªçŸ­è¯­

        Args:
            phrase: åŸå§‹çŸ­è¯­

        Returns:
            æ¸…æ´—åçš„çŸ­è¯­ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
        """
        if pd.isna(phrase) or not isinstance(phrase, str):
            return None

        # è½¬å°å†™
        phrase = phrase.lower().strip()

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        phrase = re.sub(r'\s+', ' ', phrase)

        # é•¿åº¦æ£€æŸ¥ï¼ˆ1-255å­—ç¬¦ï¼‰
        if len(phrase) < 1 or len(phrase) > 255:
            return None

        # ç§»é™¤çº¯æ•°å­—çŸ­è¯­
        if phrase.isdigit():
            return None

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦è¿‡å¤šçš„çŸ­è¯­ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ï¼‰
        valid_chars = re.findall(r'[a-z0-9\s\-]', phrase)
        if len(valid_chars) / len(phrase) < 0.5:  # æœ‰æ•ˆå­—ç¬¦å°‘äº50%
            return None

        return phrase

    def clean_seed_word(self, seed_word: str) -> str:
        """
        æ¸…æ´—ç§å­è¯

        Args:
            seed_word: åŸå§‹ç§å­è¯

        Returns:
            æ¸…æ´—åçš„ç§å­è¯
        """
        if pd.isna(seed_word) or not isinstance(seed_word, str):
            return 'unknown'

        seed_word = seed_word.lower().strip()
        if len(seed_word) == 0 or len(seed_word) > 100:
            return 'unknown'

        return seed_word

    def merge_and_clean(self, round_id: int = 1, sources: list = None) -> pd.DataFrame:
        """
        åˆå¹¶æ‰€æœ‰æ•°æ®æºå¹¶æ¸…æ´—

        Args:
            round_id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
            sources: è¦å¯¼å…¥çš„æ•°æ®æºåˆ—è¡¨ï¼Œå¦‚ ['semrush', 'dropdown']ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨å¯¼å…¥

        Returns:
            æ¸…æ´—åçš„DataFrameï¼ŒåŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - phrase: çŸ­è¯­ï¼ˆå”¯ä¸€ï¼‰
            - seed_word: ç§å­è¯
            - source_type: æ•°æ®æºç±»å‹
            - frequency: é¢‘æ¬¡
            - volume: æœç´¢é‡
            - first_seen_round: é¦–æ¬¡å‡ºç°è½®æ¬¡
        """
        logger.info("\n" + "="*60)
        logger.info("å¼€å§‹æ•°æ®æ•´åˆä¸æ¸…æ´—")
        logger.info("="*60)

        # 1. æ ¹æ®sourceså‚æ•°å†³å®šåŠ è½½å“ªäº›æ•°æ®æº
        all_data = []

        # å¦‚æœæœªæŒ‡å®šsourcesï¼ŒåŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®æº
        if sources is None:
            sources = ['semrush', 'dropdown', 'related_search']

        # åŠ è½½æŒ‡å®šçš„æ•°æ®æº
        if 'semrush' in sources:
            semrush_df = self.load_semrush_data()
            if not semrush_df.empty:
                all_data.append(semrush_df)

        if 'dropdown' in sources:
            dropdown_df = self.load_dropdown_data()
            if not dropdown_df.empty:
                all_data.append(dropdown_df)

        if 'related_search' in sources:
            related_df = self.load_related_search_data()
            if not related_df.empty:
                all_data.append(related_df)

        # 2. åˆå¹¶æ•°æ®
        logger.info("\nåˆå¹¶æ•°æ®æº...")
        logger.info(f"   å‡†å¤‡åˆå¹¶çš„æ•°æ®æº: {', '.join(sources)}")

        if not all_data:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼")
            return pd.DataFrame()

        df = pd.concat(all_data, ignore_index=True)
        logger.info(f"åˆå¹¶åæ€»è®°å½•æ•°: {len(df)}")
        logger.info("\næ¸…æ´—æ•°æ®...")
        logger.info("  - æ¸…æ´—çŸ­è¯­...")
        df['phrase_cleaned'] = df['phrase'].apply(self.clean_phrase)
        df = df[df['phrase_cleaned'].notna()].copy()
        df['phrase'] = df['phrase_cleaned']
        df.drop('phrase_cleaned', axis=1, inplace=True)
        logger.info(f"    å‰©ä½™ {len(df)} æ¡æœ‰æ•ˆè®°å½•")
        logger.info("  - æ¸…æ´—ç§å­è¯...")
        df['seed_word'] = df['seed_word'].apply(self.clean_seed_word)

        # ç¡®ä¿æ•°å€¼åˆ—æ­£ç¡®
        df['frequency'] = pd.to_numeric(df['frequency'], errors='coerce').fillna(1).astype(int)
        df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0).astype(int)

        # 4. å»é‡å¹¶èšåˆ
        logger.info("\nå»é‡å¹¶èšåˆ...")
        df_grouped = df.groupby('phrase').agg({
            'seed_word': 'first',  # å–ç¬¬ä¸€ä¸ªç§å­è¯
            'source_type': 'first',  # å–ç¬¬ä¸€ä¸ªæ•°æ®æº
            'frequency': 'sum',  # é¢‘æ¬¡æ±‚å’Œ
            'volume': 'max',  # æœç´¢é‡å–æœ€å¤§å€¼
        }).reset_index()

        logger.info(f"å»é‡åè®°å½•æ•°: {len(df_grouped)}")
        df_grouped['first_seen_round'] = round_id

        # 6. æœ€ç»ˆæ’åº
        df_grouped = df_grouped.sort_values('frequency', ascending=False).reset_index(drop=True)

        # 7. ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n" + "="*60)
        logger.info("æ•°æ®ç»Ÿè®¡")
        logger.info("="*60)
        logger.info(f"æ€»è®°å½•æ•°: {len(df_grouped)}")
        logger.info(f"\næŒ‰æ•°æ®æºåˆ†å¸ƒ:")
        logger.info(df_grouped['source_type'].value_counts())
        logger.info(f"\né¢‘æ¬¡ç»Ÿè®¡:")
        logger.info(f"  æœ€å¤§å€¼: {df_grouped['frequency'].max()}")
        logger.info(f"  æœ€å°å€¼: {df_grouped['frequency'].min()}")
        logger.info(f"  å¹³å‡å€¼: {df_grouped['frequency'].mean():.2f}")
        logger.info(f"  ä¸­ä½æ•°: {df_grouped['frequency'].median():.2f}")
        logger.info(f"\næœç´¢é‡ç»Ÿè®¡:")
        logger.info(f"  æœ‰æœç´¢é‡çš„è®°å½•: {(df_grouped['volume'] > 0).sum()}")
        logger.info(f"  æœ€å¤§æœç´¢é‡: {df_grouped['volume'].max()}")
        logger.info("="*60)

        return df_grouped

    def prepare_for_database(self, df: pd.DataFrame) -> List[Dict]:
        """
        å°†DataFrameè½¬æ¢ä¸ºé€‚åˆæ•°æ®åº“æ’å…¥çš„å­—å…¸åˆ—è¡¨

        Args:
            df: æ¸…æ´—åçš„DataFrame

        Returns:
            å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸å¯¹åº”ä¸€æ¡æ•°æ®åº“è®°å½•
        """
        logger.info("\nå‡†å¤‡æ•°æ®åº“æ’å…¥æ ¼å¼...")

        records = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="è½¬æ¢è®°å½•"):
            record = {
                'phrase': row['phrase'],
                'seed_word': row['seed_word'],
                'source_type': row['source_type'],
                'frequency': int(row['frequency']),
                'volume': int(row['volume']),
                'first_seen_round': int(row['first_seen_round']),
                # ä»¥ä¸‹å­—æ®µä½¿ç”¨é»˜è®¤å€¼
                'cluster_id_A': None,
                'cluster_id_B': None,
                'mapped_demand_id': None,
                'processed_status': 'unseen',
            }
            records.append(record)

        logger.info(f"å‡†å¤‡å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records


from utils.logger import get_logger

logger = get_logger(__name__)


def test_data_integration():
    """æµ‹è¯•æ•°æ®æ•´åˆåŠŸèƒ½"""
    from config.settings import RAW_DATA_DIR

    integrator = DataIntegration(RAW_DATA_DIR)
    df = integrator.merge_and_clean(round_id=1)

    if not df.empty:
        logger.info("\nâœ… æ•°æ®æ•´åˆæµ‹è¯•æˆåŠŸï¼")
        logger.info(f"\nå‰5æ¡è®°å½•:")
        logger.info(df.head())
        output_path = RAW_DATA_DIR.parent / 'processed' / 'integrated_data.csv'
        output_path.parent.mkdir(exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"\nğŸ’¾ æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    else:
        logger.error("\nâŒ æ•°æ®æ•´åˆå¤±è´¥ï¼")


if __name__ == "__main__":
    test_data_integration()
