"""
Phase 1: æ•°æ®æ•´åˆä¸æ¸…æ´—æ¨¡å—
åŠŸèƒ½ï¼šä»åŸå§‹CSV/Excelæ–‡ä»¶è¯»å–æ•°æ®ï¼Œæ¸…æ´—åå‡†å¤‡å¯¼å…¥æ•°æ®åº“
"""
import pandas as pd
import re
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm


class DataIntegration:
    """æ•°æ®æ•´åˆä¸æ¸…æ´—ç±»"""

    def __init__(self, raw_data_dir: Path):
        """
        åˆå§‹åŒ–æ•°æ®æ•´åˆå™¨

        Args:
            raw_data_dir: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„
        """
        self.raw_data_dir = raw_data_dir
        self.source_files = {
            'semrush': raw_data_dir / 'semrush_processed.csv',
            'dropdown': raw_data_dir / 'dropdown_processed.csv',
            'related_search': raw_data_dir / 'related_search_processed.csv',
        }

    def load_semrush_data(self) -> pd.DataFrame:
        """
        åŠ è½½SEMRUSHæ•°æ®

        Returns:
            DataFrame with columns: phrase, seed_word, source_type, frequency, volume
        """
        file_path = self.source_files['semrush']
        if not file_path.exists():
            logger.warning(f"âš ï¸  SEMRUSHæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()

        logger.info(f"ğŸ“‚ åŠ è½½SEMRUSHæ•°æ®: {file_path.name}")
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_cols = ['phrase', 'seed_word', 'source_type', 'frequency', 'volume']
        available_cols = [col for col in required_cols if col in df.columns]
        df = df[available_cols].copy()

        # å¡«å……ç¼ºå¤±çš„volumeï¼ˆå¦‚æœæ²¡æœ‰volumeåˆ—ï¼Œä½¿ç”¨frequencyï¼‰
        if 'volume' not in df.columns:
            df['volume'] = df.get('frequency', 0)

        # ç¡®ä¿source_typeæ­£ç¡®
        df['source_type'] = 'semrush'

        logger.info(f"âœ“ åŠ è½½ {len(df)} æ¡SEMRUSHè®°å½•")
        return df

    def load_dropdown_data(self) -> pd.DataFrame:
        """
        åŠ è½½ä¸‹æ‹‰è¯æ•°æ®

        Returns:
            DataFrame with columns: phrase, seed_word, source_type, frequency
        """
        file_path = self.source_files['dropdown']
        if not file_path.exists():
            logger.warning(f"âš ï¸  ä¸‹æ‹‰è¯æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()

        logger.info(f"ğŸ“‚ åŠ è½½ä¸‹æ‹‰è¯æ•°æ®: {file_path.name}")
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_cols = ['phrase', 'seed_word', 'source_type', 'frequency']
        available_cols = [col for col in required_cols if col in df.columns]
        df = df[available_cols].copy()

        # æ·»åŠ volumeåˆ—ï¼ˆä¸‹æ‹‰è¯æ²¡æœ‰volumeï¼Œè®¾ä¸º0ï¼‰
        df['volume'] = 0

        # ç¡®ä¿source_typeæ­£ç¡®
        df['source_type'] = 'dropdown'

        logger.info(f"âœ“ åŠ è½½ {len(df)} æ¡ä¸‹æ‹‰è¯è®°å½•")
        return df

    def load_related_search_data(self) -> pd.DataFrame:
        """
        åŠ è½½ç›¸å…³æœç´¢æ•°æ®

        Returns:
            DataFrame with columns: phrase, seed_word, source_type, frequency
        """
        file_path = self.source_files['related_search']
        if not file_path.exists():
            logger.warning(f"âš ï¸  ç›¸å…³æœç´¢æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()

        logger.info(f"ğŸ“‚ åŠ è½½ç›¸å…³æœç´¢æ•°æ®: {file_path.name}")
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_cols = ['phrase', 'seed_word', 'source_type', 'frequency']
        available_cols = [col for col in required_cols if col in df.columns]
        df = df[available_cols].copy()

        # æ·»åŠ volumeåˆ—ï¼ˆç›¸å…³æœç´¢æ²¡æœ‰volumeï¼Œè®¾ä¸º0ï¼‰
        df['volume'] = 0

        # ç¡®ä¿source_typeæ­£ç¡®
        df['source_type'] = 'related_search'

        logger.info(f"âœ“ åŠ è½½ {len(df)} æ¡ç›¸å…³æœç´¢è®°å½•")
        return df

    def clean_phrase(self, phrase: str) -> Optional[str]:
        """
        æ¸…æ´—å•ä¸ªçŸ­è¯­

        Args:
            phrase: åŸå§‹çŸ­è¯­

        Returns:
            æ¸…æ´—åçš„çŸ­è¯­ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
        """
        if pd.isna(phrase) or not isinstance(phrase, str):
            return None

        # è½¬å°å†™
        phrase = phrase.lower().strip()

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        phrase = re.sub(r'\s+', ' ', phrase)

        # é•¿åº¦æ£€æŸ¥ï¼ˆ1-255å­—ç¬¦ï¼‰
        if len(phrase) < 1 or len(phrase) > 255:
            return None

        # ç§»é™¤çº¯æ•°å­—çŸ­è¯­
        if phrase.isdigit():
            return None

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦è¿‡å¤šçš„çŸ­è¯­ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ï¼‰
        valid_chars = re.findall(r'[a-z0-9\s\-]', phrase)
        if len(valid_chars) / len(phrase) < 0.5:  # æœ‰æ•ˆå­—ç¬¦å°‘äº50%
            return None

        return phrase

    def clean_seed_word(self, seed_word: str) -> str:
        """
        æ¸…æ´—ç§å­è¯

        Args:
            seed_word: åŸå§‹ç§å­è¯

        Returns:
            æ¸…æ´—åçš„ç§å­è¯
        """
        if pd.isna(seed_word) or not isinstance(seed_word, str):
            return 'unknown'

        seed_word = seed_word.lower().strip()
        if len(seed_word) == 0 or len(seed_word) > 100:
            return 'unknown'

        return seed_word

    def merge_and_clean(self, round_id: int = 1) -> pd.DataFrame:
        """
        åˆå¹¶æ‰€æœ‰æ•°æ®æºå¹¶æ¸…æ´—

        Args:
            round_id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰

        Returns:
            æ¸…æ´—åçš„DataFrameï¼ŒåŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - phrase: çŸ­è¯­ï¼ˆå”¯ä¸€ï¼‰
            - seed_word: ç§å­è¯
            - source_type: æ•°æ®æºç±»å‹
            - frequency: é¢‘æ¬¡
            - volume: æœç´¢é‡
            - first_seen_round: é¦–æ¬¡å‡ºç°è½®æ¬¡
        """
        logger.info("\n" + "="*60)
        logger.info("å¼€å§‹æ•°æ®æ•´åˆä¸æ¸…æ´—")
        logger.info("="*60)
        semrush_df = self.load_semrush_data()
        dropdown_df = self.load_dropdown_data()
        related_df = self.load_related_search_data()

        # 2. åˆå¹¶æ•°æ®
        logger.info("\nğŸ”— åˆå¹¶æ•°æ®æº...")
        all_data = []
        if not semrush_df.empty:
            all_data.append(semrush_df)
        if not dropdown_df.empty:
            all_data.append(dropdown_df)
        if not related_df.empty:
            all_data.append(related_df)

        if not all_data:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼")
            return pd.DataFrame()

        df = pd.concat(all_data, ignore_index=True)
        logger.info(f"âœ“ åˆå¹¶åæ€»è®°å½•æ•°: {len(df)}")
        logger.info("\nğŸ§¹ æ¸…æ´—æ•°æ®...")
        logger.info("  - æ¸…æ´—çŸ­è¯­...")
        df['phrase_cleaned'] = df['phrase'].apply(self.clean_phrase)
        df = df[df['phrase_cleaned'].notna()].copy()
        df['phrase'] = df['phrase_cleaned']
        df.drop('phrase_cleaned', axis=1, inplace=True)
        logger.info(f"    âœ“ å‰©ä½™ {len(df)} æ¡æœ‰æ•ˆè®°å½•")
        logger.info("  - æ¸…æ´—ç§å­è¯...")
        df['seed_word'] = df['seed_word'].apply(self.clean_seed_word)

        # ç¡®ä¿æ•°å€¼åˆ—æ­£ç¡®
        df['frequency'] = pd.to_numeric(df['frequency'], errors='coerce').fillna(1).astype(int)
        df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0).astype(int)

        # 4. å»é‡å¹¶èšåˆ
        logger.info("\nğŸ”„ å»é‡å¹¶èšåˆ...")
        df_grouped = df.groupby('phrase').agg({
            'seed_word': 'first',  # å–ç¬¬ä¸€ä¸ªç§å­è¯
            'source_type': 'first',  # å–ç¬¬ä¸€ä¸ªæ•°æ®æº
            'frequency': 'sum',  # é¢‘æ¬¡æ±‚å’Œ
            'volume': 'max',  # æœç´¢é‡å–æœ€å¤§å€¼
        }).reset_index()

        logger.info(f"âœ“ å»é‡åè®°å½•æ•°: {len(df_grouped)}")
        df_grouped['first_seen_round'] = round_id

        # 6. æœ€ç»ˆæ’åº
        df_grouped = df_grouped.sort_values('frequency', ascending=False).reset_index(drop=True)

        # 7. ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æ•°æ®ç»Ÿè®¡")
        logger.info("="*60)
        logger.info(f"æ€»è®°å½•æ•°: {len(df_grouped)}")
        logger.info(f"\næŒ‰æ•°æ®æºåˆ†å¸ƒ:")
        logger.info(df_grouped['source_type'].value_counts())
        logger.info(f"\né¢‘æ¬¡ç»Ÿè®¡:")
        logger.info(f"  æœ€å¤§å€¼: {df_grouped['frequency'].max()}")
        logger.info(f"  æœ€å°å€¼: {df_grouped['frequency'].min()}")
        logger.info(f"  å¹³å‡å€¼: {df_grouped['frequency'].mean():.2f}")
        logger.info(f"  ä¸­ä½æ•°: {df_grouped['frequency'].median():.2f}")
        logger.info(f"\næœç´¢é‡ç»Ÿè®¡:")
        logger.info(f"  æœ‰æœç´¢é‡çš„è®°å½•: {(df_grouped['volume'] > 0).sum()}")
        logger.info(f"  æœ€å¤§æœç´¢é‡: {df_grouped['volume'].max()}")
        logger.info("="*60)

        return df_grouped

    def prepare_for_database(self, df: pd.DataFrame) -> List[Dict]:
        """
        å°†DataFrameè½¬æ¢ä¸ºé€‚åˆæ•°æ®åº“æ’å…¥çš„å­—å…¸åˆ—è¡¨

        Args:
            df: æ¸…æ´—åçš„DataFrame

        Returns:
            å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸å¯¹åº”ä¸€æ¡æ•°æ®åº“è®°å½•
        """
        logger.info("\nğŸ“‹ å‡†å¤‡æ•°æ®åº“æ’å…¥æ ¼å¼...")

        records = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="è½¬æ¢è®°å½•"):
            record = {
                'phrase': row['phrase'],
                'seed_word': row['seed_word'],
                'source_type': row['source_type'],
                'frequency': int(row['frequency']),
                'volume': int(row['volume']),
                'first_seen_round': int(row['first_seen_round']),
                # ä»¥ä¸‹å­—æ®µä½¿ç”¨é»˜è®¤å€¼
                'cluster_id_A': None,
                'cluster_id_B': None,
                'mapped_demand_id': None,
                'processed_status': 'unseen',
            }
            records.append(record)

        logger.info(f"âœ“ å‡†å¤‡å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records


def test_data_integration():
    """æµ‹è¯•æ•°æ®æ•´åˆåŠŸèƒ½"""
    from config.settings import RAW_DATA_DIR
from utils.logger import get_logger

logger = get_logger(__name__)


    integrator = DataIntegration(RAW_DATA_DIR)
    df = integrator.merge_and_clean(round_id=1)

    if not df.empty:
        logger.info("\nâœ… æ•°æ®æ•´åˆæµ‹è¯•æˆåŠŸï¼")
        logger.info(f"\nå‰5æ¡è®°å½•:")
        logger.info(df.head())
        output_path = RAW_DATA_DIR.parent / 'processed' / 'integrated_data.csv'
        output_path.parent.mkdir(exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"\nğŸ’¾ æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    else:
        logger.error("\nâŒ æ•°æ®æ•´åˆå¤±è´¥ï¼")


if __name__ == "__main__":
    test_data_integration()
