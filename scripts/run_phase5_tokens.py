"""
Phase 5: Tokenæå–ä¸åˆ†ç±»
ä»çŸ­è¯­ä¸­æå–å€™é€‰tokenså¹¶ä½¿ç”¨LLMè¿›è¡Œåˆ†ç±»ï¼Œå»ºç«‹éœ€æ±‚æ¡†æ¶è¯åº“

è¿è¡Œæ–¹å¼:
    python scripts/run_phase5_tokens.py [--skip-llm] [--min-frequency N] [--sample-size N]

å‚æ•°:
    --skip-llm: è·³è¿‡LLMåˆ†ç±»ï¼ˆä»…æå–tokensï¼‰
    --min-frequency: æœ€å°é¢‘æ¬¡é˜ˆå€¼ï¼ˆé»˜è®¤3ï¼‰
    --sample-size: é‡‡æ ·çŸ­è¯­æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼Œé»˜è®¤10000ï¼‰
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤1ï¼‰
"""
import sys
import argparse
from pathlib import Path
from typing import List, Dict
import pandas as pd

# è®¾ç½®UTF-8ç¼–ç è¾“å‡ºï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import OUTPUT_DIR
from utils.token_extractor import (
    extract_tokens, extract_bigrams, extract_ngrams,
    extract_demand_patterns, analyze_token_framework
)
from ai.client import LLMClient
from storage.repository import PhraseRepository, TokenRepository


def load_phrases_for_token_extraction(sample_size: int = 0, round_id: int = 1) -> List[str]:
    """
    åŠ è½½çŸ­è¯­ç”¨äºtokenæå–

    Args:
        sample_size: é‡‡æ ·å¤§å°ï¼ˆ0=å…¨éƒ¨ï¼‰
        round_id: æ•°æ®è½®æ¬¡

    Returns:
        çŸ­è¯­æ–‡æœ¬åˆ—è¡¨
    """
    print(f"\nğŸ“¥ åŠ è½½çŸ­è¯­æ•°æ®...")

    with PhraseRepository() as repo:
        # MVPç‰ˆæœ¬ï¼šä»æ‰€æœ‰çŸ­è¯­ä¸­æå–
        # ç”Ÿäº§ç‰ˆæœ¬ï¼šä»…ä»å·²éªŒè¯éœ€æ±‚çš„çŸ­è¯­ä¸­æå–
        # phrases_db = repo.session.query(Phrase).filter(
        #     Phrase.mapped_demand_id.isnot(None)
        # ).all()

        # è·å–æŒ‡å®šè½®æ¬¡çš„çŸ­è¯­
        from storage.models import Phrase

        if sample_size > 0:
            phrases_db = repo.session.query(Phrase).limit(sample_size).all()
            print(f"  âœ“ åŠ è½½äº† {len(phrases_db)} æ¡çŸ­è¯­ï¼ˆé‡‡æ ·æ¨¡å¼ï¼‰")
        else:
            count = repo.get_phrase_count()
            print(f"  âœ“ å‡†å¤‡åŠ è½½æ‰€æœ‰ {count} æ¡çŸ­è¯­...")
            phrases_db = repo.session.query(Phrase).all()
            print(f"  âœ“ åŠ è½½å®Œæˆ")

    # æå–çŸ­è¯­æ–‡æœ¬
    phrases = [p.phrase for p in phrases_db if p.phrase]

    print(f"  âœ“ æœ‰æ•ˆçŸ­è¯­: {len(phrases)} æ¡")

    return phrases


def save_tokens_to_csv(tokens_with_types: List[Dict], output_file: Path):
    """
    ä¿å­˜tokens(åŒ…å«n-grams)åˆ°CSVç”¨äºå®¡æ ¸

    Args:
        tokens_with_types: token/n-gramæ•°æ®åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    df = pd.DataFrame(tokens_with_types)

    # é‡æ–°æ’åºåˆ—ï¼ˆå¢åŠ gram_sizeåˆ—ï¼‰
    columns = ['token_text', 'gram_size', 'token_type', 'in_phrase_count', 'confidence', 'verified', 'notes']
    df = df[[col for col in columns if col in df.columns]]

    # æŒ‰gram_size(é™åº)å’Œé¢‘æ¬¡(é™åº)æ’åº
    df = df.sort_values(['gram_size', 'in_phrase_count'], ascending=[False, False])

    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"  âœ“ CSVå·²ä¿å­˜: {output_file}")


def run_phase5_tokens(skip_llm: bool = False,
                      min_frequency: int = 8,
                      sample_size: int = 10000,
                      round_id: int = 1):
    """
    æ‰§è¡ŒPhase 5: Tokenæå–ä¸åˆ†ç±»

    Args:
        skip_llm: æ˜¯å¦è·³è¿‡LLMåˆ†ç±»
        min_frequency: æœ€å°é¢‘æ¬¡é˜ˆå€¼ï¼ˆé»˜è®¤8ï¼‰
        sample_size: é‡‡æ ·çŸ­è¯­æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰
        round_id: æ•°æ®è½®æ¬¡
    """
    print("\n" + "="*70)
    print("Phase 5: Tokenæå–ä¸åˆ†ç±»".center(70))
    print("="*70)

    # ã€é˜¶æ®µ1ã€‘åŠ è½½çŸ­è¯­æ•°æ®
    print("\nã€é˜¶æ®µ1ã€‘åŠ è½½çŸ­è¯­æ•°æ®...")
    phrases = load_phrases_for_token_extraction(sample_size, round_id)

    if not phrases:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„çŸ­è¯­æ•°æ®ï¼")
        return False

    # ã€é˜¶æ®µ2ã€‘æå–N-gramè¯ç»„ï¼ˆä¼˜å…ˆçº§æ¨¡å¼ï¼‰
    print("\nã€é˜¶æ®µ2ã€‘æå–N-gramè¯ç»„...")
    print("  ç­–ç•¥: ä¼˜å…ˆçº§1=åŸç”Ÿn-gram(2-4è¯), ä¼˜å…ˆçº§2=å•token(è¡¥å……)")

    candidate_ngrams = extract_ngrams(
        phrases,
        max_gram_size=4,           # æå– 1-4è¯ç»„åˆ
        min_frequency=min_frequency,
        priority_mode=True          # å¯ç”¨ä¼˜å…ˆçº§æ¨¡å¼
    )

    if not candidate_ngrams:
        print("\nâŒ æ²¡æœ‰æå–åˆ°ä»»ä½•n-gramsï¼")
        return False

    print(f"\n  ğŸ“Š æå–ç»“æœç»Ÿè®¡:")

    # æŒ‰gram_sizeç»Ÿè®¡
    by_gram_size = {}
    for ng in candidate_ngrams:
        g = ng['gram_size']
        by_gram_size[g] = by_gram_size.get(g, 0) + 1

    for gram_size in sorted(by_gram_size.keys(), reverse=True):
        count = by_gram_size[gram_size]
        print(f"    - {gram_size}-gram: {count} ä¸ª")

    # Top 10ç¤ºä¾‹
    print(f"\n  ğŸ” Top 10 é«˜é¢‘è¯ç»„:")
    for i, ng in enumerate(candidate_ngrams[:10], 1):
        priority_mark = "â­" if ng['priority'] == 1 else "  "
        print(f"      {priority_mark}{i}. [{ng['gram_size']}-gram] '{ng['text']}' - å‡ºç° {ng['frequency']} æ¬¡")

    # ã€é˜¶æ®µ3ã€‘LLMæ‰¹é‡åˆ†ç±»
    tokens_with_types = []
    tokens_classified = {}  # {token_text: token_type}

    if skip_llm:
        print("\nã€é˜¶æ®µ3ã€‘âš ï¸  è·³è¿‡LLMåˆ†ç±»")
        # ä½¿ç”¨é»˜è®¤ç±»å‹
        for ng in candidate_ngrams:
            tokens_with_types.append({
                'token_text': ng['text'],
                'token_type': 'other',
                'gram_size': ng['gram_size'],
                'in_phrase_count': ng['frequency'],
                'confidence': 'low',
                'verified': False,
                'notes': 'æœªåˆ†ç±»'
            })
            tokens_classified[ng['text']] = 'other'
    else:
        print("\nã€é˜¶æ®µ3ã€‘LLMæ‰¹é‡åˆ†ç±»...")

        try:
            llm = LLMClient()

            # æå–n-gramæ–‡æœ¬åˆ—è¡¨
            ngram_texts = [ng['text'] for ng in candidate_ngrams]

            # æ‰¹é‡åˆ†ç±»ï¼ˆå¯¹2-4è¯ç»„åˆå’Œå•è¯éƒ½è¿›è¡Œåˆ†ç±»ï¼‰
            classifications = llm.batch_classify_tokens(
                tokens=ngram_texts,
                batch_size=50
            )

            # åˆå¹¶é¢‘æ¬¡å’Œåˆ†ç±»ç»“æœ
            ngram_data_map = {
                ng['text']: {'freq': ng['frequency'], 'gram_size': ng['gram_size']}
                for ng in candidate_ngrams
            }

            for classification in classifications:
                ngram_text = classification['token']
                token_type = classification.get('token_type', 'other')

                ng_data = ngram_data_map.get(ngram_text, {})

                tokens_with_types.append({
                    'token_text': ngram_text,
                    'token_type': token_type,
                    'gram_size': ng_data.get('gram_size', 1),
                    'in_phrase_count': ng_data.get('freq', 0),
                    'confidence': classification.get('confidence', 'medium'),
                    'verified': False,
                    'notes': None
                })

                tokens_classified[ngram_text] = token_type

            print(f"\n  âœ“ æˆåŠŸåˆ†ç±» {len(tokens_with_types)} ä¸ªn-grams")

            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = {}
            for token in tokens_with_types:
                token_type = token['token_type']
                type_counts[token_type] = type_counts.get(token_type, 0) + 1

            print(f"\n  ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
            for token_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"    - {token_type}: {count} ä¸ª")

            # æŒ‰gram_sizeåˆ†ç±»ç»Ÿè®¡
            print(f"\n  ğŸ“ æŒ‰è¯ç»„é•¿åº¦ç»Ÿè®¡:")
            by_gram = {}
            for token in tokens_with_types:
                g = token['gram_size']
                by_gram[g] = by_gram.get(g, 0) + 1

            for gram_size in sorted(by_gram.keys(), reverse=True):
                print(f"    - {gram_size}-gram: {by_gram[gram_size]} ä¸ª")

        except Exception as e:
            print(f"\n  âŒ LLMåˆ†ç±»å¤±è´¥: {str(e)}")
            print(f"  æç¤º: ä½¿ç”¨ --skip-llm å‚æ•°è·³è¿‡æ­¤æ­¥éª¤")
            return False

    # ã€é˜¶æ®µ4ã€‘æå–éœ€æ±‚æ¨¡å¼ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
    demand_patterns = []
    if tokens_classified:
        print("\nã€é˜¶æ®µ4ã€‘æå–éœ€æ±‚æ¨¡å¼...")
        try:
            demand_patterns = extract_demand_patterns(
                phrases=phrases,
                tokens_classified=tokens_classified,
                min_frequency=5
            )

            if demand_patterns:
                print(f"\n  é«˜é¢‘éœ€æ±‚æ¨¡å¼ Top 10:")
                for i, pattern in enumerate(demand_patterns[:10], 1):
                    print(f"    {i}. {pattern['pattern']:40s} - {pattern['frequency']:5d}æ¬¡")
                    if pattern['examples']:
                        print(f"       ç¤ºä¾‹: {pattern['examples'][0]}")
        except Exception as e:
            print(f"  âš ï¸  æ¨¡å¼æå–å¤±è´¥: {str(e)}")

    # ã€é˜¶æ®µ6ã€‘ä¿å­˜åˆ°æ•°æ®åº“
    print("\nã€é˜¶æ®µ6ã€‘ä¿å­˜åˆ°æ•°æ®åº“...")

    with TokenRepository() as repo:
        inserted_count = 0

        for token_data in tokens_with_types:
            try:
                token = repo.create_token(
                    token_text=token_data['token_text'],
                    token_type=token_data['token_type'],
                    in_phrase_count=token_data['in_phrase_count'],
                    first_seen_round=round_id,
                    verified=token_data['verified'],
                    notes=token_data.get('notes')
                )

                inserted_count += 1

            except Exception as e:
                print(f"    âš ï¸  ä¿å­˜å¤±è´¥: {token_data['token_text']} - {str(e)}")
                continue

        print(f"  âœ“ æˆåŠŸä¿å­˜ {inserted_count} ä¸ªtokensåˆ°æ•°æ®åº“")

    # ã€é˜¶æ®µ7ã€‘ç”ŸæˆCSVæŠ¥å‘Š
    print("\nã€é˜¶æ®µ7ã€‘ç”ŸæˆCSVæŠ¥å‘Š...")

    OUTPUT_DIR.mkdir(exist_ok=True)
    csv_file = OUTPUT_DIR / 'tokens_extracted.csv'

    save_tokens_to_csv(tokens_with_types, csv_file)

    # ã€é˜¶æ®µ8ã€‘ç”Ÿæˆæ¡†æ¶åˆ†ææŠ¥å‘Šï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
    print("\nã€é˜¶æ®µ8ã€‘ç”Ÿæˆæ¡†æ¶åˆ†ææŠ¥å‘Š...")

    framework_analysis = analyze_token_framework(tokens_with_types, demand_patterns)

    report_lines = []
    report_lines.append("="*70)
    report_lines.append("Phase 5 éœ€æ±‚æ¡†æ¶è¯åº“åˆ†ææŠ¥å‘Š")
    report_lines.append("="*70)
    report_lines.append("")

    report_lines.append("ã€æ•°æ®æ¦‚å†µã€‘")
    report_lines.append(f"  è¾“å…¥çŸ­è¯­æ•°: {len(phrases):,}")
    report_lines.append(f"  æœ€å°é¢‘æ¬¡é˜ˆå€¼: {min_frequency}")
    report_lines.append(f"  æå–n-gramæ•°: {len(candidate_ngrams)}")
    report_lines.append("")

    # Tokenåˆ†ç±»ç»Ÿè®¡
    if not skip_llm and framework_analysis['type_stats']:
        report_lines.append("ã€Tokenåˆ†ç±»ç»Ÿè®¡ã€‘")
        for token_type, stats in sorted(framework_analysis['type_stats'].items(),
                                       key=lambda x: x[1]['count'], reverse=True):
            count = stats['count']
            percentage = count / framework_analysis['total_tokens'] * 100
            report_lines.append(f"  {token_type}: {count} ä¸ª ({percentage:.1f}%)")
        report_lines.append("")

        # æ¯ä¸ªç±»å‹çš„Top 10
        report_lines.append("ã€å„ç±»å‹é«˜é¢‘Token Top 10ã€‘")
        for token_type in ['intent', 'action', 'object', 'other']:
            if token_type in framework_analysis['type_stats']:
                stats = framework_analysis['type_stats'][token_type]
                report_lines.append(f"\n  {token_type.upper()}:")
                for i, token in enumerate(stats['top_tokens'][:10], 1):
                    freq = token.get('in_phrase_count', 0)
                    report_lines.append(f"    {i:2d}. {token['token_text']:20s} - {freq:5d}æ¬¡")

        report_lines.append("")

    # é«˜é¢‘N-gram Top 20
    report_lines.append("ã€é«˜é¢‘N-gram Top 20ã€‘")
    for i, ng in enumerate(candidate_ngrams[:20], 1):
        token_info = next((t for t in tokens_with_types if t['token_text'] == ng['text']), None)
        token_type = token_info['token_type'] if token_info else 'unknown'
        gram_mark = f"[{ng['gram_size']}-gram]"
        report_lines.append(f"  {i:2d}. {gram_mark:10s} {ng['text']:25s} - {ng['frequency']:5d}æ¬¡ [{token_type}]")
    report_lines.append("")

    # éœ€æ±‚æ¨¡å¼
    if demand_patterns:
        report_lines.append("ã€é«˜é¢‘éœ€æ±‚æ¨¡å¼ Top 20ã€‘")
        for i, pattern in enumerate(demand_patterns[:20], 1):
            report_lines.append(f"  {i:2d}. {pattern['pattern']:50s} - {pattern['frequency']:5d}æ¬¡")
            if pattern['examples']:
                report_lines.append(f"      ç¤ºä¾‹: {pattern['examples'][0]}")
        report_lines.append("")

    report_lines.append("="*70)
    report_lines.append("è¾“å‡ºæ–‡ä»¶:")
    report_lines.append(f"  - Token CSV: {csv_file}")
    report_lines.append("="*70)

    # è¾“å‡ºæŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    # ä¿å­˜æŠ¥å‘Š
    report_file = OUTPUT_DIR / 'phase5_framework_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"\nğŸ’¾ æ¡†æ¶åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # ã€å®Œæˆã€‘
    print("\n" + "="*70)
    print("âœ… Phase 5 å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
    print(f"  - å¤„ç†çŸ­è¯­: {len(phrases):,}")
    print(f"  - æå–n-grams: {len(candidate_ngrams)}")
    if not skip_llm:
        print(f"  - å·²åˆ†ç±»n-grams: {len(tokens_with_types)}")
    if demand_patterns:
        print(f"  - éœ€æ±‚æ¨¡å¼: {len(demand_patterns)}")
    print(f"  - ä¿å­˜åˆ°æ•°æ®åº“: {inserted_count}")
    print(f"  - Token CSV: {csv_file}")
    print(f"  - æ¡†æ¶åˆ†ææŠ¥å‘Š: {report_file}")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  1. å®¡æ ¸ tokens_extracted.csv ä¸­çš„tokenåˆ†ç±»")
    print("  2. ä¿®æ”¹é”™è¯¯çš„token_type")
    print("  3. æ ‡è®° verified=True è¡¨ç¤ºå·²å®¡æ ¸")
    print("  4. æŸ¥çœ‹éœ€æ±‚æ¨¡å¼ï¼Œç†è§£ç”¨æˆ·æœç´¢æ„å›¾ç»“æ„")
    print("  5. Tokenè¯åº“å¯ç”¨äºéœ€æ±‚æ¨¡æ¿åŒ–å’Œç›¸ä¼¼åº¦æ£€æµ‹")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 5: Tokenæå–ä¸åˆ†ç±»')
    parser.add_argument(
        '--skip-llm',
        action='store_true',
        help='è·³è¿‡LLMåˆ†ç±»ï¼ˆä»…æå–tokensï¼‰'
    )
    parser.add_argument(
        '--min-frequency',
        type=int,
        default=8,
        help='æœ€å°é¢‘æ¬¡é˜ˆå€¼ï¼ˆé»˜è®¤8ï¼Œæ¨è8-10ï¼‰'
    )
    parser.add_argument(
        '--sample-size',
        type=int,
        default=10000,
        help='é‡‡æ ·çŸ­è¯­æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼Œé»˜è®¤10000ï¼‰'
    )
    parser.add_argument(
        '--round-id',
        type=int,
        default=1,
        help='æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤1ï¼‰'
    )

    args = parser.parse_args()

    try:
        success = run_phase5_tokens(
            skip_llm=args.skip_llm,
            min_frequency=args.min_frequency,
            sample_size=args.sample_size,
            round_id=args.round_id
        )
        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
