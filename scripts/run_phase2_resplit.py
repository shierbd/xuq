"""
Phase 2B: äºŒæ¬¡èšç±»ï¼ˆæ‹†åˆ†å¤§ç°‡ï¼‰
å¯¹æŒ‡å®šçš„å¤§ç°‡è¿›è¡ŒäºŒæ¬¡èšç±»ï¼Œç”Ÿæˆæ›´ç»†ç²’åº¦çš„å­ç°‡

è¿è¡Œæ–¹å¼ï¼š
    python scripts/run_phase2_resplit.py --cluster-id 5 [--round-id 1]

å‚æ•°ï¼š
    --cluster-id: è¦æ‹†åˆ†çš„ç°‡IDï¼ˆé»˜è®¤ä¸º5ï¼‰
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
"""
import sys
import argparse
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ========== ç¼–ç ä¿®å¤ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰ï¼‰==========
from utils.encoding_fix import setup_encoding
setup_encoding()
# ======================================================

from config.settings import OUTPUT_DIR, CACHE_DIR
from core.clustering import ClusteringEngine
from storage.repository import PhraseRepository, ClusterMetaRepository
from storage.models import Phrase


# äºŒæ¬¡èšç±»ä¸“ç”¨å‚æ•°ï¼ˆæ›´aggressiveï¼Œç”¨äºæ‹†åˆ†å¤§ç°‡ï¼‰
RESPLIT_CONFIG = {
    "min_cluster_size": 20,      # é™ä½æœ€å°ç°‡å¤§å°
    "min_samples": 1,             # é™ä½æ ¸å¿ƒç‚¹è¦æ±‚
    "metric": "cosine",           # ä¿æŒä¸€è‡´
    "cluster_selection_epsilon": 0.0,
    "cluster_selection_method": "leaf",  # æ”¹ç”¨leafæ–¹æ³•ï¼Œæ›´å®¹æ˜“äº§ç”Ÿæ›´å¤šç°‡
}


def load_embeddings_for_cluster(cluster_id: int, round_id: int = 1):
    """
    åŠ è½½æŒ‡å®šç°‡çš„çŸ­è¯­åŠå…¶embeddings

    Args:
        cluster_id: è¦æ‹†åˆ†çš„ç°‡ID
        round_id: è½®æ¬¡ID

    Returns:
        (phrases, embeddings, phrase_ids)
    """
    print(f"\nã€æ­¥éª¤1ã€‘åŠ è½½ç°‡ {cluster_id} çš„æ•°æ®...")

    # 1. ä»æ•°æ®åº“åŠ è½½è¯¥ç°‡çš„æ‰€æœ‰çŸ­è¯­
    with PhraseRepository() as repo:
        phrases_db = repo.session.query(Phrase).filter(
            Phrase.cluster_id_A == cluster_id
        ).all()

        if not phrases_db:
            print(f"\nâŒ ç°‡ {cluster_id} ä¸­æ²¡æœ‰æ‰¾åˆ°çŸ­è¯­ï¼")
            return None, None, None

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        phrases = [{
            'phrase_id': p.phrase_id,
            'phrase': p.phrase,
            'frequency': p.frequency,
            'volume': p.volume,
            'seed_word': p.seed_word,
            'source_type': p.source_type,
        } for p in phrases_db]

        phrase_ids = [p['phrase_id'] for p in phrases]

        print(f"âœ“ åŠ è½½äº† {len(phrases)} æ¡çŸ­è¯­")
        print(f"  ç¤ºä¾‹çŸ­è¯­: {', '.join([p['phrase'] for p in phrases[:5]])}...")

    # 2. ä»ç¼“å­˜åŠ è½½æ‰€æœ‰embeddings
    cache_file = CACHE_DIR / f'embeddings_round{round_id}.npz'
    if not cache_file.exists():
        print(f"\nâŒ Embeddingç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
        return None, None, None

    print(f"\nğŸ“‚ åŠ è½½embeddingç¼“å­˜: {cache_file.name}")
    data = np.load(cache_file, allow_pickle=True)
    cache_dict = data['cache'].item()

    # 3. æå–è¯¥ç°‡çŸ­è¯­çš„embeddings
    embeddings = []
    missing_count = 0

    for phrase in phrases:
        import hashlib
        cache_key = hashlib.md5(phrase['phrase'].encode('utf-8')).hexdigest()
        if cache_key in cache_dict:
            embeddings.append(cache_dict[cache_key])
        else:
            missing_count += 1
            print(f"âš ï¸  ç¼ºå¤±embedding: {phrase['phrase']}")

    if missing_count > 0:
        print(f"\nâš ï¸  è­¦å‘Š: {missing_count} æ¡çŸ­è¯­ç¼ºå¤±embedding")

    embeddings = np.array(embeddings)
    print(f"âœ“ åŠ è½½embeddingså®Œæˆ: {embeddings.shape}")

    return phrases, embeddings, phrase_ids


def resplit_cluster(cluster_id: int, round_id: int = 1, new_id_start: int = 1001):
    """
    å¯¹æŒ‡å®šç°‡è¿›è¡ŒäºŒæ¬¡èšç±»

    Args:
        cluster_id: è¦æ‹†åˆ†çš„ç°‡ID
        round_id: æ•°æ®è½®æ¬¡ID
        new_id_start: æ–°ç°‡IDçš„èµ·å§‹å€¼
    """
    print("\n" + "="*70)
    print(f"Phase 2B: äºŒæ¬¡èšç±»ï¼ˆæ‹†åˆ†ç°‡ {cluster_id}ï¼‰".center(70))
    print("="*70)

    # 1. åŠ è½½æ•°æ®
    phrases, embeddings, phrase_ids = load_embeddings_for_cluster(cluster_id, round_id)

    if phrases is None:
        return False

    # 2. æ‰§è¡ŒäºŒæ¬¡èšç±»
    print(f"\nã€æ­¥éª¤2ã€‘æ‰§è¡ŒäºŒæ¬¡èšç±»...")
    print(f"  ä½¿ç”¨å‚æ•°: {RESPLIT_CONFIG}")

    engine = ClusteringEngine(config=RESPLIT_CONFIG, cluster_level='A-resplit')
    labels, clusterer = engine.fit_predict(embeddings)

    # åˆ†æç»“æœ
    cluster_info = engine.analyze_clusters(labels, phrases)

    # 3. é‡æ–°ç¼–å· - ä»new_id_startå¼€å§‹
    unique_labels = sorted(set(labels))
    if -1 in unique_labels:
        unique_labels.remove(-1)

    # åˆ›å»ºæ˜ å°„ï¼šæ—§label -> æ–°cluster_id
    label_to_new_id = {-1: -1}  # å™ªéŸ³ç‚¹ä¿æŒ-1
    for idx, old_label in enumerate(unique_labels):
        label_to_new_id[old_label] = new_id_start + idx

    # åº”ç”¨æ˜ å°„
    new_cluster_ids = np.array([label_to_new_id[label] for label in labels])

    print(f"\nâœ“ ç°‡IDé‡æ–°ç¼–å·å®Œæˆ:")
    print(f"  åŸå§‹ç°‡ {cluster_id} -> {len(unique_labels)} ä¸ªæ–°ç°‡ (ID: {new_id_start}~{new_id_start+len(unique_labels)-1})")
    print(f"  å™ªéŸ³ç‚¹: {(new_cluster_ids == -1).sum()} æ¡")

    # 4. æ›´æ–°æ•°æ®åº“
    print(f"\nã€æ­¥éª¤3ã€‘æ›´æ–°æ•°æ®åº“...")

    # 4.1 æ›´æ–°phrasesè¡¨
    print(f"\n  æ›´æ–°phrasesè¡¨...")
    with PhraseRepository() as repo:
        success_count = 0
        for i, phrase_id in enumerate(phrase_ids):
            new_cluster_id = int(new_cluster_ids[i])
            if repo.update_cluster_assignment(phrase_id, cluster_id_A=new_cluster_id):
                success_count += 1

        print(f"  âœ“ å·²æ›´æ–° {success_count}/{len(phrase_ids)} æ¡è®°å½•çš„cluster_id_A")

    # 4.2 åˆ é™¤æ—§çš„cluster_metaè®°å½•
    print(f"\n  åˆ é™¤æ—§çš„èšç±»å…ƒæ•°æ®ï¼ˆç°‡ {cluster_id}ï¼‰...")
    from storage.models import ClusterMeta
    with ClusterMetaRepository() as repo:
        old_meta = repo.session.query(ClusterMeta).filter(
            ClusterMeta.cluster_id == cluster_id,
            ClusterMeta.cluster_level == 'A'
        ).first()
        if old_meta:
            repo.session.delete(old_meta)
            repo.session.commit()
            print(f"  âœ“ å·²åˆ é™¤ç°‡ {cluster_id} çš„å…ƒæ•°æ®")

    # 4.3 ä¿å­˜æ–°çš„cluster_metaè®°å½•
    print(f"\n  ä¿å­˜æ–°èšç±»å…ƒæ•°æ®...")
    with ClusterMetaRepository() as repo:
        saved_count = 0
        for old_label, info in cluster_info.items():
            new_cluster_id = label_to_new_id[old_label]

            # å‡†å¤‡ç¤ºä¾‹çŸ­è¯­
            example_phrases_str = '; '.join(info['example_phrases'])

            # åˆ›å»ºèšç±»å…ƒæ•°æ®
            repo.create_or_update_cluster(
                cluster_id=new_cluster_id,
                cluster_level='A',
                size=info['size'],
                example_phrases=example_phrases_str,
                main_theme=f"[ç”±ç°‡{cluster_id}æ‹†åˆ†]",  # æ ‡è®°æ¥æº
                total_frequency=info['total_frequency']
            )
            saved_count += 1

        print(f"  âœ“ å·²ä¿å­˜ {saved_count} ä¸ªæ–°ç°‡çš„å…ƒæ•°æ®")

    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print(f"\nã€æ­¥éª¤4ã€‘ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("="*70)
    report_lines.append(f"Phase 2B äºŒæ¬¡èšç±»æŠ¥å‘Š - ç°‡ {cluster_id}")
    report_lines.append("="*70)
    report_lines.append("")

    report_lines.append("ã€èšç±»æ¦‚å†µã€‘")
    report_lines.append(f"  åŸå§‹ç°‡ID: {cluster_id}")
    report_lines.append(f"  æ€»çŸ­è¯­æ•°: {len(phrases):,}")
    report_lines.append(f"  æ–°ç°‡æ•°é‡: {len(cluster_info)}")
    noise_count = (new_cluster_ids == -1).sum()
    report_lines.append(f"  å™ªéŸ³ç‚¹æ•°: {noise_count} ({noise_count/len(phrases)*100:.1f}%)")
    report_lines.append(f"  æ–°ç°‡IDèŒƒå›´: {new_id_start}~{new_id_start+len(cluster_info)-1}")
    report_lines.append("")

    # æ–°ç°‡å¤§å°åˆ†å¸ƒ
    sizes = [info['size'] for info in cluster_info.values()]
    if sizes:
        report_lines.append("ã€æ–°ç°‡å¤§å°åˆ†å¸ƒã€‘")
        report_lines.append(f"  æœ€å°: {min(sizes)}")
        report_lines.append(f"  æœ€å¤§: {max(sizes)}")
        report_lines.append(f"  å¹³å‡: {sum(sizes)/len(sizes):.1f}")
        report_lines.append(f"  ä¸­ä½æ•°: {sorted(sizes)[len(sizes)//2]}")
        report_lines.append("")

    # Top 20 æ–°ç°‡
    report_lines.append("ã€Top 20 æ–°ç°‡ã€‘")
    sorted_clusters = sorted(
        [(label_to_new_id[label], info) for label, info in cluster_info.items()],
        key=lambda x: x[1]['size'],
        reverse=True
    )

    report_lines.append(f"{'æ’å':<5} {'æ–°ç°‡ID':<10} {'å¤§å°':<8} {'é¢‘æ¬¡æ€»å’Œ':<12} {'ç¤ºä¾‹çŸ­è¯­'}")
    report_lines.append("-" * 70)

    for rank, (new_id, info) in enumerate(sorted_clusters[:20], 1):
        examples = ', '.join(info['example_phrases'][:3])
        if len(examples) > 40:
            examples = examples[:37] + '...'
        report_lines.append(
            f"{rank:<5} {new_id:<10} {info['size']:<8} "
            f"{info['total_frequency']:<12} {examples}"
        )

    report_lines.append("")
    report_lines.append("="*70)
    report_lines.append(f"ä¸‹ä¸€æ­¥: éªŒè¯æ•´ä½“èšç±»åˆ†å¸ƒæ˜¯å¦åˆç†")
    report_lines.append("="*70)

    # è¾“å‡ºæŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / f'phase2b_resplit_cluster{cluster_id}_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # 6. å®Œæˆ
    print("\n" + "="*70)
    print(f"âœ… ç°‡ {cluster_id} äºŒæ¬¡èšç±»å®Œæˆï¼".center(70))
    print("="*70)

    print(f"\nğŸ“Š äºŒæ¬¡èšç±»æ‘˜è¦:")
    print(f"  - åŸå§‹ç°‡: {cluster_id} ({len(phrases):,} æ¡çŸ­è¯­)")
    print(f"  - æ–°ç°‡æ•°: {len(cluster_info)}")
    print(f"  - æ–°ç°‡IDèŒƒå›´: {new_id_start}~{new_id_start+len(cluster_info)-1}")
    print(f"  - å™ªéŸ³ç‚¹: {noise_count}")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

    print(f"\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print(f"  1. æ£€æŸ¥æ‰€æœ‰cluster_id_Açš„æ•´ä½“åˆ†å¸ƒ")
    print(f"  2. å¦‚æœåˆ†å¸ƒåˆç†ï¼ˆ30-150ä¸ªç°‡ï¼‰ï¼Œè¿›å…¥Phase 3")
    print(f"  3. å¦‚æœè¿˜æœ‰å…¶ä»–å¤§ç°‡éœ€è¦æ‹†åˆ†ï¼Œé‡å¤è¿è¡Œæ­¤è„šæœ¬")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 2B: äºŒæ¬¡èšç±»ï¼ˆæ‹†åˆ†å¤§ç°‡ï¼‰')
    parser.add_argument(
        '--cluster-id',
        type=int,
        default=5,
        help='è¦æ‹†åˆ†çš„ç°‡IDï¼ˆé»˜è®¤ä¸º5ï¼‰'
    )
    parser.add_argument(
        '--round-id',
        type=int,
        default=1,
        help='æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰'
    )
    parser.add_argument(
        '--new-id-start',
        type=int,
        default=1001,
        help='æ–°ç°‡IDçš„èµ·å§‹å€¼ï¼ˆé»˜è®¤ä¸º1001ï¼‰'
    )

    args = parser.parse_args()

    try:
        success = resplit_cluster(
            cluster_id=args.cluster_id,
            round_id=args.round_id,
            new_id_start=args.new_id_start
        )
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
