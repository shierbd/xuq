"""
Phase 4: å°ç»„èšç±» + éœ€æ±‚å¡ç‰‡ç”Ÿæˆ
å¯¹é€‰ä¸­çš„å¤§ç»„è¿›è¡Œå°ç»„èšç±»ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆéœ€æ±‚å¡ç‰‡åˆç¨¿

è¿è¡Œæ–¹å¼:
    python scripts/run_phase4_demands.py [--skip-llm] [--test-limit N]

å‚æ•°:
    --skip-llm: è·³è¿‡LLMéœ€æ±‚å¡ç‰‡ç”Ÿæˆï¼ˆä»…åšèšç±»ï¼‰
    --test-limit: ä»…å¤„ç†å‰Nä¸ªé€‰ä¸­çš„èšç±»ï¼ˆç”¨äºæµ‹è¯•ï¼‰
"""
import sys
import argparse
import numpy as np
from pathlib import Path
from collections import defaultdict

# è®¾ç½®UTF-8ç¼–ç è¾“å‡ºï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import OUTPUT_DIR, CACHE_DIR, DEMAND_CARD_PHRASE_SAMPLE_SIZE
from core.clustering import cluster_phrases_small
from core.embedding import EmbeddingService
from ai.client import LLMClient
from storage.repository import (
    PhraseRepository,
    ClusterMetaRepository,
    DemandRepository,
    TokenRepository
)
from storage.models import Phrase, ClusterMeta, Demand
import pandas as pd
from utils.token_extractor import extract_tokens_from_phrase


def load_token_framework() -> dict:
    """
    ä»æ•°æ®åº“åŠ è½½Tokenæ¡†æ¶è¯åº“

    Returns:
        tokens_classified: {token_text: token_type} å­—å…¸
    """
    print("\nğŸ“š åŠ è½½Tokenæ¡†æ¶è¯åº“...")

    with TokenRepository() as repo:
        tokens = repo.get_all_tokens()

    if not tokens:
        print("  âš ï¸  æœªæ‰¾åˆ°Tokenæ¡†æ¶ï¼Œè¯·å…ˆè¿è¡ŒPhase 5æå–Token")
        return {}

    tokens_classified = {
        token.token_text: token.token_type
        for token in tokens
    }

    print(f"  âœ“ åŠ è½½äº† {len(tokens_classified)} ä¸ªå·²åˆ†ç±»tokens")

    # æŒ‰ç±»å‹ç»Ÿè®¡
    type_counts = {}
    for token_type in tokens_classified.values():
        type_counts[token_type] = type_counts.get(token_type, 0) + 1

    print(f"  åˆ†ç±»ç»Ÿè®¡:")
    for token_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"    - {token_type}: {count} ä¸ª")

    return tokens_classified


def extract_cluster_framework(phrases: list, tokens_classified: dict) -> dict:
    """
    æå–å°ç»„çš„éœ€æ±‚æ¡†æ¶ï¼ˆintent/action/object tokensï¼‰

    Args:
        phrases: çŸ­è¯­æ–‡æœ¬åˆ—è¡¨
        tokens_classified: Tokenåˆ†ç±»å­—å…¸

    Returns:
        æ¡†æ¶ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«å„ç±»å‹tokensåŠå…¶é¢‘æ¬¡
    """
    if not tokens_classified:
        return {}

    # ç»Ÿè®¡å„ç±»å‹tokensåœ¨æ­¤å°ç»„ä¸­çš„å‡ºç°é¢‘æ¬¡
    framework = {
        'intent': {},    # æ„å›¾è¯: {token: count}
        'action': {},    # åŠ¨ä½œè¯: {token: count}
        'object': {},    # å¯¹è±¡è¯: {token: count}
        'other': {}      # å…¶ä»–: {token: count}
    }

    for phrase in phrases:
        tokens = extract_tokens_from_phrase(phrase)
        for token in tokens:
            token_type = tokens_classified.get(token, 'other')
            if token_type in framework:
                framework[token_type][token] = framework[token_type].get(token, 0) + 1

    # ä¸ºæ¯ä¸ªç±»å‹æ’åºå¹¶å–Top 10
    for token_type in framework:
        sorted_tokens = sorted(framework[token_type].items(),
                              key=lambda x: x[1], reverse=True)
        framework[token_type] = sorted_tokens[:10]

    return framework


def format_framework_for_prompt(framework: dict) -> str:
    """
    å°†æ¡†æ¶ä¿¡æ¯æ ¼å¼åŒ–ä¸ºLLM promptçš„ä¸€éƒ¨åˆ†

    Args:
        framework: æ¡†æ¶ä¿¡æ¯å­—å…¸

    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    if not framework:
        return ""

    lines = ["\nã€éœ€æ±‚æ¡†æ¶åˆ†æã€‘"]
    lines.append("è¯¥å°ç»„çš„å…³é”®è¯åˆ†æç»“æœï¼š")

    if framework.get('intent'):
        intent_tokens = [f"{token}({count}æ¬¡)" for token, count in framework['intent'][:5]]
        lines.append(f"  æ„å›¾è¯: {', '.join(intent_tokens)}")

    if framework.get('action'):
        action_tokens = [f"{token}({count}æ¬¡)" for token, count in framework['action'][:5]]
        lines.append(f"  åŠ¨ä½œè¯: {', '.join(action_tokens)}")

    if framework.get('object'):
        object_tokens = [f"{token}({count}æ¬¡)" for token, count in framework['object'][:5]]
        lines.append(f"  å¯¹è±¡è¯: {', '.join(object_tokens)}")

    lines.append("\nè¯·åŸºäºä»¥ä¸Šæ¡†æ¶åˆ†æï¼Œç»“åˆç¤ºä¾‹çŸ­è¯­ç”Ÿæˆéœ€æ±‚å¡ç‰‡ã€‚")

    return "\n".join(lines)


def load_embeddings_for_phrases(phrase_ids: list, round_id: int = 1) -> np.ndarray:
    """
    ä»ç¼“å­˜åŠ è½½æŒ‡å®šçŸ­è¯­çš„embeddings

    Args:
        phrase_ids: çŸ­è¯­IDåˆ—è¡¨
        round_id: è½®æ¬¡ID

    Returns:
        embeddingsæ•°ç»„
    """
    # åŠ è½½ç¼“å­˜
    cache_file = CACHE_DIR / f'embeddings_round{round_id}.npz'
    if not cache_file.exists():
        raise FileNotFoundError(f"Embeddingç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")

    data = np.load(cache_file, allow_pickle=True)
    cache_dict = data['cache'].item()

    # è·å–çŸ­è¯­æ–‡æœ¬
    with PhraseRepository() as repo:
        phrases_db = repo.session.query(Phrase).filter(
            Phrase.phrase_id.in_(phrase_ids)
        ).all()

        phrase_map = {p.phrase_id: p.phrase for p in phrases_db}

    # æå–embeddings
    embeddings = []
    import hashlib
    for phrase_id in phrase_ids:
        if phrase_id not in phrase_map:
            raise ValueError(f"Phrase ID {phrase_id} ä¸å­˜åœ¨")

        phrase = phrase_map[phrase_id]
        cache_key = hashlib.md5(phrase.encode('utf-8')).hexdigest()

        if cache_key not in cache_dict:
            raise ValueError(f"çŸ­è¯­ '{phrase}' çš„embeddingä¸åœ¨ç¼“å­˜ä¸­")

        embeddings.append(cache_dict[cache_key])

    return np.array(embeddings)


def process_cluster_small_grouping(cluster_A: ClusterMeta,
                                    skip_llm: bool = False,
                                    round_id: int = 1,
                                    min_cluster_size_B: int = None,
                                    min_samples_B: int = None,
                                    tokens_classified: dict = None,
                                    use_framework: bool = False) -> list:
    """
    å¯¹å•ä¸ªå¤§ç»„è¿›è¡Œå°ç»„èšç±»å’Œéœ€æ±‚å¡ç‰‡ç”Ÿæˆ

    Args:
        cluster_A: å¤§ç»„å…ƒæ•°æ®
        skip_llm: æ˜¯å¦è·³è¿‡LLMç”Ÿæˆ
        round_id: æ•°æ®è½®æ¬¡
        min_cluster_size_B: å°ç»„æœ€å°èšç±»å¤§å°
        min_samples_B: å°ç»„æœ€å°æ ·æœ¬æ•°
        tokens_classified: Tokenæ¡†æ¶è¯åº“ï¼ˆå¦‚æœä½¿ç”¨æ¡†æ¶æ¨¡å¼ï¼‰
        use_framework: æ˜¯å¦ä½¿ç”¨æ¡†æ¶æŒ‡å¯¼éœ€æ±‚ç”Ÿæˆ

    Returns:
        ç”Ÿæˆçš„éœ€æ±‚å¡ç‰‡åˆ—è¡¨
    """
    cluster_id = cluster_A.cluster_id
    print(f"\n{'='*70}")
    print(f"å¤„ç†å¤§ç»„ {cluster_id}: {cluster_A.main_theme}")
    print(f"{'='*70}")

    # 1. åŠ è½½è¯¥å¤§ç»„çš„æ‰€æœ‰çŸ­è¯­
    print(f"\nã€æ­¥éª¤1ã€‘åŠ è½½å¤§ç»„çŸ­è¯­...")
    with PhraseRepository() as repo:
        phrases_db = repo.get_phrases_by_cluster(cluster_id, cluster_level='A')

        if not phrases_db:
            print(f"  âš ï¸  å¤§ç»„ {cluster_id} æ²¡æœ‰çŸ­è¯­ï¼Œè·³è¿‡")
            return []

        print(f"  âœ“ åŠ è½½äº† {len(phrases_db)} æ¡çŸ­è¯­")

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        phrases = [{
            'phrase_id': p.phrase_id,
            'phrase': p.phrase,
            'frequency': p.frequency,
            'volume': p.volume,
            'seed_word': p.seed_word,
            'source_type': p.source_type,
        } for p in phrases_db]

        phrase_ids = [p['phrase_id'] for p in phrases]

    # 2. åŠ è½½embeddings
    print(f"\nã€æ­¥éª¤2ã€‘åŠ è½½embeddings...")
    try:
        embeddings = load_embeddings_for_phrases(phrase_ids, round_id)
        print(f"  âœ“ åŠ è½½äº† {embeddings.shape} embeddings")
    except Exception as e:
        print(f"  âŒ åŠ è½½embeddingså¤±è´¥: {str(e)}")
        return []

    # 3. å°ç»„èšç±»
    print(f"\nã€æ­¥éª¤3ã€‘æ‰§è¡Œå°ç»„èšç±»...")
    cluster_ids_B, cluster_info, clusterer = cluster_phrases_small(
        embeddings,
        phrases,
        parent_cluster_id=cluster_id,
        min_cluster_size=min_cluster_size_B,
        min_samples=min_samples_B
    )

    # 4. æ›´æ–°æ•°æ®åº“ - cluster_id_B
    print(f"\nã€æ­¥éª¤4ã€‘æ›´æ–°æ•°æ®åº“...")
    with PhraseRepository() as repo:
        success_count = 0
        for i, phrase_id in enumerate(phrase_ids):
            cluster_id_B = int(cluster_ids_B[i])
            if repo.update_cluster_assignment(phrase_id, cluster_id_B=cluster_id_B):
                success_count += 1

        print(f"  âœ“ å·²æ›´æ–° {success_count}/{len(phrase_ids)} æ¡è®°å½•çš„cluster_id_B")

    # 5. ä¿å­˜å°ç»„å…ƒæ•°æ®
    print(f"\nã€æ­¥éª¤5ã€‘ä¿å­˜å°ç»„å…ƒæ•°æ®...")
    with ClusterMetaRepository() as repo:
        for label, info in cluster_info.items():
            # ä¸ºå°ç»„ç”Ÿæˆå…¨å±€å”¯ä¸€çš„cluster_id_B
            # æ ¼å¼: cluster_A * 10000 + local_label
            cluster_id_B = cluster_id * 10000 + label

            example_phrases_str = '; '.join(info['example_phrases'])

            repo.create_or_update_cluster(
                cluster_id=cluster_id_B,
                cluster_level='B',
                size=info['size'],
                example_phrases=example_phrases_str,
                main_theme=None,  # AIå°†ç”Ÿæˆ
                total_frequency=info['total_frequency'],
                parent_cluster_id=cluster_id
            )

        print(f"  âœ“ å·²ä¿å­˜ {len(cluster_info)} ä¸ªå°ç»„çš„å…ƒæ•°æ®")

    # 6. ç”Ÿæˆéœ€æ±‚å¡ç‰‡
    print(f"\nã€æ­¥éª¤6ã€‘ç”Ÿæˆéœ€æ±‚å¡ç‰‡...")

    demands = []

    if skip_llm:
        print("  âš ï¸  è·³è¿‡LLMéœ€æ±‚å¡ç‰‡ç”Ÿæˆ")
    else:
        try:
            llm = LLMClient()

            for label, info in cluster_info.items():
                cluster_id_B = cluster_id * 10000 + label

                # é‡‡æ ·çŸ­è¯­ï¼ˆæœ€å¤š30æ¡ï¼‰
                phrases_sample = info['all_phrases'][:DEMAND_CARD_PHRASE_SAMPLE_SIZE]

                # ã€æ–°å¢ã€‘å¦‚æœå¯ç”¨æ¡†æ¶æ¨¡å¼ï¼Œæå–å°ç»„æ¡†æ¶
                framework_info = None
                if use_framework and tokens_classified:
                    framework_info = extract_cluster_framework(
                        phrases=info['all_phrases'],  # ä½¿ç”¨å…¨éƒ¨çŸ­è¯­æå–æ¡†æ¶
                        tokens_classified=tokens_classified
                    )

                    # æ‰“å°æ¡†æ¶ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    print(f"\n  å°ç»„{label}æ¡†æ¶åˆ†æ:")
                    if framework_info.get('intent'):
                        top_intent = [f"{t}({c})" for t, c in framework_info['intent'][:3]]
                        print(f"    æ„å›¾: {', '.join(top_intent)}")
                    if framework_info.get('action'):
                        top_action = [f"{t}({c})" for t, c in framework_info['action'][:3]]
                        print(f"    åŠ¨ä½œ: {', '.join(top_action)}")
                    if framework_info.get('object'):
                        top_object = [f"{t}({c})" for t, c in framework_info['object'][:3]]
                        print(f"    å¯¹è±¡: {', '.join(top_object)}")

                # è°ƒç”¨LLMç”Ÿæˆéœ€æ±‚å¡ç‰‡
                demand_draft = llm.generate_demand_card(
                    cluster_id_A=cluster_id,
                    cluster_id_B=cluster_id_B,
                    main_theme=cluster_A.main_theme,
                    phrases=phrases_sample,
                    total_frequency=info['total_frequency'],
                    total_volume=info['total_volume'],
                    framework=framework_info  # ä¼ å…¥æ¡†æ¶ä¿¡æ¯
                )

                # ä¿å­˜åˆ°æ•°æ®åº“
                # å°†priorityæ­£ç¡®æ˜ å°„åˆ°business_valueï¼Œdemand_typeé»˜è®¤ä¸ºother
                priority_to_business = {
                    'high': 'high',
                    'medium': 'medium',
                    'low': 'low'
                }
                business_val = priority_to_business.get(
                    demand_draft.get('priority', 'medium').lower(),
                    'unknown'
                )

                with DemandRepository() as demand_repo:
                    demand = demand_repo.create_demand(
                        title=demand_draft['demand_title'],
                        description=demand_draft['demand_description'],
                        user_scenario=demand_draft['user_intent'],
                        demand_type='other',  # é»˜è®¤ä¸ºotherï¼Œåç»­å¯äººå·¥æ ‡æ³¨ä¸ºtool/content/service/education
                        source_cluster_A=cluster_id,
                        source_cluster_B=cluster_id_B,
                        related_phrases_count=info['size'],
                        business_value=business_val,  # ä½¿ç”¨LLMç”Ÿæˆçš„priorityä½œä¸ºå•†ä¸šä»·å€¼
                        status='idea'
                    )
                    # åœ¨ä¼šè¯å…³é—­å‰ï¼Œä½¿å¯¹è±¡ä¸ä¼šè¯åˆ†ç¦»ä½†ä¿ç•™å·²åŠ è½½çš„å±æ€§
                    demand_repo.session.expunge(demand)
                    demands.append(demand)

            print(f"  âœ“ å·²ç”Ÿæˆ {len(demands)} ä¸ªéœ€æ±‚å¡ç‰‡")

        except Exception as e:
            print(f"  âŒ LLMéœ€æ±‚å¡ç‰‡ç”Ÿæˆå¤±è´¥: {str(e)}")
            print(f"  æç¤º: ä½¿ç”¨ --skip-llm å‚æ•°è·³è¿‡æ­¤æ­¥éª¤")

    # 7. ç”Ÿæˆå°ç»„èšç±»ç»Ÿè®¡
    print(f"\nã€æ­¥éª¤7ã€‘å°ç»„èšç±»ç»Ÿè®¡:")
    print(f"  - å°ç»„æ•°é‡: {len(cluster_info)}")
    noise_count = (cluster_ids_B == -1).sum()
    print(f"  - å™ªéŸ³ç‚¹æ•°: {noise_count} ({noise_count/len(phrases)*100:.1f}%)")

    sizes = [info['size'] for info in cluster_info.values()]
    if sizes:
        print(f"  - å°ç»„å¤§å°: æœ€å°={min(sizes)}, æœ€å¤§={max(sizes)}, å¹³å‡={np.mean(sizes):.1f}")

    return demands


def run_phase4_demands(skip_llm: bool = False,
                       test_limit: int = 0,
                       round_id: int = 1,
                       min_cluster_size_B: int = None,
                       min_samples_B: int = None,
                       use_framework: bool = False):
    """
    æ‰§è¡ŒPhase 4: å°ç»„èšç±» + éœ€æ±‚å¡ç‰‡ç”Ÿæˆ

    Args:
        skip_llm: æ˜¯å¦è·³è¿‡LLMç”Ÿæˆ
        test_limit: æµ‹è¯•æ¨¡å¼ï¼Œä»…å¤„ç†å‰Nä¸ªèšç±»
        round_id: æ•°æ®è½®æ¬¡
        min_cluster_size_B: å°ç»„æœ€å°èšç±»å¤§å°
        min_samples_B: å°ç»„æœ€å°æ ·æœ¬æ•°
        use_framework: æ˜¯å¦ä½¿ç”¨Tokenæ¡†æ¶æŒ‡å¯¼éœ€æ±‚ç”Ÿæˆ
    """
    print("\n" + "="*70)
    print("Phase 4: å°ç»„èšç±» + éœ€æ±‚å¡ç‰‡ç”Ÿæˆ".center(70))
    print("="*70)

    # ã€æ–°å¢ã€‘å¦‚æœå¯ç”¨æ¡†æ¶æ¨¡å¼ï¼ŒåŠ è½½Tokenæ¡†æ¶
    tokens_classified = {}
    if use_framework:
        print("\nâš™ï¸  æ¡†æ¶æ¨¡å¼: å·²å¯ç”¨")
        tokens_classified = load_token_framework()
        if not tokens_classified:
            print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°Tokenæ¡†æ¶ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
            use_framework = False
    else:
        print("\nâš™ï¸  ä¼ ç»Ÿæ¨¡å¼: ä¸ä½¿ç”¨æ¡†æ¶")

    # 1. åŠ è½½é€‰ä¸­çš„å¤§ç»„
    print("\nã€é˜¶æ®µ1ã€‘åŠ è½½é€‰ä¸­çš„å¤§ç»„...")
    with ClusterMetaRepository() as repo:
        selected_clusters = repo.get_selected_clusters(cluster_level='A')

        if not selected_clusters:
            print("\nâŒ æ²¡æœ‰é€‰ä¸­çš„å¤§ç»„ï¼")
            print("  è¯·å…ˆè¿è¡Œ Phase 3 å¹¶å®Œæˆç­›é€‰")
            return False

        print(f"âœ“ åŠ è½½äº† {len(selected_clusters)} ä¸ªé€‰ä¸­çš„å¤§ç»„")

        # ã€æ–°å¢ã€‘æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥å“ªäº›å¤§ç»„å·²å¤„ç†
        clusters_B_all = repo.get_all_clusters('B')
        processed_parents = set([c.parent_cluster_id for c in clusters_B_all if c.parent_cluster_id])

        if processed_parents:
            print(f"  å·²å¤„ç†çš„å¤§ç»„: {len(processed_parents)} ä¸ª")
            print(f"  è¿›åº¦: {len(processed_parents)}/{len(selected_clusters)} ({len(processed_parents)/len(selected_clusters)*100:.1f}%)")

            # è¿‡æ»¤å‡ºæœªå¤„ç†çš„å¤§ç»„
            unprocessed_clusters = [c for c in selected_clusters if c.cluster_id not in processed_parents]

            if not unprocessed_clusters:
                print("\nâœ… æ‰€æœ‰å¤§ç»„éƒ½å·²å¤„ç†å®Œæˆï¼")
                # ä»ç„¶ç»§ç»­ï¼Œä»¥ä¾¿ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
                selected_clusters = selected_clusters
            else:
                print(f"  æœªå¤„ç†çš„å¤§ç»„: {len(unprocessed_clusters)} ä¸ª")
                print(f"  IDåˆ—è¡¨: {sorted([c.cluster_id for c in unprocessed_clusters])}")
                print("\nğŸ’¡ æ–­ç‚¹ç»­ä¼ : å°†åªå¤„ç†æœªå®Œæˆçš„å¤§ç»„")
                selected_clusters = unprocessed_clusters

        # æŒ‰å¤§å°æ’åºï¼ˆå¤§çš„å…ˆå¤„ç†ï¼‰
        selected_clusters = sorted(selected_clusters, key=lambda x: x.size, reverse=True)

        # æµ‹è¯•æ¨¡å¼
        if test_limit > 0:
            print(f"âš ï¸  æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {test_limit} ä¸ªå¤§ç»„")
            selected_clusters = selected_clusters[:test_limit]

    # 2. å¯¹æ¯ä¸ªå¤§ç»„è¿›è¡Œå°ç»„èšç±»
    print("\nã€é˜¶æ®µ2ã€‘å°ç»„èšç±»å’Œéœ€æ±‚å¡ç‰‡ç”Ÿæˆ...")

    all_demands = []
    processed_count = 0
    failed_clusters = []

    for i, cluster_A in enumerate(selected_clusters, 1):
        print(f"\nè¿›åº¦: {i}/{len(selected_clusters)}")

        try:
            demands = process_cluster_small_grouping(
                cluster_A,
                skip_llm=skip_llm,
                round_id=round_id,
                min_cluster_size_B=min_cluster_size_B,
                min_samples_B=min_samples_B,
                tokens_classified=tokens_classified,
                use_framework=use_framework
            )
            all_demands.extend(demands)
            processed_count += 1

        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤§ç»„ {cluster_A.cluster_id} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            failed_clusters.append(cluster_A.cluster_id)

    # 3. ç”Ÿæˆéœ€æ±‚å¡ç‰‡CSVæŠ¥å‘Š
    print("\nã€é˜¶æ®µ3ã€‘ç”Ÿæˆéœ€æ±‚å¡ç‰‡æŠ¥å‘Š...")

    # ã€ä¿®æ”¹ã€‘ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰éœ€æ±‚ï¼ˆä¸åªæ˜¯æœ¬æ¬¡ç”Ÿæˆçš„ï¼‰
    print("  æ­£åœ¨ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰éœ€æ±‚å¡ç‰‡...")
    with DemandRepository() as demand_repo:
        all_demands_from_db = demand_repo.session.query(Demand).order_by(Demand.demand_id).all()

    if all_demands_from_db:
        # å‡†å¤‡æ•°æ®
        data = []
        for demand in all_demands_from_db:
            data.append({
                'demand_id': demand.demand_id,
                'title': demand.title,
                'description': demand.description,
                'user_scenario': demand.user_scenario,
                'demand_type': demand.demand_type,
                'source_cluster_A': demand.source_cluster_A,
                'source_cluster_B': demand.source_cluster_B,
                'related_phrases_count': demand.related_phrases_count,
                'business_value': demand.business_value,
                'status': demand.status,
            })

        df = pd.DataFrame(data)

        # ä¿å­˜CSV
        OUTPUT_DIR.mkdir(exist_ok=True)
        csv_file = OUTPUT_DIR / 'demands_draft.csv'
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')

        print(f"  âœ“ CSVæ–‡ä»¶: {csv_file}")
        print(f"  âœ“ æ•°æ®åº“ä¸­å…±æœ‰ {len(all_demands_from_db)} ä¸ªéœ€æ±‚å¡ç‰‡")
        print(f"  âœ“ æœ¬æ¬¡è¿è¡Œæ–°å¢ {len(all_demands)} ä¸ªéœ€æ±‚å¡ç‰‡")

    else:
        print("  âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½•éœ€æ±‚å¡ç‰‡")
        csv_file = None

    # 4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\nã€é˜¶æ®µ4ã€‘ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("="*70)
    report_lines.append("Phase 4 å°ç»„èšç±»ä¸éœ€æ±‚å¡ç‰‡æŠ¥å‘Š")
    report_lines.append("="*70)
    report_lines.append("")

    report_lines.append("ã€å¤„ç†æ¦‚å†µã€‘")
    report_lines.append(f"  é€‰ä¸­å¤§ç»„æ•°: {len(selected_clusters)}")
    report_lines.append(f"  æˆåŠŸå¤„ç†: {processed_count}")
    if failed_clusters:
        report_lines.append(f"  å¤±è´¥å¤§ç»„: {failed_clusters}")
    report_lines.append("")

    # å°ç»„ç»Ÿè®¡
    with ClusterMetaRepository() as repo:
        small_clusters = repo.get_all_clusters(cluster_level='B')
        print(f"  ç”Ÿæˆå°ç»„æ•°: {len(small_clusters)}")

    report_lines.append("ã€å°ç»„èšç±»ç»Ÿè®¡ã€‘")
    report_lines.append(f"  æ€»å°ç»„æ•°: {len(small_clusters)}")

    # éœ€æ±‚å¡ç‰‡ç»Ÿè®¡
    if all_demands:
        report_lines.append("")
        report_lines.append("ã€éœ€æ±‚å¡ç‰‡ç»Ÿè®¡ã€‘")
        report_lines.append(f"  æ€»éœ€æ±‚æ•°: {len(all_demands)}")

        # æŒ‰å¤§ç»„ç»Ÿè®¡
        demands_by_cluster = defaultdict(int)
        for demand in all_demands:
            demands_by_cluster[demand.source_cluster_A] += 1

        report_lines.append("")
        report_lines.append("ã€å„å¤§ç»„éœ€æ±‚æ•°é‡ã€‘")
        for cluster_id, count in sorted(demands_by_cluster.items(), key=lambda x: x[1], reverse=True):
            report_lines.append(f"  å¤§ç»„{cluster_id}: {count} ä¸ªéœ€æ±‚")

    report_lines.append("")
    report_lines.append("="*70)
    report_lines.append("ä¸‹ä¸€æ­¥: åœ¨ demands_draft.csv ä¸­å®¡æ ¸å¹¶ä¿®æ”¹éœ€æ±‚å¡ç‰‡")
    report_lines.append("="*70)

    # è¾“å‡ºæŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    # ä¿å­˜æŠ¥å‘Š
    report_file = OUTPUT_DIR / 'phase4_demands_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"\nğŸ’¾ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # 5. å®Œæˆ
    print("\n" + "="*70)
    print("âœ… Phase 4 å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
    print(f"  - å¤„ç†å¤§ç»„: {processed_count}/{len(selected_clusters)}")
    print(f"  - ç”Ÿæˆå°ç»„: {len(small_clusters)}")
    if all_demands:
        print(f"  - ç”Ÿæˆéœ€æ±‚: {len(all_demands)}")
        print(f"  - éœ€æ±‚CSV: {csv_file}")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    if all_demands:
        print("  1. åœ¨ demands_draft.csv ä¸­å®¡æ ¸å’Œä¿®æ”¹éœ€æ±‚å¡ç‰‡")
        print("  2. å¡«å†™ business_value (high/medium/low)")
        print("  3. ä¿®æ”¹ status (validated/archived)")
        print("  4. è¿è¡Œå¯¼å…¥è„šæœ¬ï¼ˆå¾…å®ç°ï¼‰")
    else:
        print("  é‡æ–°è¿è¡Œå¹¶å¯ç”¨LLMç”Ÿæˆéœ€æ±‚å¡ç‰‡")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 4: å°ç»„èšç±» + éœ€æ±‚å¡ç‰‡ç”Ÿæˆ')
    parser.add_argument(
        '--skip-llm',
        action='store_true',
        help='è·³è¿‡LLMéœ€æ±‚å¡ç‰‡ç”Ÿæˆï¼ˆä»…åšèšç±»ï¼‰'
    )
    parser.add_argument(
        '--test-limit',
        type=int,
        default=0,
        help='æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰Nä¸ªé€‰ä¸­çš„èšç±»ï¼ˆ0=å…¨éƒ¨ï¼‰'
    )
    parser.add_argument(
        '--round-id',
        type=int,
        default=1,
        help='æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰'
    )
    parser.add_argument(
        '--min-cluster-size',
        type=int,
        default=None,
        help='å°ç»„æœ€å°èšç±»å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰'
    )
    parser.add_argument(
        '--min-samples',
        type=int,
        default=None,
        help='å°ç»„æœ€å°æ ·æœ¬æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰'
    )
    parser.add_argument(
        '--use-framework',
        action='store_true',
        help='ä½¿ç”¨Tokenæ¡†æ¶æŒ‡å¯¼éœ€æ±‚ç”Ÿæˆï¼ˆéœ€å…ˆè¿è¡ŒPhase 5ï¼‰'
    )

    args = parser.parse_args()

    try:
        success = run_phase4_demands(
            skip_llm=args.skip_llm,
            test_limit=args.test_limit,
            round_id=args.round_id,
            min_cluster_size_B=args.min_cluster_size,
            min_samples_B=args.min_samples,
            use_framework=args.use_framework
        )
        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
