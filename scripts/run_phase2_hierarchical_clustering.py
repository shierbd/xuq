"""
Phase 2: å±‚æ¬¡èšç±» + LLMä¼˜åŒ–
ä½¿ç”¨Agglomerative Clusteringç”Ÿæˆåˆæ­¥èšç±»ï¼Œç„¶åç”¨LLMéªŒè¯å’Œä¼˜åŒ–

æ–¹æ¡ˆä¼˜åŠ¿ï¼š
1. å±‚æ¬¡ç»“æ„æ›´ç¬¦åˆè¯­ä¹‰å…³ç³»
2. LLMå¯ä»¥éªŒè¯èšç±»çš„è¯­ä¹‰ä¸€è‡´æ€§
3. è‡ªåŠ¨è¯†åˆ«å’Œåˆ†è£‚ä¸ä¸€è‡´çš„èšç±»
4. ç”Ÿæˆæœ‰æ„ä¹‰çš„èšç±»ä¸»é¢˜æ ‡ç­¾

è¿è¡Œæ–¹å¼ï¼š
    python scripts/run_phase2_hierarchical_clustering.py [é€‰é¡¹]

å‚æ•°ï¼š
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
    --n-clusters: ç›®æ ‡èšç±»æ•°é‡ï¼ˆé»˜è®¤120ï¼‰
    --distance-threshold: è·ç¦»é˜ˆå€¼ï¼ˆNone=ä½¿ç”¨n_clustersï¼‰
    --linkage: é“¾æ¥æ–¹æ³•ï¼ˆward, complete, average, singleï¼‰
    --verify-with-llm: æ˜¯å¦ä½¿ç”¨LLMéªŒè¯ï¼ˆé»˜è®¤Trueï¼‰
    --consistency-threshold: LLMä¸€è‡´æ€§å¾—åˆ†é˜ˆå€¼ï¼ˆé»˜è®¤0.7ï¼‰
"""
import sys
import argparse
import json
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score, davies_bouldin_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ç¼–ç ä¿®å¤
from utils.encoding_fix import setup_encoding
setup_encoding()

from config.settings import CACHE_DIR, OUTPUT_DIR, LLM_PROVIDER
from storage.repository import PhraseRepository, ClusterMetaRepository
from storage.models import Phrase
from core.llm_service import LLMService


def load_embeddings_and_phrases(round_id=1):
    """åŠ è½½embeddingså’ŒçŸ­è¯­ä¿¡æ¯ï¼ˆä»æ•°æ®åº“å’Œç¼“å­˜ï¼‰"""
    print(f"\nåŠ è½½æ•°æ®...")

    # 1. ä»æ•°æ®åº“åŠ è½½çŸ­è¯­
    with PhraseRepository() as repo:
        phrases_db = repo.session.query(Phrase).all()
        phrases = [{
            'phrase_id': p.phrase_id,
            'phrase': p.phrase,
            'frequency': p.frequency,
            'volume': p.volume,
        } for p in phrases_db]

    print(f"  ä»æ•°æ®åº“åŠ è½½äº† {len(phrases):,} æ¡çŸ­è¯­")

    # 2. ä»ç¼“å­˜åŠ è½½embeddings
    cache_file = CACHE_DIR / f"embeddings_round{round_id}.npz"
    print(f"  ä»ç¼“å­˜åŠ è½½embeddings: {cache_file.name}")

    import hashlib
    data = np.load(cache_file, allow_pickle=True)
    cache_dict = data['cache'].item()

    # 3. æŒ‰ç…§phraseé¡ºåºæ„å»ºembeddingsçŸ©é˜µ
    embeddings = []
    valid_phrases = []

    for p in phrases:
        cache_key = hashlib.md5(p['phrase'].encode('utf-8')).hexdigest()
        if cache_key in cache_dict:
            embeddings.append(cache_dict[cache_key])
            valid_phrases.append(p)

    embeddings = np.array(embeddings)

    print(f"  æˆåŠŸåŒ¹é… {len(valid_phrases):,}/{len(phrases):,} æ¡çŸ­è¯­çš„embeddings")
    print(f"  Embeddingså½¢çŠ¶: {embeddings.shape}")

    return embeddings, valid_phrases


def run_agglomerative_clustering(embeddings_norm, n_clusters=120, linkage='ward'):
    """æ‰§è¡ŒAgglomerativeå±‚æ¬¡èšç±»"""
    print(f"\nã€æ‰§è¡ŒAgglomerativeå±‚æ¬¡èšç±»ã€‘")
    print(f"  ç›®æ ‡èšç±»æ•°: {n_clusters}")
    print(f"  é“¾æ¥æ–¹æ³•: {linkage}")
    print(f"  æ ·æœ¬æ•°: {len(embeddings_norm):,}")

    # å¯¹äºå¤§æ•°æ®é›†ï¼Œä½¿ç”¨é‡‡æ ·è¯„ä¼°æœ€ä¼˜èšç±»æ•°
    if len(embeddings_norm) > 10000:
        print(f"\n  æ•°æ®é›†è¾ƒå¤§ï¼Œé‡‡æ ·è¯„ä¼°æœ€ä¼˜èšç±»æ•°...")
        sample_indices = np.random.choice(len(embeddings_norm), 10000, replace=False)
        sample_embeddings = embeddings_norm[sample_indices]

        # æµ‹è¯•ä¸åŒèšç±»æ•°
        test_n_clusters = [80, 100, 120, 150, 180]
        scores = []

        for n in test_n_clusters:
            print(f"    æµ‹è¯• n={n}...", end='')
            clusterer = AgglomerativeClustering(n_clusters=n, linkage=linkage)
            labels = clusterer.fit_predict(sample_embeddings)
            silhouette = silhouette_score(sample_embeddings, labels)
            scores.append((n, silhouette))
            print(f" è½®å»“ç³»æ•°={silhouette:.4f}")

        # é€‰æ‹©æœ€ä½³n_clusters
        best_n = max(scores, key=lambda x: x[1])[0]
        print(f"\n  âœ“ æ¨èèšç±»æ•°: {best_n}")
        n_clusters = best_n

    # æ‰§è¡Œèšç±»
    print(f"\n  å¼€å§‹èšç±»ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    clusterer = AgglomerativeClustering(
        n_clusters=n_clusters,
        linkage=linkage,
        compute_distances=False
    )

    cluster_ids = clusterer.fit_predict(embeddings_norm)

    print(f"\n  âœ“ èšç±»å®Œæˆï¼")

    # ç»Ÿè®¡èšç±»å¤§å°
    unique, counts = np.unique(cluster_ids, return_counts=True)
    print(f"\n  èšç±»æ•°é‡: {len(unique)}")
    print(f"  æœ€å°èšç±»: {counts.min()}")
    print(f"  æœ€å¤§èšç±»: {counts.max()}")
    print(f"  å¹³å‡èšç±»: {counts.mean():.1f}")
    print(f"  ä¸­ä½èšç±»: {np.median(counts):.1f}")

    # è¯„ä¼°è´¨é‡
    if len(embeddings_norm) <= 50000:
        print(f"\n  è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡...")
        silhouette = silhouette_score(embeddings_norm, cluster_ids, sample_size=10000)
        davies_bouldin = davies_bouldin_score(embeddings_norm, cluster_ids)
        print(f"    è½®å»“ç³»æ•°: {silhouette:.4f}")
        print(f"    Davies-BouldinæŒ‡æ•°: {davies_bouldin:.4f}")

    return cluster_ids, clusterer


def verify_cluster_consistency_with_llm(phrases: List[str], cluster_id: int, llm_service: LLMService) -> Dict:
    """
    ä½¿ç”¨LLMéªŒè¯èšç±»çš„è¯­ä¹‰ä¸€è‡´æ€§

    Returns:
        {
            'is_consistent': bool,
            'consistency_score': float (0-1),
            'main_theme': str,
            'subclusters': List[Dict] or None,  # å¦‚æœä¸ä¸€è‡´ï¼Œå»ºè®®çš„å­èšç±»
            'reasoning': str
        }
    """
    # é‡‡æ ·çŸ­è¯­ï¼ˆæœ€å¤š30ä¸ªï¼‰
    sample_size = min(30, len(phrases))
    sample_phrases = np.random.choice(phrases, sample_size, replace=False).tolist()

    prompt = f"""åˆ†æä»¥ä¸‹{len(sample_phrases)}ä¸ªæœç´¢çŸ­è¯­ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦å±äºåŒä¸€ä¸ªè¯­ä¹‰ç±»åˆ«ã€‚

çŸ­è¯­åˆ—è¡¨ï¼š
{chr(10).join(f'{i+1}. {p}' for i, p in enumerate(sample_phrases))}

è¯·åˆ†æï¼š
1. è¿™äº›çŸ­è¯­æ˜¯å¦æœ‰å…±åŒçš„ä¸»é¢˜æˆ–æ„å›¾ï¼Ÿ
2. ä¸€è‡´æ€§å¾—åˆ†ï¼ˆ0-1ï¼Œ1è¡¨ç¤ºå®Œå…¨ä¸€è‡´ï¼‰
3. å¦‚æœä¸ä¸€è‡´ï¼ˆå¾—åˆ†<0.7ï¼‰ï¼Œå»ºè®®å¦‚ä½•åˆ†æˆ2-3ä¸ªå­ç±»åˆ«

è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_consistent": true/false,
    "consistency_score": 0.0-1.0,
    "main_theme": "ä¸»é¢˜æè¿°ï¼ˆ3-5ä¸ªè¯ï¼‰",
    "subclusters": [  // ä»…å½“is_consistent=falseæ—¶æä¾›
        {{"theme": "å­ä¸»é¢˜1", "example_indices": [0, 1, 5]}},
        {{"theme": "å­ä¸»é¢˜2", "example_indices": [2, 3, 4]}}
    ],
    "reasoning": "åˆ†æç†ç”±"
}}"""

    try:
        response = llm_service.generate(prompt, temperature=0.3, max_tokens=1000)

        # æå–JSON
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            return result
        else:
            print(f"  âš ï¸ èšç±»{cluster_id}: LLMè¿”å›æ ¼å¼é”™è¯¯")
            return {
                'is_consistent': True,
                'consistency_score': 0.5,
                'main_theme': 'Unknown',
                'reasoning': 'Parse error'
            }
    except Exception as e:
        print(f"  âš ï¸ èšç±»{cluster_id}: LLMéªŒè¯å¤±è´¥ - {str(e)}")
        return {
            'is_consistent': True,
            'consistency_score': 0.5,
            'main_theme': 'Unknown',
            'reasoning': f'Error: {str(e)}'
        }


def split_inconsistent_cluster(cluster_phrases: List[Dict], embeddings: np.ndarray,
                                subclusters_info: List[Dict]) -> List[int]:
    """
    æ ¹æ®LLMå»ºè®®åˆ†è£‚ä¸ä¸€è‡´çš„èšç±»

    Returns:
        æ–°çš„å­èšç±»æ ‡ç­¾åˆ—è¡¨
    """
    if not subclusters_info or len(subclusters_info) < 2:
        # å¦‚æœLLMæ²¡æœ‰æä¾›åˆ†è£‚å»ºè®®ï¼Œä½¿ç”¨KMeansåˆ†æˆ2ä¸ª
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        return kmeans.fit_predict(embeddings)

    # ä½¿ç”¨LLMå»ºè®®çš„åˆ†ç»„ï¼ˆåŸºäºç›¸ä¼¼åº¦é‡æ–°åˆ†é…ï¼‰
    n_subclusters = len(subclusters_info)

    # å¯¹äºå°èšç±»ï¼Œç›´æ¥ä½¿ç”¨KMeans
    if len(cluster_phrases) < 100 or n_subclusters > 5:
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=n_subclusters, random_state=42, n_init=10)
        return kmeans.fit_predict(embeddings)

    # å¦åˆ™ä½¿ç”¨Agglomerative
    clusterer = AgglomerativeClustering(n_clusters=n_subclusters, linkage='ward')
    return clusterer.fit_predict(embeddings)


def verify_and_optimize_clusters(cluster_ids: np.ndarray, phrases: List[Dict],
                                  embeddings: np.ndarray,
                                  verify_with_llm: bool = True,
                                  consistency_threshold: float = 0.7) -> Tuple[np.ndarray, Dict]:
    """
    éªŒè¯å’Œä¼˜åŒ–èšç±»ç»“æœ

    Returns:
        (ä¼˜åŒ–åçš„cluster_ids, cluster_infoå­—å…¸)
    """
    print(f"\nã€éªŒè¯å’Œä¼˜åŒ–èšç±»ã€‘")
    print(f"  LLMéªŒè¯: {'å¯ç”¨' if verify_with_llm else 'ç¦ç”¨'}")
    print(f"  ä¸€è‡´æ€§é˜ˆå€¼: {consistency_threshold}")

    # åˆå§‹åŒ–LLMæœåŠ¡
    llm_service = None
    if verify_with_llm:
        try:
            llm_service = LLMService()
            print(f"  âœ“ LLMæœåŠ¡å·²åˆå§‹åŒ–ï¼ˆProvider: {LLM_PROVIDER}ï¼‰")
        except Exception as e:
            print(f"  âš ï¸ LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print(f"  å°†è·³è¿‡LLMéªŒè¯")
            verify_with_llm = False

    # æ„å»ºåˆå§‹èšç±»ä¿¡æ¯
    unique_clusters = np.unique(cluster_ids)
    print(f"\n  åˆå§‹èšç±»æ•°: {len(unique_clusters)}")

    optimized_cluster_ids = cluster_ids.copy()
    cluster_info = {}
    next_cluster_id = max(unique_clusters) + 1

    inconsistent_count = 0
    split_count = 0

    for cluster_id in unique_clusters:
        indices = np.where(cluster_ids == cluster_id)[0]
        cluster_phrases = [phrases[i] for i in indices]
        cluster_embeddings = embeddings[indices]

        # ç»Ÿè®¡ä¿¡æ¯
        total_frequency = sum(p.get('frequency', 1) for p in cluster_phrases)
        total_volume = sum(p.get('volume', 0) for p in cluster_phrases)

        # ç¤ºä¾‹çŸ­è¯­
        sorted_phrases = sorted(cluster_phrases, key=lambda x: x.get('frequency', 1), reverse=True)
        example_phrases = [p['phrase'] for p in sorted_phrases[:10]]

        print(f"\n  å¤„ç†èšç±» {cluster_id} (å¤§å°: {len(cluster_phrases)})...")

        # LLMéªŒè¯
        main_theme = None
        is_consistent = True
        consistency_score = 1.0

        if verify_with_llm and llm_service and len(cluster_phrases) >= 5:
            phrase_texts = [p['phrase'] for p in cluster_phrases]
            verification = verify_cluster_consistency_with_llm(
                phrase_texts, cluster_id, llm_service
            )

            is_consistent = verification.get('is_consistent', True)
            consistency_score = verification.get('consistency_score', 1.0)
            main_theme = verification.get('main_theme', None)

            print(f"    ä¸€è‡´æ€§å¾—åˆ†: {consistency_score:.2f}")
            print(f"    ä¸»é¢˜: {main_theme}")

            # å¦‚æœä¸ä¸€è‡´ï¼Œåˆ†è£‚èšç±»
            if not is_consistent or consistency_score < consistency_threshold:
                inconsistent_count += 1
                print(f"    âš ï¸ èšç±»ä¸ä¸€è‡´ï¼Œå°è¯•åˆ†è£‚...")

                subclusters_info = verification.get('subclusters', None)
                subcluster_labels = split_inconsistent_cluster(
                    cluster_phrases, cluster_embeddings, subclusters_info
                )

                # æ›´æ–°èšç±»ID
                unique_sublabels = np.unique(subcluster_labels)
                if len(unique_sublabels) > 1:
                    split_count += len(unique_sublabels)
                    print(f"    âœ“ åˆ†è£‚ä¸º {len(unique_sublabels)} ä¸ªå­èšç±»")

                    for sublabel in unique_sublabels:
                        sub_indices = indices[subcluster_labels == sublabel]
                        new_cluster_id = next_cluster_id
                        next_cluster_id += 1

                        optimized_cluster_ids[sub_indices] = new_cluster_id

                        # ä¿å­˜å­èšç±»ä¿¡æ¯
                        sub_phrases = [phrases[i] for i in sub_indices]
                        sub_sorted = sorted(sub_phrases, key=lambda x: x.get('frequency', 1), reverse=True)
                        sub_examples = [p['phrase'] for p in sub_sorted[:10]]
                        sub_freq = sum(p.get('frequency', 1) for p in sub_phrases)
                        sub_vol = sum(p.get('volume', 0) for p in sub_phrases)

                        cluster_info[new_cluster_id] = {
                            'size': len(sub_phrases),
                            'total_frequency': sub_freq,
                            'total_volume': sub_vol,
                            'example_phrases': sub_examples,
                            'main_theme': f"{main_theme} (å­èšç±»{sublabel+1})" if main_theme else None,
                            'consistency_score': None,
                            'parent_cluster': cluster_id
                        }

                    continue  # è·³è¿‡åŸèšç±»çš„ä¿å­˜

        # ä¿å­˜èšç±»ä¿¡æ¯ï¼ˆä¸€è‡´çš„èšç±»æˆ–ä¸åšLLMéªŒè¯çš„èšç±»ï¼‰
        cluster_info[cluster_id] = {
            'size': len(cluster_phrases),
            'total_frequency': total_frequency,
            'total_volume': total_volume,
            'example_phrases': example_phrases,
            'main_theme': main_theme,
            'consistency_score': consistency_score,
            'parent_cluster': None
        }

    print(f"\n  âœ“ éªŒè¯å®Œæˆ")
    print(f"    ä¸ä¸€è‡´èšç±»: {inconsistent_count}")
    print(f"    åˆ†è£‚äº§ç”Ÿçš„æ–°èšç±»: {split_count}")
    print(f"    æœ€ç»ˆèšç±»æ•°: {len(cluster_info)}")

    return optimized_cluster_ids, cluster_info


def update_database(cluster_ids, phrases, cluster_info, round_id=1):
    """æ›´æ–°æ•°æ®åº“"""
    print("\nã€æ›´æ–°æ•°æ®åº“ã€‘")

    # 1. æ›´æ–°phrasesè¡¨
    print("\n  æ›´æ–°phrasesè¡¨çš„cluster_id_A...")
    with PhraseRepository() as repo:
        success_count = 0
        for i, phrase in enumerate(phrases):
            phrase_id = phrase['phrase_id']
            cluster_id = int(cluster_ids[i])

            if repo.update_cluster_assignment(phrase_id, cluster_id_A=cluster_id):
                success_count += 1

        print(f"  âœ“ å·²æ›´æ–° {success_count}/{len(phrases)} æ¡è®°å½•")

    # 2. ä¿å­˜cluster_metaè¡¨
    print("\n  ä¿å­˜èšç±»å…ƒæ•°æ®...")
    with ClusterMetaRepository() as repo:
        for cluster_id, info in cluster_info.items():
            example_phrases_str = '; '.join(info['example_phrases'])

            repo.create_or_update_cluster(
                cluster_id=cluster_id,
                cluster_level='A',
                size=info['size'],
                example_phrases=example_phrases_str,
                main_theme=info.get('main_theme'),
                total_frequency=info['total_frequency']
            )

        print(f"  âœ“ å·²ä¿å­˜ {len(cluster_info)} ä¸ªèšç±»çš„å…ƒæ•°æ®")


def generate_report(cluster_ids, cluster_info, output_file):
    """ç”Ÿæˆä¼˜åŒ–èšç±»æŠ¥å‘Š"""
    print("\nã€ç”Ÿæˆèšç±»æŠ¥å‘Šã€‘")

    lines = []
    lines.append("="*70)
    lines.append("å±‚æ¬¡èšç±» + LLMä¼˜åŒ–æŠ¥å‘Š")
    lines.append("="*70)
    lines.append("")

    lines.append("ã€èšç±»æ¦‚å†µã€‘")
    lines.append(f"  ç®—æ³•: Agglomerative Clustering + LLMéªŒè¯")
    lines.append(f"  èšç±»æ•°: {len(cluster_info)}")
    lines.append(f"  æ€»æ ·æœ¬æ•°: {len(cluster_ids):,}")
    lines.append("")

    lines.append("ã€èšç±»å¤§å°åˆ†å¸ƒã€‘")
    sizes = [info['size'] for info in cluster_info.values()]
    lines.append(f"  æœ€å°: {min(sizes)}")
    lines.append(f"  æœ€å¤§: {max(sizes)}")
    lines.append(f"  å¹³å‡: {sum(sizes)/len(sizes):.1f}")
    lines.append(f"  ä¸­ä½æ•°: {sorted(sizes)[len(sizes)//2]}")
    lines.append("")

    # ç»Ÿè®¡æœ‰ä¸»é¢˜çš„èšç±»
    themed_clusters = sum(1 for info in cluster_info.values() if info.get('main_theme'))
    if themed_clusters > 0:
        lines.append("ã€LLMç”Ÿæˆçš„ä¸»é¢˜ã€‘")
        lines.append(f"  æœ‰ä¸»é¢˜æ ‡ç­¾çš„èšç±»: {themed_clusters}/{len(cluster_info)}")
        lines.append("")

    lines.append("ã€Top 20 æœ€å¤§èšç±»ã€‘")
    sorted_clusters = sorted(cluster_info.items(), key=lambda x: x[1]['size'], reverse=True)
    lines.append(f"{'æ’å':<5} {'èšç±»ID':<10} {'å¤§å°':<8} {'ä¸€è‡´æ€§':<10} {'ä¸»é¢˜':<30} {'ç¤ºä¾‹çŸ­è¯­'}")
    lines.append("-" * 120)

    for rank, (cluster_id, info) in enumerate(sorted_clusters[:20], 1):
        examples = ', '.join(info['example_phrases'][:3])
        if len(examples) > 35:
            examples = examples[:32] + '...'

        theme = info.get('main_theme', 'N/A')
        if theme and len(theme) > 28:
            theme = theme[:25] + '...'

        consistency = info.get('consistency_score')
        consistency_str = f"{consistency:.2f}" if consistency is not None else "N/A"

        lines.append(
            f"{rank:<5} {cluster_id:<10} {info['size']:<8} "
            f"{consistency_str:<10} {theme:<30} {examples}"
        )

    lines.append("")
    lines.append("="*70)
    lines.append("ä¸‹ä¸€æ­¥: è¿è¡Œ Phase 3 ç”Ÿæˆå¤§ç»„ç­›é€‰æŠ¥å‘Š")
    lines.append("="*70)

    report_text = '\n'.join(lines)
    print(report_text)

    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 2: å±‚æ¬¡èšç±» + LLMä¼˜åŒ–')
    parser.add_argument('--round-id', type=int, default=1, help='æ•°æ®è½®æ¬¡ID')
    parser.add_argument('--n-clusters', type=int, default=120, help='ç›®æ ‡èšç±»æ•°é‡')
    parser.add_argument('--linkage', type=str, default='ward',
                       choices=['ward', 'complete', 'average', 'single'],
                       help='é“¾æ¥æ–¹æ³•')
    parser.add_argument('--verify-with-llm', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨LLMéªŒè¯')
    parser.add_argument('--no-llm', dest='verify_with_llm', action='store_false',
                       help='ç¦ç”¨LLMéªŒè¯')
    parser.add_argument('--consistency-threshold', type=float, default=0.7,
                       help='LLMä¸€è‡´æ€§å¾—åˆ†é˜ˆå€¼')

    args = parser.parse_args()

    print("\n" + "="*70)
    print("å±‚æ¬¡èšç±» + LLMä¼˜åŒ–".center(70))
    print("="*70)

    round_id = args.round_id

    # 1. åŠ è½½æ•°æ®
    print("\nã€æ­¥éª¤1ã€‘åŠ è½½æ•°æ®...")
    embeddings, phrases = load_embeddings_and_phrases(round_id)

    # 2. å½’ä¸€åŒ–
    print("\nã€æ­¥éª¤2ã€‘å½’ä¸€åŒ–å‘é‡...")
    embeddings_norm = normalize(embeddings, norm='l2')

    # 3. æ‰§è¡Œå±‚æ¬¡èšç±»
    print("\nã€æ­¥éª¤3ã€‘æ‰§è¡Œå±‚æ¬¡èšç±»...")
    cluster_ids, clusterer = run_agglomerative_clustering(
        embeddings_norm,
        n_clusters=args.n_clusters,
        linkage=args.linkage
    )

    # 4. LLMéªŒè¯å’Œä¼˜åŒ–
    print("\nã€æ­¥éª¤4ã€‘LLMéªŒè¯å’Œä¼˜åŒ–...")
    optimized_cluster_ids, cluster_info = verify_and_optimize_clusters(
        cluster_ids, phrases, embeddings_norm,
        verify_with_llm=args.verify_with_llm,
        consistency_threshold=args.consistency_threshold
    )

    # 5. æ›´æ–°æ•°æ®åº“
    print("\nã€æ­¥éª¤5ã€‘æ›´æ–°æ•°æ®åº“...")
    update_database(optimized_cluster_ids, phrases, cluster_info, round_id)

    # 6. ç”ŸæˆæŠ¥å‘Š
    print("\nã€æ­¥éª¤6ã€‘ç”ŸæˆæŠ¥å‘Š...")
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / f'phase2_hierarchical_clustering_report_round{round_id}.txt'
    generate_report(optimized_cluster_ids, cluster_info, report_file)

    print("\n" + "="*70)
    print("âœ… å±‚æ¬¡èšç±» + LLMä¼˜åŒ–å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"  - å¤„ç†çŸ­è¯­æ•°: {len(phrases):,}")
    print(f"  - ç”Ÿæˆèšç±»æ•°: {len(cluster_info)}")
    print(f"  - LLMéªŒè¯: {'å¯ç”¨' if args.verify_with_llm else 'ç¦ç”¨'}")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ Phase 3: python scripts/run_phase3_selection.py")


if __name__ == "__main__":
    main()
