"""
ä½¿ç”¨K-Meansæ›¿ä»£HDBSCANè¿›è¡Œèšç±»
åŸºäºæ·±åº¦åˆ†ææŠ¥å‘Šçš„æ¨èæ–¹æ¡ˆ
"""
import sys
from pathlib import Path
import numpy as np
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ç¼–ç ä¿®å¤
from utils.encoding_fix import setup_encoding
setup_encoding()

from config.settings import CACHE_DIR, OUTPUT_DIR
from storage.repository import PhraseRepository, ClusterMetaRepository
from storage.models import Phrase


def load_embeddings_and_phrases(round_id=1):
    """åŠ è½½embeddingså’ŒçŸ­è¯­ä¿¡æ¯ï¼ˆä»æ•°æ®åº“ï¼‰"""
    print(f"\nåŠ è½½æ•°æ®...")

    # 1. ä»æ•°æ®åº“åŠ è½½çŸ­è¯­
    with PhraseRepository() as repo:
        # åŠ è½½æ‰€æœ‰çŸ­è¯­ï¼ˆä¸è¿‡æ»¤çŠ¶æ€ï¼Œå› ä¸ºå¯èƒ½å·²è¢«HDBSCANå¤„ç†è¿‡ï¼‰
        phrases_db = repo.session.query(Phrase).all()

        phrases = [{
            'phrase_id': p.phrase_id,
            'phrase': p.phrase,
            'frequency': p.frequency,
            'volume': p.volume,
        } for p in phrases_db]

    print(f"  ä»æ•°æ®åº“åŠ è½½äº† {len(phrases):,} æ¡çŸ­è¯­")

    # 2. ä»ç¼“å­˜åŠ è½½embeddings
    cache_file = CACHE_DIR / f"embeddings_round{round_id}.npz"
    print(f"  ä»ç¼“å­˜åŠ è½½embeddings: {cache_file.name}")

    import hashlib
    data = np.load(cache_file, allow_pickle=True)
    cache_dict = data['cache'].item()

    # 3. æŒ‰ç…§phraseé¡ºåºæ„å»ºembeddingsçŸ©é˜µ
    embeddings = []
    valid_phrases = []

    for p in phrases:
        # è®¡ç®—ç¼“å­˜é”®ï¼ˆä¸EmbeddingService._get_cache_keyä¿æŒä¸€è‡´ï¼‰
        cache_key = hashlib.md5(p['phrase'].encode('utf-8')).hexdigest()
        if cache_key in cache_dict:
            embeddings.append(cache_dict[cache_key])
            valid_phrases.append(p)

    embeddings = np.array(embeddings)

    print(f"  æˆåŠŸåŒ¹é… {len(valid_phrases):,}/{len(phrases):,} æ¡çŸ­è¯­çš„embeddings")
    print(f"  Embeddingså½¢çŠ¶: {embeddings.shape}")

    return embeddings, valid_phrases


def find_optimal_k(embeddings_norm, k_values, sample_size=10000):
    """æ‰¾åˆ°æœ€ä¼˜Kå€¼"""
    print("\nã€å¯»æ‰¾æœ€ä¼˜Kå€¼ã€‘")

    # é‡‡æ ·åŠ é€Ÿ
    if len(embeddings_norm) > sample_size:
        print(f"  ä½¿ç”¨{sample_size}ä¸ªæ ·æœ¬å¿«é€Ÿè¯„ä¼°...")
        indices = np.random.choice(len(embeddings_norm), sample_size, replace=False)
        sample_embeddings = embeddings_norm[indices]
    else:
        sample_embeddings = embeddings_norm

    results = []

    for k in k_values:
        print(f"\n  æµ‹è¯• K={k}...")

        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)
        labels = kmeans.fit_predict(sample_embeddings)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        silhouette = silhouette_score(sample_embeddings, labels)
        davies_bouldin = davies_bouldin_score(sample_embeddings, labels)
        calinski_harabasz = calinski_harabasz_score(sample_embeddings, labels)

        print(f"    è½®å»“ç³»æ•°: {silhouette:.4f}")
        print(f"    Davies-Bouldin: {davies_bouldin:.4f} (è¶Šå°è¶Šå¥½)")
        print(f"    Calinski-Harabasz: {calinski_harabasz:.2f} (è¶Šå¤§è¶Šå¥½)")

        results.append({
            'k': k,
            'silhouette': silhouette,
            'davies_bouldin': davies_bouldin,
            'calinski_harabasz': calinski_harabasz
        })

    # é€‰æ‹©æœ€ä¼˜K
    best_result = max(results, key=lambda x: x['silhouette'])
    print(f"\n  âœ“ æœ€ä¼˜Kå€¼: {best_result['k']} (è½®å»“ç³»æ•°={best_result['silhouette']:.4f})")

    return best_result['k'], results


def run_kmeans_clustering(k, embeddings_norm, use_minibatch=True):
    """æ‰§è¡ŒK-Meansèšç±»"""
    print(f"\nã€æ‰§è¡ŒK-Meansèšç±»ã€‘")
    print(f"  K={k}, æ ·æœ¬æ•°={len(embeddings_norm):,}")

    if use_minibatch and len(embeddings_norm) > 50000:
        print(f"  ä½¿ç”¨MiniBatch K-MeansåŠ é€Ÿå¤§æ•°æ®é›†èšç±»...")
        kmeans = MiniBatchKMeans(
            n_clusters=k,
            random_state=42,
            batch_size=1024,
            max_iter=100,
            n_init=10,
            verbose=1
        )
    else:
        print(f"  ä½¿ç”¨æ ‡å‡†K-Means...")
        kmeans = KMeans(
            n_clusters=k,
            random_state=42,
            n_init=20,
            max_iter=300,
            verbose=1
        )

    cluster_ids = kmeans.fit_predict(embeddings_norm)

    print(f"\n  âœ“ èšç±»å®Œæˆï¼")

    # ç»Ÿè®¡èšç±»å¤§å°
    unique, counts = np.unique(cluster_ids, return_counts=True)
    print(f"\n  èšç±»æ•°é‡: {len(unique)}")
    print(f"  æœ€å°èšç±»: {counts.min()}")
    print(f"  æœ€å¤§èšç±»: {counts.max()}")
    print(f"  å¹³å‡èšç±»: {counts.mean():.1f}")
    print(f"  ä¸­ä½èšç±»: {np.median(counts):.1f}")

    return cluster_ids, kmeans


def update_database(cluster_ids, phrases, round_id=1):
    """æ›´æ–°æ•°æ®åº“"""
    print("\nã€æ›´æ–°æ•°æ®åº“ã€‘")

    # 1. æ›´æ–°phrasesè¡¨
    print("\n  æ›´æ–°phrasesè¡¨çš„cluster_id_A...")
    with PhraseRepository() as repo:
        success_count = 0
        for i, phrase in enumerate(phrases):
            phrase_id = phrase['phrase_id']
            cluster_id = int(cluster_ids[i])

            if repo.update_cluster_assignment(phrase_id, cluster_id_A=cluster_id):
                success_count += 1

        print(f"  âœ“ å·²æ›´æ–° {success_count}/{len(phrases)} æ¡è®°å½•")

    # 2. ä¿å­˜cluster_metaè¡¨
    print("\n  ä¿å­˜èšç±»å…ƒæ•°æ®...")

    # ç»Ÿè®¡æ¯ä¸ªèšç±»
    cluster_info = {}
    for cluster_id in set(cluster_ids):
        indices = np.where(cluster_ids == cluster_id)[0]
        cluster_phrases = [phrases[i] for i in indices]

        # ç»Ÿè®¡
        total_frequency = sum(p.get('frequency', 1) for p in cluster_phrases)
        total_volume = sum(p.get('volume', 0) for p in cluster_phrases)

        # ç¤ºä¾‹çŸ­è¯­ï¼ˆæŒ‰é¢‘æ¬¡æ’åºï¼‰
        sorted_phrases = sorted(cluster_phrases, key=lambda x: x.get('frequency', 1), reverse=True)
        example_phrases = [p['phrase'] for p in sorted_phrases[:10]]

        cluster_info[cluster_id] = {
            'size': len(cluster_phrases),
            'total_frequency': total_frequency,
            'total_volume': total_volume,
            'example_phrases': example_phrases
        }

    with ClusterMetaRepository() as repo:
        for cluster_id, info in cluster_info.items():
            example_phrases_str = '; '.join(info['example_phrases'])

            repo.create_or_update_cluster(
                cluster_id=cluster_id,
                cluster_level='A',
                size=info['size'],
                example_phrases=example_phrases_str,
                main_theme=None,
                total_frequency=info['total_frequency']
            )

        print(f"  âœ“ å·²ä¿å­˜ {len(cluster_info)} ä¸ªèšç±»çš„å…ƒæ•°æ®")

    return cluster_info


def generate_report(k, cluster_ids, cluster_info, eval_results, output_file):
    """ç”Ÿæˆèšç±»æŠ¥å‘Š"""
    print("\nã€ç”Ÿæˆèšç±»æŠ¥å‘Šã€‘")

    lines = []
    lines.append("="*70)
    lines.append("K-Meansèšç±»æŠ¥å‘Š (æ›¿ä»£HDBSCAN)")
    lines.append("="*70)
    lines.append("")

    lines.append("ã€èšç±»æ¦‚å†µã€‘")
    lines.append(f"  ç®—æ³•: K-Means")
    lines.append(f"  èšç±»æ•°(K): {k}")
    lines.append(f"  æ€»æ ·æœ¬æ•°: {len(cluster_ids):,}")
    lines.append(f"  å™ªéŸ³ç‚¹æ•°: 0 (K-Meansæ— å™ªéŸ³ç‚¹)")
    lines.append("")

    lines.append("ã€èšç±»å¤§å°åˆ†å¸ƒã€‘")
    sizes = [info['size'] for info in cluster_info.values()]
    lines.append(f"  æœ€å°: {min(sizes)}")
    lines.append(f"  æœ€å¤§: {max(sizes)}")
    lines.append(f"  å¹³å‡: {sum(sizes)/len(sizes):.1f}")
    lines.append(f"  ä¸­ä½æ•°: {sorted(sizes)[len(sizes)//2]}")
    lines.append("")

    lines.append("ã€Kå€¼è¯„ä¼°ç»“æœã€‘")
    lines.append(f"{'Kå€¼':<10} {'è½®å»“ç³»æ•°':<12} {'DBæŒ‡æ•°':<12} {'CHæŒ‡æ•°':<12}")
    lines.append("-" * 70)
    for r in eval_results:
        lines.append(f"{r['k']:<10} {r['silhouette']:<12.4f} {r['davies_bouldin']:<12.4f} {r['calinski_harabasz']:<12.2f}")
    lines.append("")

    lines.append("ã€Top 20 æœ€å¤§èšç±»ã€‘")
    sorted_clusters = sorted(cluster_info.items(), key=lambda x: x[1]['size'], reverse=True)
    lines.append(f"{'æ’å':<5} {'èšç±»ID':<10} {'å¤§å°':<8} {'é¢‘æ¬¡æ€»å’Œ':<12} {'ç¤ºä¾‹çŸ­è¯­'}")
    lines.append("-" * 70)

    for rank, (cluster_id, info) in enumerate(sorted_clusters[:20], 1):
        examples = ', '.join(info['example_phrases'][:3])
        if len(examples) > 40:
            examples = examples[:37] + '...'
        lines.append(f"{rank:<5} {cluster_id:<10} {info['size']:<8} {info['total_frequency']:<12} {examples}")

    lines.append("")
    lines.append("="*70)
    lines.append("è¯´æ˜:")
    lines.append("  - è½®å»“ç³»æ•°: è¶Šæ¥è¿‘1è¶Šå¥½ï¼Œ>0.2ä¸ºå¯æ¥å—")
    lines.append("  - DBæŒ‡æ•°: è¶Šå°è¶Šå¥½")
    lines.append("  - CHæŒ‡æ•°: è¶Šå¤§è¶Šå¥½")
    lines.append("")
    lines.append("ä¸‹ä¸€æ­¥: è¿è¡Œ Phase 3 ç”Ÿæˆå¤§ç»„ç­›é€‰æŠ¥å‘Š")
    lines.append("="*70)

    report_text = '\n'.join(lines)
    print(report_text)

    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("K-Meansèšç±» (æ›¿ä»£HDBSCAN)".center(70))
    print("="*70)

    round_id = 1

    # 1. åŠ è½½æ•°æ®
    print("\nã€æ­¥éª¤1ã€‘åŠ è½½æ•°æ®...")
    embeddings, phrases = load_embeddings_and_phrases(round_id)

    # 2. å½’ä¸€åŒ–
    print("\nã€æ­¥éª¤2ã€‘å½’ä¸€åŒ–å‘é‡...")
    embeddings_norm = normalize(embeddings, norm='l2')

    # 3. å¯»æ‰¾æœ€ä¼˜K
    k_values = [60, 70, 80, 90, 100]
    optimal_k, eval_results = find_optimal_k(embeddings_norm, k_values, sample_size=10000)

    # 4. æ‰§è¡Œèšç±»
    cluster_ids, kmeans = run_kmeans_clustering(optimal_k, embeddings_norm, use_minibatch=True)

    # 5. æ›´æ–°æ•°æ®åº“
    cluster_info = update_database(cluster_ids, phrases, round_id)

    # 6. ç”ŸæˆæŠ¥å‘Š
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / f'phase2_kmeans_clustering_report_round{round_id}.txt'
    generate_report(optimal_k, cluster_ids, cluster_info, eval_results, report_file)

    print("\n" + "="*70)
    print("âœ… K-Meansèšç±»å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š å¯¹æ¯”HDBSCAN:")
    print("  HDBSCAN: 2ä¸ªèšç±», 58%å™ªéŸ³, 6å°æ—¶+")
    print(f"  K-Means: {len(cluster_info)}ä¸ªèšç±», 0%å™ªéŸ³, <1å°æ—¶")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ Phase 3: python scripts/run_phase3_selection.py")


if __name__ == "__main__":
    main()
