"""
æ·±å…¥åˆ†æHDBSCANèšç±»æ•ˆæœå·®çš„æ ¹æœ¬åŸå› 

åˆ†æå†…å®¹ï¼š
1. Embeddingså‘é‡åˆ†å¸ƒç‰¹å¾
2. è·ç¦»ç»Ÿè®¡ï¼ˆæœ€è¿‘é‚»ã€ä¸­ä½æ•°ã€æ–¹å·®ç­‰ï¼‰
3. å¯†åº¦åˆ†æï¼ˆå±€éƒ¨å¯†åº¦åˆ†å¸ƒï¼‰
4. é™ç»´å¯è§†åŒ–åˆ†æ
5. HDBSCANå‚æ•°æ•æ„Ÿæ€§åˆ†æ
6. ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”ï¼ˆK-Means, GMMï¼‰
"""
import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import pdist, squareform
from scipy.stats import describe
import hdbscan
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ========== ç¼–ç ä¿®å¤ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰ï¼‰==========
from utils.encoding_fix import setup_encoding
setup_encoding()
# ======================================================

from config.settings import CACHE_DIR, OUTPUT_DIR

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆWindowsï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


def load_embeddings(round_id=1):
    """åŠ è½½embeddingsæ•°æ®"""
    cache_file = CACHE_DIR / f"embeddings_round{round_id}.npz"
    print(f"\nåŠ è½½embeddings: {cache_file}")

    if not cache_file.exists():
        raise FileNotFoundError(f"Embeddingsç¼“å­˜ä¸å­˜åœ¨: {cache_file}")

    data = np.load(cache_file, allow_pickle=True)
    cache_dict = data['cache'].item()  # ä»å­—å…¸æ ¼å¼æå–

    print(f"  ç¼“å­˜ä¸­çš„embeddingæ•°é‡: {len(cache_dict)}")

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    embeddings_list = list(cache_dict.values())
    embeddings = np.array(embeddings_list)

    print(f"  Embeddingså½¢çŠ¶: {embeddings.shape}")
    print(f"  å‘é‡ç»´åº¦: {embeddings.shape[1]}")

    return embeddings


def analyze_vector_distribution(embeddings):
    """åˆ†æå‘é‡åˆ†å¸ƒç‰¹å¾"""
    print("\n" + "="*70)
    print("1. å‘é‡åˆ†å¸ƒåˆ†æ")
    print("="*70)

    # åŸºæœ¬ç»Ÿè®¡
    print("\nã€å‘é‡ç»Ÿè®¡ã€‘")
    print(f"  å¹³å‡èŒƒæ•°: {np.linalg.norm(embeddings, axis=1).mean():.4f}")
    print(f"  èŒƒæ•°æ ‡å‡†å·®: {np.linalg.norm(embeddings, axis=1).std():.4f}")
    print(f"  èŒƒæ•°æœ€å°å€¼: {np.linalg.norm(embeddings, axis=1).min():.4f}")
    print(f"  èŒƒæ•°æœ€å¤§å€¼: {np.linalg.norm(embeddings, axis=1).max():.4f}")

    # å½’ä¸€åŒ–å‘é‡
    embeddings_norm = normalize(embeddings, norm='l2')

    # è®¡ç®—æ‰€æœ‰ç»´åº¦çš„ç»Ÿè®¡
    print("\nã€å„ç»´åº¦ç»Ÿè®¡ã€‘")
    dim_stats = {
        'mean': embeddings_norm.mean(axis=0),
        'std': embeddings_norm.std(axis=0),
        'min': embeddings_norm.min(axis=0),
        'max': embeddings_norm.max(axis=0)
    }

    print(f"  å¹³å‡å€¼çš„å‡å€¼: {dim_stats['mean'].mean():.4f}")
    print(f"  å¹³å‡å€¼çš„æ ‡å‡†å·®: {dim_stats['mean'].std():.4f}")
    print(f"  æ ‡å‡†å·®çš„å‡å€¼: {dim_stats['std'].mean():.4f}")
    print(f"  æ ‡å‡†å·®çš„æ ‡å‡†å·®: {dim_stats['std'].std():.4f}")

    return embeddings_norm, dim_stats


def analyze_distance_distribution(embeddings_norm, sample_size=10000):
    """åˆ†æè·ç¦»åˆ†å¸ƒï¼ˆä½¿ç”¨é‡‡æ ·ä»¥èŠ‚çœæ—¶é—´ï¼‰"""
    print("\n" + "="*70)
    print("2. è·ç¦»åˆ†å¸ƒåˆ†æ")
    print("="*70)

    n_samples = len(embeddings_norm)

    # é‡‡æ ·åˆ†æï¼ˆå¦‚æœæ•°æ®å¤ªå¤§ï¼‰
    if n_samples > sample_size:
        print(f"\nâš ï¸  æ•°æ®é‡è¾ƒå¤§ï¼Œé‡‡æ · {sample_size} ä¸ªæ ·æœ¬è¿›è¡Œè·ç¦»åˆ†æ")
        indices = np.random.choice(n_samples, sample_size, replace=False)
        sample_embeddings = embeddings_norm[indices]
    else:
        sample_embeddings = embeddings_norm

    # è®¡ç®—æˆå¯¹è·ç¦»ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œå½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦è·ç¦»ï¼‰
    print("\nè®¡ç®—æˆå¯¹è·ç¦»...")
    distances = pdist(sample_embeddings, metric='euclidean')

    print("\nã€è·ç¦»ç»Ÿè®¡ã€‘")
    dist_stats = describe(distances)
    print(f"  æ ·æœ¬æ•°: {dist_stats.nobs:,}")
    print(f"  æœ€å°è·ç¦»: {dist_stats.minmax[0]:.4f}")
    print(f"  æœ€å¤§è·ç¦»: {dist_stats.minmax[1]:.4f}")
    print(f"  å¹³å‡è·ç¦»: {dist_stats.mean:.4f}")
    print(f"  æ–¹å·®: {dist_stats.variance:.4f}")
    print(f"  æ ‡å‡†å·®: {np.sqrt(dist_stats.variance):.4f}")
    print(f"  ååº¦: {dist_stats.skewness:.4f}")
    print(f"  å³°åº¦: {dist_stats.kurtosis:.4f}")

    # è·ç¦»åˆ†ä½æ•°
    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
    print("\nã€è·ç¦»åˆ†ä½æ•°ã€‘")
    for p in percentiles:
        val = np.percentile(distances, p)
        print(f"  {p:2d}%: {val:.4f}")

    return distances, dist_stats


def analyze_nearest_neighbors(embeddings_norm, k_values=[5, 10, 20, 50]):
    """åˆ†ææœ€è¿‘é‚»è·ç¦»åˆ†å¸ƒ"""
    print("\n" + "="*70)
    print("3. æœ€è¿‘é‚»è·ç¦»åˆ†æ")
    print("="*70)

    results = {}

    for k in k_values:
        print(f"\nã€K={k} æœ€è¿‘é‚»ã€‘")

        # è®¡ç®—Kæœ€è¿‘é‚»
        nbrs = NearestNeighbors(n_neighbors=k+1, metric='euclidean', n_jobs=-1)
        nbrs.fit(embeddings_norm)
        distances, indices = nbrs.kneighbors(embeddings_norm)

        # æ’é™¤è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªé‚»å±…ï¼‰
        knn_distances = distances[:, 1:]

        # ç»Ÿè®¡
        mean_dist = knn_distances.mean(axis=1)
        max_dist = knn_distances.max(axis=1)

        print(f"  å¹³å‡Kè¿‘é‚»è·ç¦»çš„å‡å€¼: {mean_dist.mean():.4f}")
        print(f"  å¹³å‡Kè¿‘é‚»è·ç¦»çš„æ ‡å‡†å·®: {mean_dist.std():.4f}")
        print(f"  å¹³å‡Kè¿‘é‚»è·ç¦»çš„ä¸­ä½æ•°: {np.median(mean_dist):.4f}")
        print(f"  æœ€è¿œKè¿‘é‚»è·ç¦»çš„å‡å€¼: {max_dist.mean():.4f}")
        print(f"  æœ€è¿œKè¿‘é‚»è·ç¦»çš„æ ‡å‡†å·®: {max_dist.std():.4f}")

        results[k] = {
            'mean_dist': mean_dist,
            'max_dist': max_dist,
            'knn_distances': knn_distances
        }

    return results


def analyze_density_distribution(embeddings_norm, k=20):
    """åˆ†æå¯†åº¦åˆ†å¸ƒ"""
    print("\n" + "="*70)
    print("4. å¯†åº¦åˆ†å¸ƒåˆ†æ")
    print("="*70)

    print(f"\nä½¿ç”¨ K={k} ä¼°è®¡å±€éƒ¨å¯†åº¦...")

    # ä½¿ç”¨Kè¿‘é‚»è·ç¦»ä¼°è®¡å¯†åº¦
    nbrs = NearestNeighbors(n_neighbors=k+1, metric='euclidean', n_jobs=-1)
    nbrs.fit(embeddings_norm)
    distances, _ = nbrs.kneighbors(embeddings_norm)

    # å¯†åº¦ = 1 / Kè¿‘é‚»å¹³å‡è·ç¦»
    knn_mean_dist = distances[:, 1:].mean(axis=1)
    density = 1.0 / (knn_mean_dist + 1e-10)

    print("\nã€å¯†åº¦ç»Ÿè®¡ã€‘")
    print(f"  æœ€å°å¯†åº¦: {density.min():.4f}")
    print(f"  æœ€å¤§å¯†åº¦: {density.max():.4f}")
    print(f"  å¹³å‡å¯†åº¦: {density.mean():.4f}")
    print(f"  å¯†åº¦æ ‡å‡†å·®: {density.std():.4f}")
    print(f"  å¯†åº¦ä¸­ä½æ•°: {np.median(density):.4f}")

    # å¯†åº¦åˆ†ä½æ•°
    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
    print("\nã€å¯†åº¦åˆ†ä½æ•°ã€‘")
    for p in percentiles:
        val = np.percentile(density, p)
        print(f"  {p:2d}%: {val:.4f}")

    # é«˜å¯†åº¦åŒºåŸŸç»Ÿè®¡
    high_density_threshold = np.percentile(density, 90)
    high_density_count = (density >= high_density_threshold).sum()
    print(f"\nã€é«˜å¯†åº¦åŒºåŸŸã€‘ï¼ˆ> 90åˆ†ä½æ•°ï¼‰")
    print(f"  é«˜å¯†åº¦ç‚¹æ•°: {high_density_count:,} ({high_density_count/len(density)*100:.2f}%)")

    # ä½å¯†åº¦åŒºåŸŸç»Ÿè®¡
    low_density_threshold = np.percentile(density, 10)
    low_density_count = (density <= low_density_threshold).sum()
    print(f"\nã€ä½å¯†åº¦åŒºåŸŸã€‘ï¼ˆ< 10åˆ†ä½æ•°ï¼‰")
    print(f"  ä½å¯†åº¦ç‚¹æ•°: {low_density_count:,} ({low_density_count/len(density)*100:.2f}%)")

    return density, knn_mean_dist


def perform_pca_analysis(embeddings_norm, n_components=50):
    """PCAé™ç»´åˆ†æ"""
    print("\n" + "="*70)
    print("5. PCAé™ç»´åˆ†æ")
    print("="*70)

    print(f"\næ‰§è¡ŒPCAé™ç»´åˆ° {n_components} ç»´...")
    pca = PCA(n_components=n_components)
    embeddings_pca = pca.fit_transform(embeddings_norm)

    # æ–¹å·®è§£é‡Šç‡
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)

    print("\nã€ä¸»æˆåˆ†æ–¹å·®è§£é‡Šç‡ã€‘")
    print(f"  PC1: {explained_variance[0]*100:.2f}%")
    print(f"  PC2: {explained_variance[1]*100:.2f}%")
    print(f"  PC3: {explained_variance[2]*100:.2f}%")
    print(f"  å‰10ä¸ªPCç´¯è®¡: {cumulative_variance[9]*100:.2f}%")
    print(f"  å‰20ä¸ªPCç´¯è®¡: {cumulative_variance[19]*100:.2f}%")
    print(f"  å‰50ä¸ªPCç´¯è®¡: {cumulative_variance[-1]*100:.2f}%")

    # æ‰¾åˆ°ç´¯è®¡æ–¹å·®è¾¾åˆ°90%éœ€è¦çš„ç»´åº¦
    n_dims_90 = np.argmax(cumulative_variance >= 0.9) + 1
    n_dims_95 = np.argmax(cumulative_variance >= 0.95) + 1
    print(f"\n  è¾¾åˆ°90%æ–¹å·®éœ€è¦: {n_dims_90} ç»´")
    print(f"  è¾¾åˆ°95%æ–¹å·®éœ€è¦: {n_dims_95} ç»´")

    return embeddings_pca, pca, explained_variance


def test_hdbscan_parameters(embeddings_norm, param_grid):
    """æµ‹è¯•ä¸åŒçš„HDBSCANå‚æ•°"""
    print("\n" + "="*70)
    print("6. HDBSCANå‚æ•°æ•æ„Ÿæ€§åˆ†æ")
    print("="*70)

    results = []

    for params in param_grid:
        min_cluster_size = params['min_cluster_size']
        min_samples = params['min_samples']

        print(f"\næµ‹è¯•å‚æ•°: min_cluster_size={min_cluster_size}, min_samples={min_samples}")

        try:
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric='euclidean',
                cluster_selection_method='eom',
                core_dist_n_jobs=-1
            )

            labels = clusterer.fit_predict(embeddings_norm)

            # ç»Ÿè®¡
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = (labels == -1).sum()
            noise_ratio = n_noise / len(labels) * 100

            # èšç±»å¤§å°
            cluster_sizes = []
            for label in set(labels):
                if label != -1:
                    cluster_sizes.append((labels == label).sum())

            # è½®å»“ç³»æ•°
            silhouette = None
            if n_clusters > 1 and n_noise < len(labels):
                mask = labels != -1
                if mask.sum() > 1:
                    try:
                        silhouette = silhouette_score(embeddings_norm[mask], labels[mask])
                    except:
                        pass

            result = {
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples,
                'n_clusters': n_clusters,
                'n_noise': n_noise,
                'noise_ratio': noise_ratio,
                'min_size': min(cluster_sizes) if cluster_sizes else 0,
                'max_size': max(cluster_sizes) if cluster_sizes else 0,
                'mean_size': np.mean(cluster_sizes) if cluster_sizes else 0,
                'silhouette': silhouette
            }

            results.append(result)

            print(f"  èšç±»æ•°: {n_clusters}, å™ªéŸ³: {n_noise} ({noise_ratio:.1f}%), "
                  f"è½®å»“ç³»æ•°: {silhouette:.3f if silhouette else 'N/A'}")
            if cluster_sizes:
                print(f"  èšç±»å¤§å°: æœ€å°={min(cluster_sizes)}, æœ€å¤§={max(cluster_sizes)}, "
                      f"å¹³å‡={np.mean(cluster_sizes):.1f}")

        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            continue

    return results


def compare_clustering_algorithms(embeddings_norm, sample_size=10000):
    """å¯¹æ¯”ä¸åŒèšç±»ç®—æ³•"""
    print("\n" + "="*70)
    print("7. èšç±»ç®—æ³•å¯¹æ¯”åˆ†æ")
    print("="*70)

    # å¦‚æœæ•°æ®å¤ªå¤§ï¼Œé‡‡æ ·
    if len(embeddings_norm) > sample_size:
        print(f"\nâš ï¸  é‡‡æ · {sample_size} ä¸ªæ ·æœ¬è¿›è¡Œç®—æ³•å¯¹æ¯”")
        indices = np.random.choice(len(embeddings_norm), sample_size, replace=False)
        sample_embeddings = embeddings_norm[indices]
    else:
        sample_embeddings = embeddings_norm

    results = {}

    # K-Means (æµ‹è¯•ä¸åŒKå€¼)
    print("\nã€K-Meansã€‘")
    from sklearn.cluster import KMeans

    for k in [50, 75, 100, 150]:
        print(f"\n  K={k}:")
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(sample_embeddings)

        silhouette = silhouette_score(sample_embeddings, labels)
        davies_bouldin = davies_bouldin_score(sample_embeddings, labels)
        calinski_harabasz = calinski_harabasz_score(sample_embeddings, labels)

        print(f"    è½®å»“ç³»æ•°: {silhouette:.4f}")
        print(f"    Davies-Bouldin: {davies_bouldin:.4f} (è¶Šå°è¶Šå¥½)")
        print(f"    Calinski-Harabasz: {calinski_harabasz:.2f} (è¶Šå¤§è¶Šå¥½)")

        results[f'kmeans_k{k}'] = {
            'n_clusters': k,
            'silhouette': silhouette,
            'davies_bouldin': davies_bouldin,
            'calinski_harabasz': calinski_harabasz
        }

    # GMM (æµ‹è¯•ä¸åŒæˆåˆ†æ•°)
    print("\nã€é«˜æ–¯æ··åˆæ¨¡å‹ GMMã€‘")
    from sklearn.mixture import GaussianMixture

    for n_components in [50, 75, 100]:
        print(f"\n  n_components={n_components}:")
        gmm = GaussianMixture(n_components=n_components, random_state=42, covariance_type='diag')
        labels = gmm.fit_predict(sample_embeddings)

        silhouette = silhouette_score(sample_embeddings, labels)
        davies_bouldin = davies_bouldin_score(sample_embeddings, labels)

        print(f"    è½®å»“ç³»æ•°: {silhouette:.4f}")
        print(f"    Davies-Bouldin: {davies_bouldin:.4f}")
        print(f"    BIC: {gmm.bic(sample_embeddings):.2f} (è¶Šå°è¶Šå¥½)")
        print(f"    AIC: {gmm.aic(sample_embeddings):.2f} (è¶Šå°è¶Šå¥½)")

        results[f'gmm_{n_components}'] = {
            'n_clusters': n_components,
            'silhouette': silhouette,
            'davies_bouldin': davies_bouldin,
            'bic': gmm.bic(sample_embeddings),
            'aic': gmm.aic(sample_embeddings)
        }

    return results


def generate_visualizations(embeddings_norm, embeddings_pca, density, distances, output_dir):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    print("\n" + "="*70)
    print("8. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print("="*70)

    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)

    # 1. è·ç¦»åˆ†å¸ƒç›´æ–¹å›¾
    print("\nç”Ÿæˆè·ç¦»åˆ†å¸ƒå›¾...")
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(distances, bins=100, alpha=0.7, edgecolor='black')
    ax.set_xlabel('æ¬§æ°è·ç¦»ï¼ˆå½’ä¸€åŒ–åï¼‰')
    ax.set_ylabel('é¢‘æ¬¡')
    ax.set_title('æˆå¯¹è·ç¦»åˆ†å¸ƒ')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'distance_distribution.png', dpi=150)
    plt.close()

    # 2. å¯†åº¦åˆ†å¸ƒç›´æ–¹å›¾
    print("ç”Ÿæˆå¯†åº¦åˆ†å¸ƒå›¾...")
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(density, bins=100, alpha=0.7, edgecolor='black')
    ax.set_xlabel('å±€éƒ¨å¯†åº¦ï¼ˆ1/Kè¿‘é‚»è·ç¦»ï¼‰')
    ax.set_ylabel('é¢‘æ¬¡')
    ax.set_title('å±€éƒ¨å¯†åº¦åˆ†å¸ƒ')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'density_distribution.png', dpi=150)
    plt.close()

    # 3. PCA 2DæŠ•å½±ï¼ˆé‡‡æ ·ï¼‰
    print("ç”ŸæˆPCA 2DæŠ•å½±å›¾...")
    sample_size = min(5000, len(embeddings_pca))
    indices = np.random.choice(len(embeddings_pca), sample_size, replace=False)

    fig, ax = plt.subplots(figsize=(12, 10))
    scatter = ax.scatter(
        embeddings_pca[indices, 0],
        embeddings_pca[indices, 1],
        c=density[indices],
        cmap='viridis',
        alpha=0.5,
        s=10
    )
    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_title(f'PCA 2DæŠ•å½±ï¼ˆæŒ‰å¯†åº¦ç€è‰²ï¼Œé‡‡æ ·{sample_size}ä¸ªç‚¹ï¼‰')
    plt.colorbar(scatter, label='å±€éƒ¨å¯†åº¦')
    plt.tight_layout()
    plt.savefig(output_dir / 'pca_2d_projection.png', dpi=150)
    plt.close()

    print(f"\nâœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("HDBSCANèšç±»é—®é¢˜æ·±åº¦åˆ†æ".center(70))
    print("="*70)

    # 1. åŠ è½½æ•°æ®
    embeddings = load_embeddings(round_id=1)

    # 2. å‘é‡åˆ†å¸ƒåˆ†æ
    embeddings_norm, dim_stats = analyze_vector_distribution(embeddings)

    # 3. è·ç¦»åˆ†å¸ƒåˆ†æ
    distances, dist_stats = analyze_distance_distribution(embeddings_norm, sample_size=10000)

    # 4. æœ€è¿‘é‚»åˆ†æ
    knn_results = analyze_nearest_neighbors(embeddings_norm, k_values=[5, 10, 20, 50])

    # 5. å¯†åº¦åˆ†æ
    density, knn_mean_dist = analyze_density_distribution(embeddings_norm, k=20)

    # 6. PCAåˆ†æ
    embeddings_pca, pca, explained_variance = perform_pca_analysis(embeddings_norm, n_components=50)

    # 7. HDBSCANå‚æ•°æµ‹è¯•
    param_grid = [
        {'min_cluster_size': 10, 'min_samples': 1},
        {'min_cluster_size': 20, 'min_samples': 2},
        {'min_cluster_size': 30, 'min_samples': 3},
        {'min_cluster_size': 50, 'min_samples': 5},
        {'min_cluster_size': 100, 'min_samples': 10},
        {'min_cluster_size': 200, 'min_samples': 20},
        {'min_cluster_size': 500, 'min_samples': 50},
        {'min_cluster_size': 1000, 'min_samples': 100},
    ]
    hdbscan_results = test_hdbscan_parameters(embeddings_norm, param_grid)

    # 8. ç®—æ³•å¯¹æ¯”
    algo_results = compare_clustering_algorithms(embeddings_norm, sample_size=10000)

    # 9. ç”Ÿæˆå¯è§†åŒ–
    output_viz_dir = OUTPUT_DIR / 'clustering_analysis'
    generate_visualizations(embeddings_norm, embeddings_pca, density, distances, output_viz_dir)

    # 10. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
    print("="*70)

    report_lines = []
    report_lines.append("="*70)
    report_lines.append("HDBSCANèšç±»é—®é¢˜æ·±åº¦åˆ†ææŠ¥å‘Š")
    report_lines.append("="*70)
    report_lines.append("")

    # æ•°æ®æ¦‚å†µ
    report_lines.append("ã€æ•°æ®æ¦‚å†µã€‘")
    report_lines.append(f"  çŸ­è¯­æ•°é‡: {len(embeddings):,}")
    report_lines.append(f"  å‘é‡ç»´åº¦: {embeddings.shape[1]}")
    report_lines.append("")

    # è·ç¦»ç»Ÿè®¡
    report_lines.append("ã€è·ç¦»åˆ†å¸ƒã€‘")
    report_lines.append(f"  å¹³å‡è·ç¦»: {dist_stats.mean:.4f}")
    report_lines.append(f"  æ ‡å‡†å·®: {np.sqrt(dist_stats.variance):.4f}")
    report_lines.append(f"  æœ€å°è·ç¦»: {dist_stats.minmax[0]:.4f}")
    report_lines.append(f"  æœ€å¤§è·ç¦»: {dist_stats.minmax[1]:.4f}")
    report_lines.append(f"  ä¸­ä½æ•°: {np.median(distances):.4f}")
    report_lines.append("")

    # å¯†åº¦ç»Ÿè®¡
    report_lines.append("ã€å¯†åº¦åˆ†å¸ƒã€‘")
    report_lines.append(f"  å¹³å‡å¯†åº¦: {density.mean():.4f}")
    report_lines.append(f"  å¯†åº¦æ ‡å‡†å·®: {density.std():.4f}")
    report_lines.append(f"  å¯†åº¦ä¸­ä½æ•°: {np.median(density):.4f}")
    report_lines.append("")

    # PCAåˆ†æ
    report_lines.append("ã€PCAé™ç»´ã€‘")
    report_lines.append(f"  å‰10ä¸ªPCç´¯è®¡æ–¹å·®: {np.cumsum(explained_variance)[9]*100:.2f}%")
    report_lines.append(f"  å‰20ä¸ªPCç´¯è®¡æ–¹å·®: {np.cumsum(explained_variance)[19]*100:.2f}%")
    report_lines.append(f"  å‰50ä¸ªPCç´¯è®¡æ–¹å·®: {np.cumsum(explained_variance)[-1]*100:.2f}%")
    report_lines.append("")

    # HDBSCANå‚æ•°æµ‹è¯•ç»“æœ
    report_lines.append("ã€HDBSCANå‚æ•°æµ‹è¯•ã€‘")
    report_lines.append(f"{'min_size':<10} {'min_samp':<10} {'n_clust':<10} {'å™ªéŸ³ç‡%':<10} {'è½®å»“ç³»æ•°':<12}")
    report_lines.append("-" * 70)
    for r in hdbscan_results:
        silh_str = f"{r['silhouette']:.4f}" if r['silhouette'] else "N/A"
        report_lines.append(
            f"{r['min_cluster_size']:<10} {r['min_samples']:<10} "
            f"{r['n_clusters']:<10} {r['noise_ratio']:<10.1f} {silh_str:<12}"
        )
    report_lines.append("")

    # ç®—æ³•å¯¹æ¯”
    report_lines.append("ã€èšç±»ç®—æ³•å¯¹æ¯”ã€‘")
    report_lines.append(f"{'ç®—æ³•':<20} {'èšç±»æ•°':<10} {'è½®å»“ç³»æ•°':<12} {'Davies-Bouldin':<16}")
    report_lines.append("-" * 70)
    for algo_name, r in algo_results.items():
        if 'davies_bouldin' in r:
            report_lines.append(
                f"{algo_name:<20} {r['n_clusters']:<10} "
                f"{r['silhouette']:<12.4f} {r['davies_bouldin']:<16.4f}"
            )
    report_lines.append("")

    # é—®é¢˜è¯Šæ–­
    report_lines.append("="*70)
    report_lines.append("ã€é—®é¢˜è¯Šæ–­ã€‘")
    report_lines.append("="*70)
    report_lines.append("")

    # æ ¹æ®åˆ†æç»“æœç»™å‡ºè¯Šæ–­
    avg_dist = dist_stats.mean
    std_dist = np.sqrt(dist_stats.variance)

    report_lines.append("1. æ•°æ®åˆ†å¸ƒç‰¹å¾ï¼š")
    if avg_dist > 1.2:
        report_lines.append(f"   âš ï¸  å¹³å‡è·ç¦»è¿‡å¤§({avg_dist:.4f})ï¼Œæ•°æ®ç‚¹åˆ†æ•£")
    if std_dist < 0.1:
        report_lines.append(f"   âš ï¸  è·ç¦»æ ‡å‡†å·®è¿‡å°({std_dist:.4f})ï¼Œç¼ºä¹æ˜æ˜¾èšé›†")

    report_lines.append("")
    report_lines.append("2. HDBSCANé€‚ç”¨æ€§ï¼š")
    best_hdbscan = max(hdbscan_results, key=lambda x: x['n_clusters'])
    if best_hdbscan['n_clusters'] < 10:
        report_lines.append(f"   âŒ HDBSCANæœ€å¤šåªèƒ½ç”Ÿæˆ{best_hdbscan['n_clusters']}ä¸ªèšç±»")
        report_lines.append("   åŸå› ï¼šæ•°æ®ç¼ºä¹æ˜æ˜¾çš„å¯†åº¦åˆ†éš”ï¼ŒHDBSCANæ— æ³•æ‰¾åˆ°è¶³å¤Ÿå¤šçš„å¯†é›†åŒºåŸŸ")

    report_lines.append("")
    report_lines.append("3. æ¨èæ–¹æ¡ˆï¼š")

    # æ‰¾æœ€ä½³K-Meansç»“æœ
    best_kmeans = max(
        [(k, v) for k, v in algo_results.items() if k.startswith('kmeans')],
        key=lambda x: x[1]['silhouette']
    )

    report_lines.append(f"   æ¨èç®—æ³•: K-Means")
    report_lines.append(f"   æ¨èKå€¼: {best_kmeans[1]['n_clusters']}")
    report_lines.append(f"   é¢„æœŸè½®å»“ç³»æ•°: {best_kmeans[1]['silhouette']:.4f}")
    report_lines.append("")

    report_lines.append("="*70)

    # è¾“å‡ºå¹¶ä¿å­˜æŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    report_file = OUTPUT_DIR / 'clustering_analysis_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_viz_dir}")

    print("\n" + "="*70)
    print("åˆ†æå®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
