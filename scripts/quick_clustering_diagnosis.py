"""
å¿«é€Ÿè¯Šæ–­HDBSCANèšç±»é—®é¢˜ - ç®€åŒ–ç‰ˆ
åªåšå…³é”®åˆ†æï¼Œæé«˜é€Ÿåº¦
"""
import sys
from pathlib import Path
import numpy as np
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import pdist
from scipy.stats import describe
import hdbscan

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ç¼–ç ä¿®å¤
from utils.encoding_fix import setup_encoding
setup_encoding()

from config.settings import CACHE_DIR, OUTPUT_DIR


def load_embeddings(round_id=1):
    """åŠ è½½embeddingsæ•°æ®"""
    cache_file = CACHE_DIR / f"embeddings_round{round_id}.npz"
    print(f"\nåŠ è½½embeddings: {cache_file}")

    data = np.load(cache_file, allow_pickle=True)
    cache_dict = data['cache'].item()

    print(f"  ç¼“å­˜embeddingæ•°é‡: {len(cache_dict)}")

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    embeddings_list = list(cache_dict.values())
    embeddings = np.array(embeddings_list)

    print(f"  Embeddingså½¢çŠ¶: {embeddings.shape}")

    return embeddings


def quick_diagnosis():
    """å¿«é€Ÿè¯Šæ–­"""
    print("\n" + "="*70)
    print("HDBSCANèšç±»é—®é¢˜å¿«é€Ÿè¯Šæ–­".center(70))
    print("="*70)

    # 1. åŠ è½½æ•°æ®
    print("\nã€æ­¥éª¤1ã€‘åŠ è½½æ•°æ®...")
    embeddings = load_embeddings(round_id=1)
    n_samples = len(embeddings)

    # 2. å½’ä¸€åŒ–
    print("\nã€æ­¥éª¤2ã€‘å½’ä¸€åŒ–å‘é‡...")
    embeddings_norm = normalize(embeddings, norm='l2')

    # 3. é‡‡æ ·åˆ†æè·ç¦»åˆ†å¸ƒ
    print("\nã€æ­¥éª¤3ã€‘åˆ†æè·ç¦»åˆ†å¸ƒï¼ˆé‡‡æ ·10000ä¸ªæ ·æœ¬ï¼‰...")
    sample_size = min(10000, n_samples)
    indices = np.random.choice(n_samples, sample_size, replace=False)
    sample_embeddings = embeddings_norm[indices]

    print(f"  è®¡ç®— {sample_size} ä¸ªæ ·æœ¬çš„æˆå¯¹è·ç¦»...")
    distances = pdist(sample_embeddings, metric='euclidean')

    dist_stats = describe(distances)
    print(f"\n  è·ç¦»ç»Ÿè®¡:")
    print(f"    æœ€å°è·ç¦»: {dist_stats.minmax[0]:.4f}")
    print(f"    æœ€å¤§è·ç¦»: {dist_stats.minmax[1]:.4f}")
    print(f"    å¹³å‡è·ç¦»: {dist_stats.mean:.4f}")
    print(f"    æ ‡å‡†å·®: {np.sqrt(dist_stats.variance):.4f}")
    print(f"    ä¸­ä½æ•°: {np.median(distances):.4f}")

    # å…³é”®è¯Šæ–­æŒ‡æ ‡
    print(f"\n  å…³é”®è¯Šæ–­:")
    if dist_stats.mean > 1.2:
        print(f"    âŒ å¹³å‡è·ç¦»è¿‡å¤§ ({dist_stats.mean:.4f})ï¼Œæ•°æ®ç‚¹ä¸¥é‡åˆ†æ•£")
    elif dist_stats.mean > 1.0:
        print(f"    âš ï¸  å¹³å‡è·ç¦»è¾ƒå¤§ ({dist_stats.mean:.4f})ï¼Œæ•°æ®ç‚¹åˆ†æ•£")
    else:
        print(f"    âœ“ å¹³å‡è·ç¦»æ­£å¸¸ ({dist_stats.mean:.4f})")

    if np.sqrt(dist_stats.variance) < 0.1:
        print(f"    âŒ è·ç¦»æ ‡å‡†å·®è¿‡å° ({np.sqrt(dist_stats.variance):.4f})ï¼Œç¼ºä¹æ˜æ˜¾èšé›†")
    elif np.sqrt(dist_stats.variance) < 0.2:
        print(f"    âš ï¸  è·ç¦»æ ‡å‡†å·®è¾ƒå° ({np.sqrt(dist_stats.variance):.4f})ï¼Œèšé›†ä¸æ˜æ˜¾")
    else:
        print(f"    âœ“ è·ç¦»æ ‡å‡†å·®æ­£å¸¸ ({np.sqrt(dist_stats.variance):.4f})")

    # 4. Kè¿‘é‚»å¯†åº¦åˆ†æ
    print("\nã€æ­¥éª¤4ã€‘åˆ†æå±€éƒ¨å¯†åº¦ï¼ˆK=20è¿‘é‚»ï¼‰...")
    nbrs = NearestNeighbors(n_neighbors=21, metric='euclidean', n_jobs=-1)
    nbrs.fit(embeddings_norm)
    distances_knn, _ = nbrs.kneighbors(embeddings_norm)

    knn_mean_dist = distances_knn[:, 1:].mean(axis=1)  # æ’é™¤è‡ªå·±
    density = 1.0 / (knn_mean_dist + 1e-10)

    print(f"\n  å¯†åº¦ç»Ÿè®¡:")
    print(f"    å¹³å‡å¯†åº¦: {density.mean():.4f}")
    print(f"    å¯†åº¦æ ‡å‡†å·®: {density.std():.4f}")
    print(f"    å¯†åº¦ä¸­ä½æ•°: {np.median(density):.4f}")
    print(f"    é«˜å¯†åº¦ç‚¹(>90%): {(density >= np.percentile(density, 90)).sum():,} ({(density >= np.percentile(density, 90)).sum()/n_samples*100:.1f}%)")

    # 5. å¿«é€Ÿæµ‹è¯•å‡ ä¸ªHDBSCANå‚æ•°
    print("\nã€æ­¥éª¤5ã€‘æµ‹è¯•HDBSCANå‚æ•°ï¼ˆå¿«é€Ÿç‰ˆï¼‰...")

    test_params = [
        {'min_cluster_size': 30, 'min_samples': 3},
        {'min_cluster_size': 50, 'min_samples': 5},
        {'min_cluster_size': 100, 'min_samples': 10},
        {'min_cluster_size': 200, 'min_samples': 20},
    ]

    results = []
    for params in test_params:
        print(f"\n  æµ‹è¯•: min_cluster_size={params['min_cluster_size']}, min_samples={params['min_samples']}")

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=params['min_cluster_size'],
            min_samples=params['min_samples'],
            metric='euclidean',
            cluster_selection_method='eom',
            core_dist_n_jobs=-1
        )

        labels = clusterer.fit_predict(embeddings_norm)

        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = (labels == -1).sum()
        noise_ratio = n_noise / n_samples * 100

        print(f"    èšç±»æ•°: {n_clusters}, å™ªéŸ³ç‚¹: {n_noise:,} ({noise_ratio:.1f}%)")

        results.append({
            'params': params,
            'n_clusters': n_clusters,
            'noise_ratio': noise_ratio
        })

    # 6. æµ‹è¯•K-Meanså¯¹æ¯”
    print("\nã€æ­¥éª¤6ã€‘å¯¹æ¯”K-Meansç®—æ³•ï¼ˆé‡‡æ ·10000ä¸ªæ ·æœ¬ï¼‰...")

    from sklearn.cluster import KMeans

    for k in [60, 80, 100]:
        print(f"\n  K-Means with K={k}:")
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=5)
        labels_km = kmeans.fit_predict(sample_embeddings)

        silhouette = silhouette_score(sample_embeddings, labels_km)
        print(f"    è½®å»“ç³»æ•°: {silhouette:.4f}")

    # 7. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
    print("\n" + "="*70)
    print("è¯Šæ–­æŠ¥å‘Š")
    print("="*70)

    print("\nã€é—®é¢˜è¯Šæ–­ã€‘")
    print(f"1. æ•°æ®è§„æ¨¡: {n_samples:,} ä¸ªçŸ­è¯­, {embeddings.shape[1]} ç»´å‘é‡")

    print(f"\n2. æ•°æ®åˆ†å¸ƒç‰¹å¾:")
    print(f"   - å¹³å‡è·ç¦»: {dist_stats.mean:.4f}")
    print(f"   - è·ç¦»æ ‡å‡†å·®: {np.sqrt(dist_stats.variance):.4f}")

    if dist_stats.mean > 1.2 or np.sqrt(dist_stats.variance) < 0.1:
        print(f"\n   âŒ æ•°æ®åˆ†å¸ƒé—®é¢˜ä¸¥é‡:")
        if dist_stats.mean > 1.2:
            print(f"      - æ•°æ®ç‚¹è¿‡äºåˆ†æ•£ï¼ˆå¹³å‡è·ç¦»={dist_stats.mean:.4f} > 1.2ï¼‰")
        if np.sqrt(dist_stats.variance) < 0.1:
            print(f"      - ç¼ºä¹æ˜æ˜¾çš„èšé›†åŒºåŸŸï¼ˆæ ‡å‡†å·®={np.sqrt(dist_stats.variance):.4f} < 0.1ï¼‰")

    print(f"\n3. HDBSCANè¡¨ç°:")
    best_result = max(results, key=lambda x: x['n_clusters'])
    print(f"   - æœ€å¥½æƒ…å†µ: {best_result['n_clusters']} ä¸ªèšç±» (å‚æ•°: {best_result['params']})")
    print(f"   - æœ€å¤šå™ªéŸ³: {max(r['noise_ratio'] for r in results):.1f}%")

    if best_result['n_clusters'] < 10:
        print(f"\n   âŒ HDBSCANä¸¥é‡å¤±æ•ˆ:")
        print(f"      - æœ€å¤šåªèƒ½ç”Ÿæˆ {best_result['n_clusters']} ä¸ªèšç±»ï¼ˆæœŸæœ›60-100ä¸ªï¼‰")
        print(f"      - æ ¹æœ¬åŸå› : æ•°æ®ç¼ºä¹è¶³å¤Ÿçš„å¯†åº¦åˆ†éš”")

    print(f"\nã€æ¨èæ–¹æ¡ˆã€‘")
    print(f"\nä¼˜å…ˆçº§1: åˆ‡æ¢åˆ°K-Meansç®—æ³•")
    print(f"  - åŸå› : HDBSCANä¾èµ–å¯†åº¦èšç±»ï¼Œå½“å‰æ•°æ®ç¼ºä¹æ˜æ˜¾å¯†åº¦åˆ†éš”")
    print(f"  - K-Meansæ˜¯åŸºäºè·ç¦»çš„èšç±»ï¼Œæ›´é€‚åˆå½“å‰æ•°æ®åˆ†å¸ƒ")
    print(f"  - å»ºè®®Kå€¼: 60-100")

    print(f"\nä¼˜å…ˆçº§2: æ”¹è¿›Embeddingè´¨é‡")
    print(f"  - è€ƒè™‘æ›´å¼ºå¤§çš„embeddingæ¨¡å‹ï¼ˆå¦‚multilingual-e5-largeï¼‰")
    print(f"  - æˆ–è€…ä½¿ç”¨é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒæ¨¡å‹")

    print(f"\nä¼˜å…ˆçº§3: é™ç»´+èšç±»")
    print(f"  - PCAé™ç»´åˆ°50-100ç»´åå†èšç±»")
    print(f"  - å¯èƒ½æé«˜èšç±»æ•ˆæœ")

    # ä¿å­˜æŠ¥å‘Š
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / 'clustering_quick_diagnosis.txt'

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("HDBSCANèšç±»é—®é¢˜å¿«é€Ÿè¯Šæ–­æŠ¥å‘Š\n")
        f.write("="*70 + "\n\n")
        f.write(f"æ•°æ®è§„æ¨¡: {n_samples:,} ä¸ªçŸ­è¯­\n")
        f.write(f"å‘é‡ç»´åº¦: {embeddings.shape[1]}\n\n")
        f.write(f"è·ç¦»ç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡è·ç¦»: {dist_stats.mean:.4f}\n")
        f.write(f"  è·ç¦»æ ‡å‡†å·®: {np.sqrt(dist_stats.variance):.4f}\n")
        f.write(f"  ä¸­ä½æ•°: {np.median(distances):.4f}\n\n")
        f.write(f"HDBSCANæµ‹è¯•ç»“æœ:\n")
        for r in results:
            f.write(f"  {r['params']}: {r['n_clusters']}ä¸ªèšç±», {r['noise_ratio']:.1f}%å™ªéŸ³\n")
        f.write(f"\næ¨èæ–¹æ¡ˆ: åˆ‡æ¢åˆ°K-Meansç®—æ³•ï¼ˆK=60-100ï¼‰\n")

    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    print("\n" + "="*70)
    print("å¿«é€Ÿè¯Šæ–­å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    quick_diagnosis()
