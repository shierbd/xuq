"""
Phase 2: å¤§ç»„èšç±»
å¯¹æ‰€æœ‰çŸ­è¯­è¿›è¡Œè¯­ä¹‰èšç±»ï¼Œç”Ÿæˆ60-100ä¸ªå¤§ç»„

è¿è¡Œæ–¹å¼ï¼š
    python scripts/run_phase2_clustering.py [é€‰é¡¹]

å‚æ•°ï¼š
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
    --limit: é™åˆ¶å¤„ç†çš„çŸ­è¯­æ•°é‡ï¼Œç”¨äºæµ‹è¯•ï¼ˆ0=å…¨éƒ¨ï¼‰
    --min-cluster-size: HDBSCANæœ€å°èšç±»å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
    --min-samples: HDBSCANæœ€å°æ ·æœ¬æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
    --force-recalculate: å¼ºåˆ¶é‡æ–°è®¡ç®—embeddingsï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰

ç¤ºä¾‹ï¼š
    # ä½¿ç”¨é»˜è®¤å‚æ•°
    python scripts/run_phase2_clustering.py

    # è‡ªå®šä¹‰èšç±»å‚æ•°
    python scripts/run_phase2_clustering.py --min-cluster-size=30 --min-samples=3

    # å¼ºåˆ¶é‡æ–°è®¡ç®—embeddings
    python scripts/run_phase2_clustering.py --force-recalculate

    # æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†100æ¡ï¼‰
    python scripts/run_phase2_clustering.py --limit=100
"""
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ========== ç¼–ç ä¿®å¤ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰ï¼‰==========
from utils.encoding_fix import setup_encoding
setup_encoding()
# ======================================================

from config.settings import OUTPUT_DIR
from core.embedding import EmbeddingService
from core.clustering import cluster_phrases_large
from storage.repository import PhraseRepository, ClusterMetaRepository
from storage.models import Phrase


def run_phase2_clustering(round_id: int = 1, limit: int = 0,
                          min_cluster_size: int = None,
                          min_samples: int = None,
                          force_recalculate: bool = False):
    """
    æ‰§è¡ŒPhase 2å¤§ç»„èšç±»

    Args:
        round_id: æ•°æ®è½®æ¬¡ID
        limit: é™åˆ¶å¤„ç†æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰
        min_cluster_size: HDBSCANæœ€å°èšç±»å¤§å°ï¼ˆNone=ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰
        min_samples: HDBSCANæœ€å°æ ·æœ¬æ•°ï¼ˆNone=ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰
        force_recalculate: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—embeddingsï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    """
    print("\n" + "="*70)
    print("Phase 2: å¤§ç»„èšç±»".center(70))
    print("="*70)

    # 1. ä»æ•°æ®åº“åŠ è½½çŸ­è¯­
    print("\nã€æ­¥éª¤1ã€‘ä»æ•°æ®åº“åŠ è½½çŸ­è¯­...")
    with PhraseRepository() as repo:
        # è·å–æœªå¤„ç†çš„çŸ­è¯­
        query = repo.session.query(Phrase).filter(
            Phrase.processed_status == 'unseen'
        )

        if limit > 0:
            query = query.limit(limit)
            print(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {limit} æ¡çŸ­è¯­")

        phrases_db = query.all()

        if not phrases_db:
            print("\nâŒ æ²¡æœ‰å¾…å¤„ç†çš„çŸ­è¯­ï¼")
            return False

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        phrases = [{
            'phrase_id': p.phrase_id,
            'phrase': p.phrase,
            'frequency': p.frequency,
            'volume': p.volume,
            'seed_word': p.seed_word,
            'source_type': p.source_type,
        } for p in phrases_db]

        print(f"âœ“ åŠ è½½äº† {len(phrases)} æ¡å¾…èšç±»çŸ­è¯­")

    # 2. è®¡ç®—Embeddings
    print("\nã€æ­¥éª¤2ã€‘è®¡ç®—Embeddings...")
    embedding_service = EmbeddingService(use_cache=not force_recalculate)
    if force_recalculate:
        print("âš ï¸  å¼ºåˆ¶é‡æ–°è®¡ç®—æ¨¡å¼ï¼šå¿½ç•¥embeddingsç¼“å­˜")
    embeddings, phrase_ids = embedding_service.embed_phrases_from_db(phrases, round_id)

    # éªŒè¯
    assert len(embeddings) == len(phrases), "Embeddingsæ•°é‡ä¸åŒ¹é…"
    assert len(phrase_ids) == len(phrases), "Phrase IDsæ•°é‡ä¸åŒ¹é…"

    # 3. æ‰§è¡Œå¤§ç»„èšç±»
    print("\nã€æ­¥éª¤3ã€‘æ‰§è¡Œå¤§ç»„èšç±»...")

    # å‡†å¤‡èšç±»é…ç½®ï¼ˆå¦‚æœæœ‰è‡ªå®šä¹‰å‚æ•°ï¼‰
    cluster_config = None
    if min_cluster_size is not None or min_samples is not None:
        from config.settings import LARGE_CLUSTER_CONFIG
        cluster_config = LARGE_CLUSTER_CONFIG.copy()
        if min_cluster_size is not None:
            cluster_config['min_cluster_size'] = min_cluster_size
            print(f"  è‡ªå®šä¹‰å‚æ•°: min_cluster_size={min_cluster_size}")
        if min_samples is not None:
            cluster_config['min_samples'] = min_samples
            print(f"  è‡ªå®šä¹‰å‚æ•°: min_samples={min_samples}")

    cluster_ids, cluster_info, clusterer = cluster_phrases_large(
        embeddings, phrases, config=cluster_config
    )

    # 4. æ›´æ–°æ•°æ®åº“
    print("\nã€æ­¥éª¤4ã€‘æ›´æ–°æ•°æ®åº“...")

    # 4.1 æ›´æ–°phrasesè¡¨çš„cluster_id_A
    print("\n  æ›´æ–°phrasesè¡¨...")
    with PhraseRepository() as repo:
        success_count = 0
        for i, phrase_id in enumerate(phrase_ids):
            cluster_id = int(cluster_ids[i])
            if repo.update_cluster_assignment(phrase_id, cluster_id_A=cluster_id):
                success_count += 1

        print(f"  âœ“ å·²æ›´æ–° {success_count}/{len(phrase_ids)} æ¡è®°å½•çš„cluster_id_A")

    # 4.2 ä¿å­˜cluster_metaè¡¨
    print("\n  ä¿å­˜èšç±»å…ƒæ•°æ®...")
    with ClusterMetaRepository() as repo:
        for cluster_id, info in cluster_info.items():
            # å‡†å¤‡ç¤ºä¾‹çŸ­è¯­ï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰
            example_phrases_str = '; '.join(info['example_phrases'])

            # åˆ›å»ºæˆ–æ›´æ–°èšç±»å…ƒæ•°æ®
            repo.create_or_update_cluster(
                cluster_id=cluster_id,
                cluster_level='A',
                size=info['size'],
                example_phrases=example_phrases_str,
                main_theme=None,  # Phase 3ä¼šç”¨AIç”Ÿæˆ
                total_frequency=info['total_frequency']
            )

        print(f"  âœ“ å·²ä¿å­˜ {len(cluster_info)} ä¸ªèšç±»çš„å…ƒæ•°æ®")

    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\nã€æ­¥éª¤5ã€‘ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("="*70)
    report_lines.append("Phase 2 å¤§ç»„èšç±»æŠ¥å‘Š")
    report_lines.append("="*70)
    report_lines.append("")

    report_lines.append("ã€èšç±»æ¦‚å†µã€‘")
    report_lines.append(f"  æ€»çŸ­è¯­æ•°: {len(phrases):,}")
    report_lines.append(f"  èšç±»æ•°é‡: {len(cluster_info)}")
    noise_count = (cluster_ids == -1).sum()
    report_lines.append(f"  å™ªéŸ³ç‚¹æ•°: {noise_count} ({noise_count/len(phrases)*100:.1f}%)")
    report_lines.append("")

    # èšç±»å¤§å°åˆ†å¸ƒ
    sizes = [info['size'] for info in cluster_info.values()]
    report_lines.append("ã€èšç±»å¤§å°åˆ†å¸ƒã€‘")
    report_lines.append(f"  æœ€å°: {min(sizes)}")
    report_lines.append(f"  æœ€å¤§: {max(sizes)}")
    report_lines.append(f"  å¹³å‡: {sum(sizes)/len(sizes):.1f}")
    report_lines.append(f"  ä¸­ä½æ•°: {sorted(sizes)[len(sizes)//2]}")
    report_lines.append("")

    # Top 20 æœ€å¤§èšç±»
    report_lines.append("ã€Top 20 æœ€å¤§èšç±»ã€‘")
    sorted_clusters = sorted(
        cluster_info.items(),
        key=lambda x: x[1]['size'],
        reverse=True
    )

    report_lines.append(f"{'æ’å':<5} {'èšç±»ID':<10} {'å¤§å°':<8} {'é¢‘æ¬¡æ€»å’Œ':<12} {'ç¤ºä¾‹çŸ­è¯­'}")
    report_lines.append("-" * 70)

    for rank, (cluster_id, info) in enumerate(sorted_clusters[:20], 1):
        examples = ', '.join(info['example_phrases'][:3])
        if len(examples) > 40:
            examples = examples[:37] + '...'
        report_lines.append(
            f"{rank:<5} {cluster_id:<10} {info['size']:<8} "
            f"{info['total_frequency']:<12} {examples}"
        )

    report_lines.append("")
    report_lines.append("="*70)
    report_lines.append(f"ä¸‹ä¸€æ­¥: è¿è¡Œ Phase 3 ç”Ÿæˆå¤§ç»„ç­›é€‰æŠ¥å‘Š")
    report_lines.append("="*70)

    # è¾“å‡ºæŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / f'phase2_clustering_report_round{round_id}.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # 6. å®Œæˆ
    print("\n" + "="*70)
    print("âœ… Phase 2 å¤§ç»„èšç±»å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š èšç±»æ‘˜è¦:")
    print(f"  - å¤„ç†çŸ­è¯­æ•°: {len(phrases):,}")
    print(f"  - ç”Ÿæˆèšç±»æ•°: {len(cluster_info)}")
    print(f"  - å™ªéŸ³ç‚¹æ•°: {noise_count}")
    print(f"  - Embeddingç¼“å­˜: data/cache/embeddings_round{round_id}.npz")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ Phase 3: python scripts/run_phase3_selection.py")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 2: å¤§ç»„èšç±»')
    parser.add_argument(
        '--round-id',
        type=int,
        default=1,
        help='æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=0,
        help='é™åˆ¶å¤„ç†çš„çŸ­è¯­æ•°é‡ï¼Œç”¨äºæµ‹è¯•ï¼ˆ0=å…¨éƒ¨ï¼Œé»˜è®¤0ï¼‰'
    )
    parser.add_argument(
        '--min-cluster-size',
        type=int,
        default=None,
        help='HDBSCANæœ€å°èšç±»å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--min-samples',
        type=int,
        default=None,
        help='HDBSCANæœ€å°æ ·æœ¬æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--force-recalculate',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°è®¡ç®—embeddingsï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰'
    )

    args = parser.parse_args()

    try:
        success = run_phase2_clustering(
            round_id=args.round_id,
            limit=args.limit,
            min_cluster_size=args.min_cluster_size,
            min_samples=args.min_samples,
            force_recalculate=args.force_recalculate
        )
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        # å®‰å…¨åœ°æ‰“å°é”™è¯¯æ¶ˆæ¯ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
        try:
            error_msg = str(e)
        except:
            error_msg = repr(e)

        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {error_msg}")

        # å®‰å…¨åœ°æ‰“å°traceback
        import traceback
        import io

        try:
            # å°è¯•æ­£å¸¸æ‰“å°traceback
            traceback.print_exc()
        except UnicodeEncodeError:
            # å¦‚æœå¤±è´¥ï¼Œå°†tracebackå†™å…¥å­—ç¬¦ä¸²ç¼“å†²åŒº
            try:
                buffer = io.StringIO()
                traceback.print_exc(file=buffer)
                tb_str = buffer.getvalue()
                # ç§»é™¤æ— æ³•ç¼–ç çš„å­—ç¬¦
                tb_safe = tb_str.encode('utf-8', errors='replace').decode('utf-8')
                print(tb_safe)
            except:
                print("âš ï¸  æ— æ³•æ˜¾ç¤ºå®Œæ•´é”™è¯¯å †æ ˆï¼ˆç¼–ç é—®é¢˜ï¼‰")

        sys.exit(1)


if __name__ == "__main__":
    main()
