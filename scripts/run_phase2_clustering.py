"""
Phase 2: å¤§ç»„èšç±»
å¯¹æ‰€æœ‰çŸ­è¯­è¿›è¡Œè¯­ä¹‰èšç±»ï¼Œç”Ÿæˆ60-100ä¸ªå¤§ç»„

è¿è¡Œæ–¹å¼ï¼š
    python scripts/run_phase2_clustering.py [--round-id 1] [--limit 0]

å‚æ•°ï¼š
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
    --limit: é™åˆ¶å¤„ç†çš„çŸ­è¯­æ•°é‡ï¼Œç”¨äºæµ‹è¯•ï¼ˆ0=å…¨éƒ¨ï¼‰
"""
import sys
import argparse
from pathlib import Path

# è®¾ç½®UTF-8ç¼–ç è¾“å‡ºï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import OUTPUT_DIR
from core.embedding import EmbeddingService
from core.clustering import cluster_phrases_large
from storage.repository import PhraseRepository, ClusterMetaRepository
from storage.models import Phrase


def run_phase2_clustering(round_id: int = 1, limit: int = 0):
    """
    æ‰§è¡ŒPhase 2å¤§ç»„èšç±»

    Args:
        round_id: æ•°æ®è½®æ¬¡ID
        limit: é™åˆ¶å¤„ç†æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰
    """
    print("\n" + "="*70)
    print("Phase 2: å¤§ç»„èšç±»".center(70))
    print("="*70)

    # 1. ä»æ•°æ®åº“åŠ è½½çŸ­è¯­
    print("\nã€æ­¥éª¤1ã€‘ä»æ•°æ®åº“åŠ è½½çŸ­è¯­...")
    with PhraseRepository() as repo:
        # è·å–æœªå¤„ç†çš„çŸ­è¯­
        query = repo.session.query(Phrase).filter(
            Phrase.processed_status == 'unseen'
        )

        if limit > 0:
            query = query.limit(limit)
            print(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {limit} æ¡çŸ­è¯­")

        phrases_db = query.all()

        if not phrases_db:
            print("\nâŒ æ²¡æœ‰å¾…å¤„ç†çš„çŸ­è¯­ï¼")
            return False

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        phrases = [{
            'phrase_id': p.phrase_id,
            'phrase': p.phrase,
            'frequency': p.frequency,
            'volume': p.volume,
            'seed_word': p.seed_word,
            'source_type': p.source_type,
        } for p in phrases_db]

        print(f"âœ“ åŠ è½½äº† {len(phrases)} æ¡å¾…èšç±»çŸ­è¯­")

    # 2. è®¡ç®—Embeddings
    print("\nã€æ­¥éª¤2ã€‘è®¡ç®—Embeddings...")
    embedding_service = EmbeddingService(use_cache=True)
    embeddings, phrase_ids = embedding_service.embed_phrases_from_db(phrases, round_id)

    # éªŒè¯
    assert len(embeddings) == len(phrases), "Embeddingsæ•°é‡ä¸åŒ¹é…"
    assert len(phrase_ids) == len(phrases), "Phrase IDsæ•°é‡ä¸åŒ¹é…"

    # 3. æ‰§è¡Œå¤§ç»„èšç±»
    print("\nã€æ­¥éª¤3ã€‘æ‰§è¡Œå¤§ç»„èšç±»...")
    cluster_ids, cluster_info, clusterer = cluster_phrases_large(embeddings, phrases)

    # 4. æ›´æ–°æ•°æ®åº“
    print("\nã€æ­¥éª¤4ã€‘æ›´æ–°æ•°æ®åº“...")

    # 4.1 æ›´æ–°phrasesè¡¨çš„cluster_id_A
    print("\n  æ›´æ–°phrasesè¡¨...")
    with PhraseRepository() as repo:
        success_count = 0
        for i, phrase_id in enumerate(phrase_ids):
            cluster_id = int(cluster_ids[i])
            if repo.update_cluster_assignment(phrase_id, cluster_id_A=cluster_id):
                success_count += 1

        print(f"  âœ“ å·²æ›´æ–° {success_count}/{len(phrase_ids)} æ¡è®°å½•çš„cluster_id_A")

    # 4.2 ä¿å­˜cluster_metaè¡¨
    print("\n  ä¿å­˜èšç±»å…ƒæ•°æ®...")
    with ClusterMetaRepository() as repo:
        for cluster_id, info in cluster_info.items():
            # å‡†å¤‡ç¤ºä¾‹çŸ­è¯­ï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰
            example_phrases_str = '; '.join(info['example_phrases'])

            # åˆ›å»ºæˆ–æ›´æ–°èšç±»å…ƒæ•°æ®
            repo.create_or_update_cluster(
                cluster_id=cluster_id,
                cluster_level='A',
                size=info['size'],
                example_phrases=example_phrases_str,
                main_theme=None,  # Phase 3ä¼šç”¨AIç”Ÿæˆ
                total_frequency=info['total_frequency']
            )

        print(f"  âœ“ å·²ä¿å­˜ {len(cluster_info)} ä¸ªèšç±»çš„å…ƒæ•°æ®")

    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\nã€æ­¥éª¤5ã€‘ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("="*70)
    report_lines.append("Phase 2 å¤§ç»„èšç±»æŠ¥å‘Š")
    report_lines.append("="*70)
    report_lines.append("")

    report_lines.append("ã€èšç±»æ¦‚å†µã€‘")
    report_lines.append(f"  æ€»çŸ­è¯­æ•°: {len(phrases):,}")
    report_lines.append(f"  èšç±»æ•°é‡: {len(cluster_info)}")
    noise_count = (cluster_ids == -1).sum()
    report_lines.append(f"  å™ªéŸ³ç‚¹æ•°: {noise_count} ({noise_count/len(phrases)*100:.1f}%)")
    report_lines.append("")

    # èšç±»å¤§å°åˆ†å¸ƒ
    sizes = [info['size'] for info in cluster_info.values()]
    report_lines.append("ã€èšç±»å¤§å°åˆ†å¸ƒã€‘")
    report_lines.append(f"  æœ€å°: {min(sizes)}")
    report_lines.append(f"  æœ€å¤§: {max(sizes)}")
    report_lines.append(f"  å¹³å‡: {sum(sizes)/len(sizes):.1f}")
    report_lines.append(f"  ä¸­ä½æ•°: {sorted(sizes)[len(sizes)//2]}")
    report_lines.append("")

    # Top 20 æœ€å¤§èšç±»
    report_lines.append("ã€Top 20 æœ€å¤§èšç±»ã€‘")
    sorted_clusters = sorted(
        cluster_info.items(),
        key=lambda x: x[1]['size'],
        reverse=True
    )

    report_lines.append(f"{'æ’å':<5} {'èšç±»ID':<10} {'å¤§å°':<8} {'é¢‘æ¬¡æ€»å’Œ':<12} {'ç¤ºä¾‹çŸ­è¯­'}")
    report_lines.append("-" * 70)

    for rank, (cluster_id, info) in enumerate(sorted_clusters[:20], 1):
        examples = ', '.join(info['example_phrases'][:3])
        if len(examples) > 40:
            examples = examples[:37] + '...'
        report_lines.append(
            f"{rank:<5} {cluster_id:<10} {info['size']:<8} "
            f"{info['total_frequency']:<12} {examples}"
        )

    report_lines.append("")
    report_lines.append("="*70)
    report_lines.append(f"ä¸‹ä¸€æ­¥: è¿è¡Œ Phase 3 ç”Ÿæˆå¤§ç»„ç­›é€‰æŠ¥å‘Š")
    report_lines.append("="*70)

    # è¾“å‡ºæŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / f'phase2_clustering_report_round{round_id}.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)
    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # 6. å®Œæˆ
    print("\n" + "="*70)
    print("âœ… Phase 2 å¤§ç»„èšç±»å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š èšç±»æ‘˜è¦:")
    print(f"  - å¤„ç†çŸ­è¯­æ•°: {len(phrases):,}")
    print(f"  - ç”Ÿæˆèšç±»æ•°: {len(cluster_info)}")
    print(f"  - å™ªéŸ³ç‚¹æ•°: {noise_count}")
    print(f"  - Embeddingç¼“å­˜: data/cache/embeddings_round{round_id}.npz")
    print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ Phase 3: python scripts/run_phase3_selection.py")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 2: å¤§ç»„èšç±»')
    parser.add_argument(
        '--round-id',
        type=int,
        default=1,
        help='æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=0,
        help='é™åˆ¶å¤„ç†çš„çŸ­è¯­æ•°é‡ï¼Œç”¨äºæµ‹è¯•ï¼ˆ0=å…¨éƒ¨ï¼Œé»˜è®¤0ï¼‰'
    )

    args = parser.parse_args()

    try:
        success = run_phase2_clustering(round_id=args.round_id, limit=args.limit)
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
