"""
Phase 2C: DeepSeekè¯­ä¹‰æ ‡æ³¨è„šæœ¬
ä¸ºLouvainèšç±»ç»“æœæ·»åŠ è¯­ä¹‰æ ‡ç­¾å’Œéœ€æ±‚åˆ†ç±»

è¿è¡Œæ–¹å¼:
    python scripts/run_phase2_label_clusters.py [é€‰é¡¹]

å‚æ•°:
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
    --limit: é™åˆ¶æ ‡æ³¨çš„èšç±»æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰
    --min-cluster-size: ä»…æ ‡æ³¨å¤§å°>=æ­¤å€¼çš„èšç±»ï¼ˆé»˜è®¤10ï¼‰

ç¤ºä¾‹:
    # æ ‡æ³¨æ‰€æœ‰èšç±»
    python scripts/run_phase2_label_clusters.py

    # ä»…æ ‡æ³¨å‰20ä¸ªæœ€å¤§èšç±»
    python scripts/run_phase2_label_clusters.py --limit=20

    # ä»…æ ‡æ³¨å¤§å°>=20çš„èšç±»
    python scripts/run_phase2_label_clusters.py --min-cluster-size=20
"""
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ç¼–ç ä¿®å¤
from utils.encoding_fix import setup_encoding
setup_encoding()

from config.settings import OUTPUT_DIR
from core.cluster_labeling import ClusterLabeler
from storage.repository import ClusterMetaRepository, PhraseRepository
from storage.models import ClusterMeta

def run_phase2_label_clusters(
    round_id: int = 1,
    limit: int = 0,
    min_cluster_size: int = 10
):
    """æ‰§è¡ŒPhase 2C DeepSeekè¯­ä¹‰æ ‡æ³¨"""
    print("\n" + "="*70)
    print("Phase 2C: DeepSeekè¯­ä¹‰æ ‡æ³¨".center(70))
    print("="*70)

    # 1. ä»æ•°æ®åº“åŠ è½½èšç±»
    print("\nã€æ­¥éª¤1ã€‘ä»æ•°æ®åº“åŠ è½½èšç±»...")
    with ClusterMetaRepository() as meta_repo:
        query = meta_repo.session.query(ClusterMeta).filter(
            ClusterMeta.cluster_level == 'A',
            ClusterMeta.size >= min_cluster_size
        ).order_by(ClusterMeta.size.desc())  # æŒ‰å¤§å°é™åº

        if limit > 0:
            query = query.limit(limit)
            print(f"âš ï¸  é™åˆ¶æ¨¡å¼ï¼šä»…æ ‡æ³¨å‰ {limit} ä¸ªèšç±»")

        clusters_db = query.all()

        if not clusters_db:
            print("\nâŒ æ²¡æœ‰å¾…æ ‡æ³¨çš„èšç±»ï¼")
            return False

        print(f"âœ“ åŠ è½½äº† {len(clusters_db)} ä¸ªå¾…æ ‡æ³¨èšç±»")

    # 2. åŠ è½½æ¯ä¸ªèšç±»çš„çŸ­è¯­
    print("\nã€æ­¥éª¤2ã€‘åŠ è½½èšç±»çŸ­è¯­...")
    clusters_to_label = []

    with PhraseRepository() as phrase_repo:
        for cluster in clusters_db:
            # æŸ¥è¯¢è¯¥èšç±»çš„æ‰€æœ‰çŸ­è¯­
            phrases_db = phrase_repo.session.query(phrase_repo.model).filter(
                phrase_repo.model.cluster_id_A == cluster.cluster_id
            ).all()

            phrases = [p.phrase for p in phrases_db]

            clusters_to_label.append({
                'cluster_id': cluster.cluster_id,
                'phrases': phrases,
                'size': cluster.size
            })

    print(f"âœ“ å·²åŠ è½½ {len(clusters_to_label)} ä¸ªèšç±»çš„çŸ­è¯­æ•°æ®")

    # 3. åˆå§‹åŒ–DeepSeekæ ‡æ³¨å™¨
    print("\nã€æ­¥éª¤3ã€‘åˆå§‹åŒ–DeepSeekæ ‡æ³¨å™¨...")
    try:
        labeler = ClusterLabeler(provider="deepseek")
        print("âœ“ DeepSeekæ ‡æ³¨å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ æ ‡æ³¨å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False

    # 4. æ‰¹é‡æ ‡æ³¨
    print("\nã€æ­¥éª¤4ã€‘æ‰§è¡Œæ‰¹é‡æ ‡æ³¨...")
    print(f"  å…± {len(clusters_to_label)} ä¸ªèšç±»å¾…æ ‡æ³¨")

    labeling_results = {}
    success_count = 0
    fail_count = 0

    for i, cluster in enumerate(clusters_to_label, 1):
        cluster_id = cluster['cluster_id']
        phrases = cluster['phrases']

        print(f"\n[{i}/{len(clusters_to_label)}] æ ‡æ³¨èšç±» {cluster_id} ({cluster['size']} phrases)...")

        try:
            result = labeler.label_cluster(cluster_id, phrases)
            labeling_results[cluster_id] = result

            print(f"  âœ“ æ ‡ç­¾: {result['llm_label']}")
            print(f"  âœ“ éœ€æ±‚ç±»å‹: {result['primary_demand_type']}")
            print(f"  âœ“ ç½®ä¿¡åº¦: {result['labeling_confidence']}")

            success_count += 1

        except Exception as e:
            print(f"  âœ— æ ‡æ³¨å¤±è´¥: {str(e)}")
            fail_count += 1
            continue

    # 5. æ›´æ–°æ•°æ®åº“
    print("\nã€æ­¥éª¤5ã€‘æ›´æ–°æ•°æ®åº“...")
    update_count = 0

    with ClusterMetaRepository() as repo:
        for cluster_id, result in labeling_results.items():
            # è½¬æ¢secondary_demand_typesä¸ºJSONå­—ç¬¦ä¸²
            import json
            secondary_types_json = json.dumps(result['secondary_demand_types'])

            # æ›´æ–°cluster_metaè¡¨
            success = repo.update_cluster_labeling(
                cluster_id=cluster_id,
                llm_label=result['llm_label'],
                llm_summary=result['llm_summary'],
                primary_demand_type=result['primary_demand_type'],
                secondary_demand_types=secondary_types_json,
                labeling_confidence=result['labeling_confidence']
            )

            if success:
                update_count += 1

    print(f"  âœ“ å·²æ›´æ–° {update_count}/{len(labeling_results)} ä¸ªèšç±»çš„æ ‡æ³¨æ•°æ®")

    # 6. ç”Ÿæˆæ ‡æ³¨æŠ¥å‘Š
    print("\nã€æ­¥éª¤6ã€‘ç”Ÿæˆæ ‡æ³¨æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("="*70)
    report_lines.append("Phase 2C DeepSeekè¯­ä¹‰æ ‡æ³¨æŠ¥å‘Š")
    report_lines.append("="*70)
    report_lines.append("")

    # æ ‡æ³¨æ¦‚å†µ
    report_lines.append("ã€æ ‡æ³¨æ¦‚å†µã€‘")
    report_lines.append(f"  å¾…æ ‡æ³¨èšç±»æ•°: {len(clusters_to_label)}")
    report_lines.append(f"  æ ‡æ³¨æˆåŠŸæ•°: {success_count}")
    report_lines.append(f"  æ ‡æ³¨å¤±è´¥æ•°: {fail_count}")
    report_lines.append(f"  æˆåŠŸç‡: {success_count/(success_count+fail_count)*100:.1f}%")
    report_lines.append("")

    # éœ€æ±‚ç±»å‹åˆ†å¸ƒ
    report_lines.append("ã€éœ€æ±‚ç±»å‹åˆ†å¸ƒã€‘")
    type_counts = {}
    for result in labeling_results.values():
        dtype = result['primary_demand_type']
        type_counts[dtype] = type_counts.get(dtype, 0) + 1

    for dtype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        pct = count / len(labeling_results) * 100
        report_lines.append(f"  {dtype}: {count} ({pct:.1f}%)")
    report_lines.append("")

    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confidences = [r['labeling_confidence'] for r in labeling_results.values()]
    if confidences:
        report_lines.append("ã€ç½®ä¿¡åº¦ç»Ÿè®¡ã€‘")
        report_lines.append(f"  æœ€ä½: {min(confidences)}")
        report_lines.append(f"  æœ€é«˜: {max(confidences)}")
        report_lines.append(f"  å¹³å‡: {sum(confidences)/len(confidences):.1f}")
        report_lines.append(f"  ä¸­ä½æ•°: {sorted(confidences)[len(confidences)//2]}")
        report_lines.append("")

    # Top 10 æœ€å¤§èšç±»åŠå…¶æ ‡æ³¨
    report_lines.append("ã€Top 10 æœ€å¤§èšç±»æ ‡æ³¨ç»“æœã€‘")
    sorted_clusters = sorted(clusters_to_label, key=lambda x: x['size'], reverse=True)

    for i, cluster in enumerate(sorted_clusters[:10], 1):
        cluster_id = cluster['cluster_id']
        if cluster_id in labeling_results:
            result = labeling_results[cluster_id]
            report_lines.append(f"\n{i}. èšç±» {cluster_id} (size={cluster['size']})")
            report_lines.append(f"   æ ‡ç­¾: {result['llm_label']}")
            report_lines.append(f"   æè¿°: {result['llm_summary']}")
            report_lines.append(f"   éœ€æ±‚ç±»å‹: {result['primary_demand_type']}")
            report_lines.append(f"   ç½®ä¿¡åº¦: {result['labeling_confidence']}")

    report_lines.append("")
    report_lines.append("="*70)
    report_lines.append("âœ… è¯­ä¹‰æ ‡æ³¨å®Œæˆï¼")
    report_lines.append("")
    report_lines.append("ä¸‹ä¸€æ­¥:")
    report_lines.append("  1. æ£€æŸ¥æ ‡æ³¨è´¨é‡ï¼Œä¿®æ­£æ˜æ˜¾é”™è¯¯")
    report_lines.append("  2. åœ¨UIä¸­ç­›é€‰é«˜ä»·å€¼èšç±»")
    report_lines.append("  3. è¿è¡ŒPhase 3: äººå·¥ç­›é€‰å’Œè¯„åˆ†")
    report_lines.append("="*70)

    # è¾“å‡ºæŠ¥å‘Š
    report_text = '\n'.join(report_lines)
    print('\n' + report_text)

    # ä¿å­˜æŠ¥å‘Š
    OUTPUT_DIR.mkdir(exist_ok=True)
    report_file = OUTPUT_DIR / f'phase2c_labeling_report_round{round_id}.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # 7. å®Œæˆ
    print("\n" + "="*70)
    print("âœ… Phase 2C DeepSeekè¯­ä¹‰æ ‡æ³¨å®Œæˆï¼".center(70))
    print("="*70)

    print(f"\nğŸ“Š æ ‡æ³¨æ‘˜è¦:")
    print(f"  - æ ‡æ³¨èšç±»æ•°: {success_count}/{len(clusters_to_label)}")
    print(f"  - å¹³å‡ç½®ä¿¡åº¦: {sum(confidences)/len(confidences):.1f}" if confidences else "  - å¹³å‡ç½®ä¿¡åº¦: N/A")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 2C: DeepSeekè¯­ä¹‰æ ‡æ³¨')
    parser.add_argument('--round-id', type=int, default=1, help='æ•°æ®è½®æ¬¡ID')
    parser.add_argument('--limit', type=int, default=0, help='é™åˆ¶æ ‡æ³¨æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰')
    parser.add_argument('--min-cluster-size', type=int, default=10, help='æœ€å°èšç±»å¤§å°')

    args = parser.parse_args()

    try:
        success = run_phase2_label_clusters(
            round_id=args.round_id,
            limit=args.limit,
            min_cluster_size=args.min_cluster_size
        )
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        import traceback
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
