"""
Phase 0 - Experiment B: Tokenè¦†ç›–ç‡æµ‹é‡
Token Coverage Measurement

ç›®æ ‡ï¼šæµ‹é‡å½“å‰26ä¸ªtokenè¦†ç›–äº†å¤šå°‘çŸ­è¯­

åˆ¤æ–­æ ‡å‡†ï¼š
- è¦†ç›–ç‡â‰¥80% â†’ tokenå¤Ÿç”¨ï¼Œæš‚ä¸éœ€è¦æ‰©å±•
- è¦†ç›–ç‡â‰¤60% â†’ éœ€è¦å®æ–½Phase 2.2æ¨¡æ¿-å˜é‡è¿­ä»£æ‰©å±•

åˆ›å»ºæ—¥æœŸï¼š2025-12-23
"""

import json
import io
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set
from collections import Counter
import sys

# Set UTF-8 encoding for Windows console
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from storage.repository import PhraseRepository, TokenRepository
from storage.models import Phrase


def check_phrase_coverage(phrase: str, tokens: Set[str]) -> List[str]:
    """
    æ£€æŸ¥çŸ­è¯­æ˜¯å¦è¢«tokensè¦†ç›–

    Args:
        phrase: çŸ­è¯­æ–‡æœ¬
        tokens: tokené›†åˆ

    Returns:
        åŒ¹é…åˆ°çš„tokensåˆ—è¡¨
    """
    phrase_lower = phrase.lower()
    matched_tokens = []

    for token in tokens:
        if token.lower() in phrase_lower:
            matched_tokens.append(token)

    return matched_tokens


def analyze_uncovered_phrases(uncovered_phrases: List[str], top_k: int = 50) -> Dict:
    """
    åˆ†ææœªè¦†ç›–çŸ­è¯­çš„ç‰¹å¾

    Args:
        uncovered_phrases: æœªè¦†ç›–çš„çŸ­è¯­åˆ—è¡¨
        top_k: æå–top Kä¸ªé«˜é¢‘è¯

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    # æå–æ‰€æœ‰è¯
    all_words = []
    for phrase in uncovered_phrases:
        words = phrase.lower().split()
        all_words.extend(words)

    # ç»Ÿè®¡è¯é¢‘
    word_counter = Counter(all_words)

    # è¿‡æ»¤åœç”¨è¯
    BASIC_STOP_WORDS = {
        'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at',
        'to', 'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were'
    }

    filtered_words = [
        (word, freq) for word, freq in word_counter.most_common()
        if word not in BASIC_STOP_WORDS and len(word) > 2
    ]

    return {
        'top_words': filtered_words[:top_k],
        'total_unique_words': len(word_counter),
        'uncovered_sample': uncovered_phrases[:20]  # å‰20ä¸ªæ ·æœ¬
    }


def run_experiment_b() -> Dict:
    """
    æ‰§è¡Œå®éªŒBï¼šTokenè¦†ç›–ç‡æµ‹é‡

    Returns:
        å®éªŒç»“æœå­—å…¸
    """
    print("\n" + "="*70)
    print("Phase 0 - Experiment B: Tokenè¦†ç›–ç‡æµ‹é‡")
    print("="*70)

    # 1. åŠ è½½æ‰€æœ‰çŸ­è¯­
    print("\n1. åŠ è½½çŸ­è¯­æ•°æ®...")

    with PhraseRepository() as phrase_repo:
        all_phrases = phrase_repo.session.query(Phrase).all()

    total_phrases = len(all_phrases)
    print(f"âœ“ çŸ­è¯­æ€»æ•°: {total_phrases:,}")

    # 2. åŠ è½½å½“å‰tokens
    print("\n2. åŠ è½½Tokenè¯åº“...")

    with TokenRepository() as token_repo:
        tokens = token_repo.get_all_tokens()

    token_set = {token.token_text for token in tokens}
    token_count = len(token_set)

    print(f"âœ“ Tokenæ€»æ•°: {token_count}")
    print(f"\nTokenåˆ—è¡¨:")
    for i, token in enumerate(sorted(token_set), 1):
        print(f"  {i:2d}. {token}")

    # 3. æ£€æŸ¥è¦†ç›–ç‡
    print(f"\n3. æ£€æŸ¥Tokenè¦†ç›–ç‡...")
    print("   (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")

    covered_phrases = []
    uncovered_phrases = []
    phrase_token_mapping = {}  # {phrase: [matched_tokens]}

    for phrase_obj in all_phrases:
        phrase = phrase_obj.phrase
        matched_tokens = check_phrase_coverage(phrase, token_set)

        if matched_tokens:
            covered_phrases.append(phrase)
            phrase_token_mapping[phrase] = matched_tokens
        else:
            uncovered_phrases.append(phrase)

    covered_count = len(covered_phrases)
    uncovered_count = len(uncovered_phrases)
    coverage_rate = covered_count / total_phrases if total_phrases > 0 else 0

    print(f"\nâœ“ è¦†ç›–ç»Ÿè®¡:")
    print(f"  - è¢«è¦†ç›–çŸ­è¯­: {covered_count:,} ({coverage_rate:.1%})")
    print(f"  - æœªè¦†ç›–çŸ­è¯­: {uncovered_count:,} ({1-coverage_rate:.1%})")

    # 4. Tokenä½¿ç”¨é¢‘ç‡ç»Ÿè®¡
    print("\n4. Tokenä½¿ç”¨é¢‘ç‡ç»Ÿè®¡...")

    token_usage = Counter()
    for matched_tokens in phrase_token_mapping.values():
        token_usage.update(matched_tokens)

    print(f"\nTop 10 é«˜é¢‘Token:")
    for i, (token, count) in enumerate(token_usage.most_common(10), 1):
        percentage = count / covered_count * 100 if covered_count > 0 else 0
        print(f"  {i:2d}. '{token:20s}' - {count:6,}æ¬¡ ({percentage:5.1f}% of covered)")

    # 5. åˆ†ææœªè¦†ç›–çŸ­è¯­
    print("\n5. åˆ†ææœªè¦†ç›–çŸ­è¯­ç‰¹å¾...")

    uncovered_analysis = analyze_uncovered_phrases(uncovered_phrases, top_k=50)

    print(f"\næœªè¦†ç›–çŸ­è¯­ä¸­çš„é«˜é¢‘è¯ï¼ˆTop 20ï¼‰:")
    for i, (word, freq) in enumerate(uncovered_analysis['top_words'][:20], 1):
        print(f"  {i:2d}. '{word:20s}' - {freq:6,}æ¬¡")

    print(f"\næœªè¦†ç›–çŸ­è¯­æ ·æœ¬ï¼ˆå‰10ä¸ªï¼‰:")
    for i, phrase in enumerate(uncovered_analysis['uncovered_sample'][:10], 1):
        print(f"  {i:2d}. {phrase}")

    # 6. ç”Ÿæˆç»“æœ
    result = {
        'experiment': 'B',
        'name': 'Tokenè¦†ç›–ç‡',
        'timestamp': datetime.now().isoformat(),
        'total_phrases': total_phrases,
        'token_count': token_count,
        'tokens': sorted(list(token_set)),
        'covered_count': covered_count,
        'uncovered_count': uncovered_count,
        'coverage_rate': round(coverage_rate, 4),
        'token_usage': dict(token_usage.most_common()),
        'uncovered_top_words': uncovered_analysis['top_words'][:50],
        'uncovered_sample': uncovered_analysis['uncovered_sample']
    }

    # 7. åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰©å±•
    if coverage_rate >= 0.80:
        result['recommendation'] = 'sufficient'
        result['recommendation_detail'] = f'Tokenè¦†ç›–ç‡{coverage_rate:.1%}ï¼Œè¯åº“å……è¶³ï¼Œæš‚ä¸éœ€è¦æ‰©å±•'
    elif coverage_rate <= 0.60:
        result['recommendation'] = 'need_expansion'
        result['recommendation_detail'] = f'Tokenè¦†ç›–ç‡{coverage_rate:.1%}ï¼Œè¯åº“ä¸è¶³ï¼Œå»ºè®®å®æ–½Phase 2.2æ¨¡æ¿-å˜é‡è¿­ä»£æ‰©å±•'
    else:
        result['recommendation'] = 'moderate'
        result['recommendation_detail'] = f'Tokenè¦†ç›–ç‡{coverage_rate:.1%}ï¼Œè¯åº“ä¸­ç­‰ï¼Œå¯è€ƒè™‘é€‚åº¦æ‰©å±•'

    # 8. ä¿å­˜ç»“æœ
    output_dir = project_root / 'data' / 'phase0_results'
    output_dir.mkdir(parents=True, exist_ok=True)

    result_file = output_dir / 'experiment_b_result.json'
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    # 9. æ˜¾ç¤ºç»“æœ
    print("\n" + "="*70)
    print("å®éªŒBç»“æœ")
    print("="*70)
    print(f"çŸ­è¯­æ€»æ•°:       {result['total_phrases']:,}")
    print(f"Tokenæ€»æ•°:      {result['token_count']}")
    print(f"è¢«è¦†ç›–çŸ­è¯­:     {result['covered_count']:,}")
    print(f"æœªè¦†ç›–çŸ­è¯­:     {result['uncovered_count']:,}")
    print(f"è¦†ç›–ç‡:         {result['coverage_rate']:.1%}")
    print(f"\nåˆ¤æ–­ç»“æœ:       {result['recommendation']}")
    print(f"å»ºè®®:           {result['recommendation_detail']}")
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print("="*70)

    return result


if __name__ == "__main__":
    try:
        result = run_experiment_b()

        print("\nâœ… å®éªŒBå®Œæˆï¼")
        print(f"\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼šè¿è¡Œå®éªŒCï¼ˆåŒä¹‰å†—ä½™ç‡æµ‹é‡ï¼‰")
        print(f"   å‘½ä»¤: python scripts/phase0_experiment_c_redundancy.py")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  å®éªŒè¢«ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å®éªŒæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
