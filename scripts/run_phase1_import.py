"""
Phase 1: æ•°æ®å¯¼å…¥
å°†åŸå§‹å…³é”®è¯æ•°æ®å¯¼å…¥æ•°æ®åº“

è¿è¡Œæ–¹å¼ï¼š
    python scripts/run_phase1_import.py [--round-id 1] [--dry-run]

å‚æ•°ï¼š
    --round-id: æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰
    --dry-run: è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…æ’å…¥æ•°æ®åº“ï¼Œä»…å±•ç¤ºç»Ÿè®¡ä¿¡æ¯
"""
import sys
import argparse
from pathlib import Path

# è®¾ç½®UTF-8ç¼–ç è¾“å‡ºï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import RAW_DATA_DIR, PROCESSED_DATA_DIR
from core.data_integration import DataIntegration
from storage.repository import PhraseRepository, test_database_connection
from storage.models import create_all_tables


def run_phase1_import(round_id: int = 1, dry_run: bool = False):
    """
    æ‰§è¡ŒPhase 1æ•°æ®å¯¼å…¥

    Args:
        round_id: æ•°æ®è½®æ¬¡ID
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
    """
    print("\n" + "="*70)
    print("Phase 1: æ•°æ®å¯¼å…¥".center(70))
    print("="*70)

    # 1. æµ‹è¯•æ•°æ®åº“è¿æ¥
    print("\nã€æ­¥éª¤1ã€‘æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    if not test_database_connection():
        print("\nâŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥é…ç½®ã€‚")
        print("æç¤ºï¼š")
        print("  1. ç¡®ä¿MySQL/MariaDBæœåŠ¡å·²å¯åŠ¨")
        print("  2. æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„æ•°æ®åº“é…ç½®")
        print("  3. ç¡®ä¿æ•°æ®åº“å·²åˆ›å»ºï¼ˆè¿è¡Œ: python -c 'from storage.models import create_all_tables; create_all_tables()'ï¼‰")
        return False

    # 2. ç¡®ä¿æ•°æ®è¡¨å·²åˆ›å»º
    print("\nã€æ­¥éª¤2ã€‘æ£€æŸ¥æ•°æ®è¡¨...")
    try:
        create_all_tables()
        print("âœ“ æ•°æ®è¡¨æ£€æŸ¥å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸  æ•°æ®è¡¨åˆ›å»ºè­¦å‘Š: {str(e)}")

    # 3. æ•°æ®æ•´åˆä¸æ¸…æ´—
    print("\nã€æ­¥éª¤3ã€‘æ•°æ®æ•´åˆä¸æ¸…æ´—...")
    integrator = DataIntegration(RAW_DATA_DIR)
    df = integrator.merge_and_clean(round_id=round_id)

    if df.empty:
        print("\nâŒ æ²¡æœ‰å¯å¯¼å…¥çš„æ•°æ®ï¼")
        return False

    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®åˆ°processedç›®å½•
    processed_file = PROCESSED_DATA_DIR / f'integrated_round{round_id}.csv'
    PROCESSED_DATA_DIR.mkdir(exist_ok=True)
    df.to_csv(processed_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: {processed_file}")

    # 4. å‡†å¤‡æ•°æ®åº“æ’å…¥æ ¼å¼
    print("\nã€æ­¥éª¤4ã€‘å‡†å¤‡æ•°æ®åº“æ’å…¥æ ¼å¼...")
    records = integrator.prepare_for_database(df)

    # 5. æ’å…¥æ•°æ®åº“
    if dry_run:
        print("\nã€è¯•è¿è¡Œæ¨¡å¼ã€‘è·³è¿‡æ•°æ®åº“æ’å…¥")
        print(f"âœ“ é¢„è®¡æ’å…¥ {len(records)} æ¡è®°å½•")
        print("\nç¤ºä¾‹è®°å½•ï¼ˆå‰3æ¡ï¼‰:")
        for i, record in enumerate(records[:3], 1):
            print(f"\nè®°å½• {i}:")
            for key, value in record.items():
                print(f"  {key}: {value}")
    else:
        print("\nã€æ­¥éª¤5ã€‘æ’å…¥æ•°æ®åº“...")
        with PhraseRepository() as repo:
            # æ£€æŸ¥å½“å‰è®°å½•æ•°
            current_count = repo.get_phrase_count()
            print(f"å½“å‰phrasesè¡¨è®°å½•æ•°: {current_count}")

            # æ‰¹é‡æ’å…¥
            inserted_count = repo.bulk_insert_phrases(records, batch_size=1000)

            # æ£€æŸ¥æ’å…¥åè®°å½•æ•°
            new_count = repo.get_phrase_count()
            print(f"\næ’å…¥åphrasesè¡¨è®°å½•æ•°: {new_count}")
            print(f"å®é™…æ–°å¢è®°å½•: {new_count - current_count}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print("\nã€æ•°æ®åº“ç»Ÿè®¡ã€‘")
            stats = repo.get_statistics()
            print(f"æ€»è®°å½•æ•°: {stats['total_count']}")
            print(f"\næŒ‰æ•°æ®æºåˆ†å¸ƒ:")
            for source, count in stats['by_source'].items():
                print(f"  {source}: {count}")
            print(f"\næŒ‰å¤„ç†çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in stats['by_status'].items():
                print(f"  {status}: {count}")
            print(f"\næŒ‰è½®æ¬¡åˆ†å¸ƒ:")
            for round_num, count in stats['by_round'].items():
                print(f"  Round {round_num}: {count}")

    # 6. å®Œæˆ
    print("\n" + "="*70)
    print("âœ… Phase 1 æ•°æ®å¯¼å…¥å®Œæˆï¼".center(70))
    print("="*70)

    print("\nğŸ“Š å¯¼å…¥æ‘˜è¦:")
    print(f"  - è½®æ¬¡ID: {round_id}")
    print(f"  - æ¸…æ´—åè®°å½•æ•°: {len(df)}")
    print(f"  - æ’å…¥è®°å½•æ•°: {len(records)}")
    if not dry_run:
        print(f"  - æ•°æ®åº“æ€»è®°å½•æ•°: {new_count}")
    print(f"  - æ¸…æ´—æ•°æ®ä¿å­˜ä½ç½®: {processed_file}")

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ Phase 2: python scripts/run_phase2_clustering.py")

    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Phase 1: æ•°æ®å¯¼å…¥')
    parser.add_argument(
        '--round-id',
        type=int,
        default=1,
        help='æ•°æ®è½®æ¬¡IDï¼ˆé»˜è®¤ä¸º1ï¼‰'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…æ’å…¥æ•°æ®åº“'
    )

    args = parser.parse_args()

    try:
        success = run_phase1_import(round_id=args.round_id, dry_run=args.dry_run)
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
