"""
Phase 0 - Experiment C (è‡ªåŠ¨åŒ–ç‰ˆæœ¬): åŒä¹‰å†—ä½™ç‡æµ‹é‡
Redundancy Rate Measurement (Automated)

è‡ªåŠ¨åŒ–ç­–ç•¥ï¼š
- ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•ï¼ˆåŸºäºè¯è¢‹å’Œç¼–è¾‘è·ç¦»ï¼‰
- è‡ªåŠ¨è¯†åˆ«åŒä¹‰ç»„
- ä¼°ç®—å†—ä½™ç‡

åˆ›å»ºæ—¥æœŸï¼š2025-12-23
"""

import json
import io
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set
import sys
import random
from difflib import SequenceMatcher

# Set UTF-8 encoding for Windows console
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from storage.repository import PhraseRepository
from storage.models import Phrase
from sqlalchemy import text


def normalize_phrase(phrase: str) -> str:
    """
    è§„èŒƒåŒ–çŸ­è¯­ï¼ˆç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒï¼‰
    - è½¬å°å†™
    - æ’åºè¯åºï¼ˆå¿½ç•¥è¯åºå·®å¼‚ï¼‰
    """
    words = phrase.lower().split()
    return ' '.join(sorted(words))


def calculate_similarity(phrase1: str, phrase2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªçŸ­è¯­çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰

    ç­–ç•¥ï¼š
    1. è§„èŒƒåŒ–åå®Œå…¨ç›¸åŒ -> 1.0
    2. SequenceMatcher ç›¸ä¼¼åº¦ > 0.8 -> è§†ä¸ºåŒä¹‰
    3. è¯è¢‹äº¤é›†æ¯”ä¾‹ > 0.8 -> è§†ä¸ºåŒä¹‰
    """
    # è§„èŒƒåŒ–
    norm1 = normalize_phrase(phrase1)
    norm2 = normalize_phrase(phrase2)

    # å®Œå…¨ç›¸åŒ
    if norm1 == norm2:
        return 1.0

    # åºåˆ—ç›¸ä¼¼åº¦
    seq_similarity = SequenceMatcher(None, norm1, norm2).ratio()

    # è¯è¢‹ç›¸ä¼¼åº¦
    words1 = set(phrase1.lower().split())
    words2 = set(phrase2.lower().split())

    if len(words1) == 0 or len(words2) == 0:
        return 0.0

    intersection = words1 & words2
    union = words1 | words2

    word_similarity = len(intersection) / len(union) if union else 0.0

    # ç»¼åˆç›¸ä¼¼åº¦
    return max(seq_similarity, word_similarity)


def auto_find_synonyms(sample_phrases: List[str],
                       similarity_threshold: float = 0.85) -> List[List[int]]:
    """
    è‡ªåŠ¨è¯†åˆ«åŒä¹‰ç»„

    Args:
        sample_phrases: çŸ­è¯­åˆ—è¡¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        åŒä¹‰ç»„åˆ—è¡¨ï¼Œæ¯ä¸ªç»„æ˜¯çŸ­è¯­ç´¢å¼•çš„åˆ—è¡¨
    """
    print(f"\nğŸ“Š è‡ªåŠ¨è¯†åˆ«åŒä¹‰ç»„ï¼ˆé˜ˆå€¼={similarity_threshold}ï¼‰...")

    synonym_groups = []
    phrase_to_group = {}  # {phrase_idx: group_idx}

    for i, phrase1 in enumerate(sample_phrases):
        if i in phrase_to_group:
            continue  # å·²åœ¨æŸä¸ªç»„ä¸­

        # åˆ›å»ºæ–°ç»„
        current_group = [i]
        phrase_to_group[i] = len(synonym_groups)

        # æŸ¥æ‰¾åŒä¹‰çŸ­è¯­
        for j in range(i + 1, len(sample_phrases)):
            if j in phrase_to_group:
                continue

            phrase2 = sample_phrases[j]
            similarity = calculate_similarity(phrase1, phrase2)

            if similarity >= similarity_threshold:
                current_group.append(j)
                phrase_to_group[j] = len(synonym_groups)

        # åªä¿å­˜æœ‰å¤šä¸ªæˆå‘˜çš„ç»„ï¼ˆçœŸæ­£çš„åŒä¹‰ç»„ï¼‰
        if len(current_group) > 1:
            synonym_groups.append(current_group)

    return synonym_groups


def run_experiment_c_auto() -> Dict:
    """
    è‡ªåŠ¨åŒ–è¿è¡Œå®éªŒC
    """
    print("\n" + "="*70)
    print("Phase 0 - Experiment C (è‡ªåŠ¨åŒ–): åŒä¹‰å†—ä½™ç‡æµ‹é‡")
    print("="*70)

    # 1. åŠ è½½æ‰€æœ‰çŸ­è¯­
    print("\n1. åŠ è½½çŸ­è¯­æ•°æ®...")

    with PhraseRepository() as phrase_repo:
        all_phrases_objs = phrase_repo.session.query(Phrase).all()

    all_phrases = [p.phrase for p in all_phrases_objs]
    print(f"âœ“ çŸ­è¯­æ€»æ•°: {len(all_phrases):,}")

    # 2. éšæœºæŠ½æ ·ï¼ˆå›ºå®šç§å­ä»¥ç¡®ä¿å¯å¤ç°ï¼‰
    sample_size = min(1000, len(all_phrases))
    random.seed(42)
    sample_indices = random.sample(range(len(all_phrases)), sample_size)
    sample_phrases = [all_phrases[i] for i in sample_indices]

    print(f"âœ“ æŠ½æ ·æ•°é‡: {sample_size:,}")

    # 3. è‡ªåŠ¨è¯†åˆ«åŒä¹‰ç»„
    print(f"\n2. è‡ªåŠ¨è¯†åˆ«åŒä¹‰ç»„...")

    synonym_groups = auto_find_synonyms(sample_phrases)

    synonym_groups_count = len(synonym_groups)
    phrases_in_groups = sum(len(g) for g in synonym_groups)
    redundancy_rate = (phrases_in_groups - synonym_groups_count) / sample_size if sample_size > 0 else 0.0

    print(f"\nâœ“ è¯†åˆ«å®Œæˆ")
    print(f"  åŒä¹‰ç»„æ•°: {synonym_groups_count}")
    print(f"  åŒä¹‰çŸ­è¯­æ•°: {phrases_in_groups}")
    print(f"  å†—ä½™ç‡: {redundancy_rate:.1%}")

    # æ˜¾ç¤ºå‰å‡ ä¸ªåŒä¹‰ç»„ç¤ºä¾‹
    if synonym_groups:
        print(f"\nğŸ“‹ åŒä¹‰ç»„ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰:")
        for i, group in enumerate(synonym_groups[:3], 1):
            print(f"\n  ç»„{i} ({len(group)}ä¸ªçŸ­è¯­):")
            for idx in group[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                print(f"    - {sample_phrases[idx]}")

    # 4. ç”Ÿæˆç»“æœ
    result = {
        'experiment': 'C',
        'name': 'åŒä¹‰å†—ä½™ç‡',
        'timestamp': datetime.now().isoformat(),
        'sample_size': sample_size,
        'synonym_groups_count': synonym_groups_count,
        'phrases_in_groups': phrases_in_groups,
        'redundancy_rate': redundancy_rate,
        'automation_note': 'æ­¤ç»“æœç”±è‡ªåŠ¨åŒ–è„šæœ¬ç”Ÿæˆï¼ŒåŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•è¯†åˆ«åŒä¹‰ç»„'
    }

    # 5. å†³ç­–é€»è¾‘
    if redundancy_rate < 0.10:
        result['recommendation'] = 'ok'
        result['recommendation_detail'] = f'å†—ä½™ç‡{redundancy_rate:.1%}å¯æ¥å—ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†'
    elif redundancy_rate > 0.20:
        result['recommendation'] = 'need_canonicalization'
        result['recommendation_detail'] = f'å†—ä½™ç‡{redundancy_rate:.1%}è¾ƒé«˜ï¼Œå»ºè®®å®æ–½è¯çº§è§„èŒƒåŒ–å»é‡'
    else:
        result['recommendation'] = 'moderate'
        result['recommendation_detail'] = f'å†—ä½™ç‡{redundancy_rate:.1%}ä¸­ç­‰ï¼Œå¯è€ƒè™‘è½»é‡çº§å»é‡'

    print(f"\n" + "="*70)
    print("å®éªŒCç»“æœ")
    print("="*70)
    print(f"æŠ½æ ·æ•°é‡: {sample_size:,}")
    print(f"åŒä¹‰ç»„æ•°: {synonym_groups_count}")
    print(f"å†—ä½™ç‡: {redundancy_rate:.1%}")
    print(f"\nåˆ¤æ–­: {result['recommendation']}")
    print(f"è¯¦æƒ…: {result['recommendation_detail']}")

    # 6. ä¿å­˜ç»“æœ
    output_dir = project_root / 'data' / 'phase0_results'
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / 'experiment_c_result.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return result


if __name__ == "__main__":
    try:
        result = run_experiment_c_auto()

        print("\nâœ… å®éªŒC (è‡ªåŠ¨åŒ–ç‰ˆæœ¬) å®Œæˆï¼")
        print(f"\nğŸ“Š å»ºè®®: {result['recommendation']}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å®éªŒæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
