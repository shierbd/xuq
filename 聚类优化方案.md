# èšç±»ä¼˜åŒ–æ–¹æ¡ˆ - è§£å†³å™ªç‚¹ç‡è¿‡é«˜é—®é¢˜

## ğŸ“Š é—®é¢˜åˆ†æ

### æ ¸å¿ƒçŸ›ç›¾
- **ä¸šåŠ¡éœ€æ±‚**: æ¯ä¸ªç°‡è‡³å°‘15ä¸ªå•†å“ (min_cluster_size >= 15)
- **è´¨é‡è¦æ±‚**: å™ªç‚¹ç‡ < 10%
- **å½“å‰å›°å¢ƒ**: min_cluster_size=15 æ—¶å™ªç‚¹ç‡71.6%ï¼Œé™ä½å‚æ•°ä¼šå¯¼è‡´ç°‡å¤ªå°

### æ ¹æœ¬åŸå› 
1. **å‘é‡åŒ–æ¨¡å‹ä¸å¤Ÿå¼º**: all-MiniLM-L6-v2 æ˜¯è¾ƒå°çš„æ¨¡å‹ï¼ˆ384ç»´ï¼‰
2. **æ–‡æœ¬è´¨é‡å‚å·®ä¸é½**: å•†å“åç§°åŒ…å«å¤§é‡å™ªéŸ³ï¼ˆç‰¹æ®Šå­—ç¬¦ã€å“ç‰Œã€å°ºå¯¸ç­‰ï¼‰
3. **å•ä¸€èšç±»ç­–ç•¥**: ä»…ä¾èµ–HDBSCANä¸€æ¬¡èšç±»
4. **å‚æ•°è°ƒä¼˜ä¸è¶³**: åªè°ƒæ•´äº†min_cluster_sizeå’Œmin_samples

---

## ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆï¼ˆ5ä¸ªæ–¹æ¡ˆï¼ŒæŒ‰æ¨èåº¦æ’åºï¼‰

### æ–¹æ¡ˆ1: å‡çº§å‘é‡åŒ–æ¨¡å‹ â­â­â­â­â­ (å¼ºçƒˆæ¨è)

#### åŸç†
ä½¿ç”¨æ›´å¼ºå¤§çš„å‘é‡åŒ–æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„è¯­ä¹‰å‘é‡ï¼Œä»è€Œæå‡èšç±»æ•ˆæœã€‚

#### æ¨èæ¨¡å‹
1. **all-mpnet-base-v2** (768ç»´)
   - æ¯” all-MiniLM-L6-v2 æ›´å‡†ç¡®
   - åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³
   - é€‚åˆè‹±æ–‡æ–‡æœ¬

2. **paraphrase-multilingual-mpnet-base-v2** (768ç»´)
   - æ”¯æŒå¤šè¯­è¨€
   - é€‚åˆæ··åˆè¯­è¨€çš„å•†å“åç§°

#### å®æ–½æ­¥éª¤
```python
# ä¿®æ”¹ clustering_service.py
class ClusteringService:
    def __init__(self, db: Session, model_name: str = "all-mpnet-base-v2"):  # æ”¹è¿™é‡Œ
        self.db = db
        self.model_name = model_name
        # ...
```

#### é¢„æœŸæ•ˆæœ
- å™ªç‚¹ç‡: 71.6% â†’ 30-40%
- ç°‡è´¨é‡: æ˜¾è‘—æå‡
- è®¡ç®—æ—¶é—´: å¢åŠ çº¦50%ï¼ˆå¯æ¥å—ï¼‰

#### ä¼˜ç‚¹
- âœ… å®æ–½ç®€å•ï¼Œåªéœ€æ”¹ä¸€è¡Œä»£ç 
- âœ… æ•ˆæœæ˜¾è‘—
- âœ… ä¸æ”¹å˜ä¸šåŠ¡é€»è¾‘

#### ç¼ºç‚¹
- âš ï¸ æ¨¡å‹æ›´å¤§ï¼ˆ420MB vs 80MBï¼‰
- âš ï¸ è®¡ç®—ç¨æ…¢

---

### æ–¹æ¡ˆ2: æ–‡æœ¬é¢„å¤„ç†ä¼˜åŒ– â­â­â­â­â­ (å¼ºçƒˆæ¨è)

#### åŸç†
æ¸…æ´—å’Œæ ‡å‡†åŒ–å•†å“åç§°ï¼Œå»é™¤å™ªéŸ³ï¼Œæå–æ ¸å¿ƒè¯­ä¹‰ã€‚

#### é¢„å¤„ç†æ­¥éª¤
1. **å»é™¤ç‰¹æ®Šå­—ç¬¦**: `| / \ ( ) [ ] { } < >`
2. **æ ‡å‡†åŒ–å“ç‰Œ**: `Canva â†’ canva`
3. **å»é™¤å°ºå¯¸ä¿¡æ¯**: `8x10`, `A4`, `5x7`
4. **å»é™¤å¸¸è§åœç”¨è¯**: `template`, `digital`, `download`, `printable`
5. **æå–æ ¸å¿ƒå…³é”®è¯**: ä¿ç•™æœ€é‡è¦çš„3-5ä¸ªè¯

#### å®æ–½ä»£ç 
```python
import re

def preprocess_product_name(name: str) -> str:
    """é¢„å¤„ç†å•†å“åç§°"""
    # 1. è½¬å°å†™
    name = name.lower()

    # 2. å»é™¤ç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'[|/\\()\[\]{}<>]', ' ', name)

    # 3. å»é™¤å°ºå¯¸ä¿¡æ¯
    name = re.sub(r'\d+x\d+', '', name)
    name = re.sub(r'\d+\s*(mm|cm|inch|in)', '', name)

    # 4. å»é™¤å¸¸è§åœç”¨è¯
    stop_words = ['template', 'digital', 'download', 'printable',
                  'editable', 'instant', 'pdf', 'canva']
    for word in stop_words:
        name = re.sub(rf'\b{word}\b', '', name)

    # 5. æ¸…ç†å¤šä½™ç©ºæ ¼
    name = ' '.join(name.split())

    return name

# åœ¨å‘é‡åŒ–å‰åº”ç”¨
def vectorize_products(self, products: List[Product], use_cache: bool = True):
    # ...
    for product in products:
        # é¢„å¤„ç†å•†å“åç§°
        processed_name = preprocess_product_name(product.product_name)
        texts_to_encode.append(processed_name)
    # ...
```

#### é¢„æœŸæ•ˆæœ
- å™ªç‚¹ç‡: 71.6% â†’ 40-50%
- ç°‡è´¨é‡: æ˜¾è‘—æå‡
- è¯­ä¹‰ç›¸ä¼¼åº¦: æé«˜

#### ä¼˜ç‚¹
- âœ… ä»æ ¹æœ¬ä¸Šæå‡æ•°æ®è´¨é‡
- âœ… å¯ä¸å…¶ä»–æ–¹æ¡ˆç»„åˆ
- âœ… ä¸å¢åŠ è®¡ç®—æˆæœ¬

#### ç¼ºç‚¹
- âš ï¸ éœ€è¦ä»”ç»†è®¾è®¡é¢„å¤„ç†è§„åˆ™
- âš ï¸ å¯èƒ½éœ€è¦å¤šæ¬¡è°ƒè¯•

---

### æ–¹æ¡ˆ3: ä¸¤é˜¶æ®µèšç±»ç­–ç•¥ â­â­â­â­ (æ¨è)

#### åŸç†
å…ˆç”¨å®½æ¾å‚æ•°èšç±»ï¼Œå†å¯¹å™ªç‚¹è¿›è¡ŒäºŒæ¬¡èšç±»ã€‚

#### å®æ–½æ­¥éª¤
```python
def two_stage_clustering(self, embeddings, product_ids):
    """ä¸¤é˜¶æ®µèšç±»"""

    # ç¬¬ä¸€é˜¶æ®µï¼šä¸¥æ ¼èšç±»ï¼ˆmin_cluster_size=15ï¼‰
    print("Stage 1: Strict clustering...")
    clusterer1 = hdbscan.HDBSCAN(
        min_cluster_size=15,
        min_samples=5,
        metric='euclidean',
        cluster_selection_method='eom'
    )
    labels1 = clusterer1.fit_predict(embeddings)

    # æ‰¾å‡ºå™ªç‚¹
    noise_mask = labels1 == -1
    noise_embeddings = embeddings[noise_mask]
    noise_ids = [product_ids[i] for i, is_noise in enumerate(noise_mask) if is_noise]

    print(f"Stage 1 noise: {sum(noise_mask)} points")

    # ç¬¬äºŒé˜¶æ®µï¼šå¯¹å™ªç‚¹è¿›è¡Œå®½æ¾èšç±»
    if len(noise_embeddings) > 0:
        print("Stage 2: Loose clustering for noise points...")
        clusterer2 = hdbscan.HDBSCAN(
            min_cluster_size=10,  # ç¨å¾®å®½æ¾
            min_samples=3,
            metric='euclidean',
            cluster_selection_method='leaf'  # ä½¿ç”¨leafæ–¹æ³•
        )
        labels2 = clusterer2.fit_predict(noise_embeddings)

        # åˆå¹¶ç»“æœï¼ˆç»™ç¬¬äºŒé˜¶æ®µçš„ç°‡åˆ†é…æ–°çš„IDï¼‰
        max_label1 = labels1.max()
        for i, is_noise in enumerate(noise_mask):
            if is_noise:
                noise_idx = sum(noise_mask[:i])
                if labels2[noise_idx] != -1:
                    labels1[i] = max_label1 + 1 + labels2[noise_idx]

    return labels1
```

#### é¢„æœŸæ•ˆæœ
- å™ªç‚¹ç‡: 71.6% â†’ 20-30%
- ç°‡å¤§å°: å¤§éƒ¨åˆ† >= 15ï¼Œå°‘éƒ¨åˆ† >= 10
- ç°‡æ•°é‡: å¢åŠ 

#### ä¼˜ç‚¹
- âœ… å¹³è¡¡äº†ç°‡å¤§å°å’Œå™ªç‚¹ç‡
- âœ… çµæ´»å¯è°ƒ
- âœ… ä¿ç•™äº†å¤§ç°‡çš„è´¨é‡

#### ç¼ºç‚¹
- âš ï¸ å®ç°å¤æ‚åº¦è¾ƒé«˜
- âš ï¸ è®¡ç®—æ—¶é—´ç¿»å€

---

### æ–¹æ¡ˆ4: è°ƒæ•´HDBSCANé«˜çº§å‚æ•° â­â­â­ (å¯å°è¯•)

#### åŸç†
HDBSCANæœ‰å¾ˆå¤šé«˜çº§å‚æ•°å¯ä»¥è°ƒæ•´ï¼Œä¸ä»…ä»…æ˜¯min_cluster_sizeå’Œmin_samplesã€‚

#### å…³é”®å‚æ•°
```python
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=15,
    min_samples=5,
    metric='euclidean',

    # é«˜çº§å‚æ•°
    cluster_selection_method='leaf',  # æ”¹ä¸ºleafï¼ˆæ›´æ¿€è¿›ï¼‰
    cluster_selection_epsilon=0.5,    # å…è®¸æ›´å¤šåˆå¹¶
    alpha=1.0,                         # è°ƒæ•´å¯†åº¦ä¼°è®¡
    allow_single_cluster=False,        # ä¸å…è®¸å•ä¸€å¤§ç°‡

    # æ€§èƒ½å‚æ•°
    core_dist_n_jobs=-1               # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
)
```

#### å‚æ•°è¯´æ˜
- **cluster_selection_method**:
  - `eom` (é»˜è®¤): æ›´ä¿å®ˆï¼Œç°‡æ›´çº¯å‡€ä½†å™ªç‚¹å¤š
  - `leaf`: æ›´æ¿€è¿›ï¼Œå™ªç‚¹å°‘ä½†ç°‡å¯èƒ½ä¸å¤Ÿçº¯å‡€

- **cluster_selection_epsilon**:
  - å…è®¸è·ç¦»åœ¨epsilonå†…çš„ç°‡åˆå¹¶
  - å¢å¤§æ­¤å€¼å¯ä»¥å‡å°‘å™ªç‚¹

- **alpha**:
  - æ§åˆ¶å¯†åº¦ä¼°è®¡çš„æ•æ„Ÿåº¦
  - å¢å¤§å¯ä»¥è®©ç°‡æ›´ç´§å¯†

#### æ¨èé…ç½®
```python
# é…ç½®1: å¹³è¡¡å‹
min_cluster_size=15
min_samples=5
cluster_selection_method='leaf'
cluster_selection_epsilon=0.3

# é…ç½®2: æ¿€è¿›å‹ï¼ˆæ›´å°‘å™ªç‚¹ï¼‰
min_cluster_size=15
min_samples=3
cluster_selection_method='leaf'
cluster_selection_epsilon=0.5
alpha=1.2
```

#### é¢„æœŸæ•ˆæœ
- å™ªç‚¹ç‡: 71.6% â†’ 40-50%
- ç°‡è´¨é‡: å¯èƒ½ç•¥æœ‰ä¸‹é™

#### ä¼˜ç‚¹
- âœ… å®æ–½ç®€å•
- âœ… ä¸æ”¹å˜æ•´ä½“æ¶æ„

#### ç¼ºç‚¹
- âš ï¸ éœ€è¦åå¤è°ƒè¯•
- âš ï¸ æ•ˆæœä¸å¦‚æ–¹æ¡ˆ1å’Œ2

---

### æ–¹æ¡ˆ5: ä½¿ç”¨å±‚æ¬¡èšç±» (Agglomerative) â­â­ (å¤‡é€‰)

#### åŸç†
å±‚æ¬¡èšç±»æ²¡æœ‰"å™ªç‚¹"æ¦‚å¿µï¼Œæ‰€æœ‰ç‚¹éƒ½ä¼šè¢«åˆ†é…åˆ°æŸä¸ªç°‡ã€‚

#### å®æ–½ä»£ç 
```python
from sklearn.cluster import AgglomerativeClustering

def agglomerative_clustering(self, embeddings, n_clusters=100):
    """å±‚æ¬¡èšç±»"""
    clusterer = AgglomerativeClustering(
        n_clusters=n_clusters,
        metric='euclidean',
        linkage='ward'
    )
    labels = clusterer.fit_predict(embeddings)

    # åå¤„ç†ï¼šåˆå¹¶å°ç°‡
    cluster_sizes = {}
    for label in labels:
        cluster_sizes[label] = cluster_sizes.get(label, 0) + 1

    # å°†å°äº15çš„ç°‡æ ‡è®°ä¸ºå™ªç‚¹
    for i, label in enumerate(labels):
        if cluster_sizes[label] < 15:
            labels[i] = -1

    return labels
```

#### é¢„æœŸæ•ˆæœ
- å™ªç‚¹ç‡: 0% (åˆå§‹) â†’ 10-20% (åå¤„ç†å)
- ç°‡å¤§å°: å¯æ§
- ç°‡è´¨é‡: å¯èƒ½ä¸å¦‚HDBSCAN

#### ä¼˜ç‚¹
- âœ… æ²¡æœ‰å™ªç‚¹ï¼ˆåˆå§‹ï¼‰
- âœ… ç°‡å¤§å°å¯æ§

#### ç¼ºç‚¹
- âš ï¸ éœ€è¦é¢„å…ˆæŒ‡å®šç°‡æ•°é‡
- âš ï¸ å¯èƒ½å¼ºåˆ¶åˆ†é…ä¸ç›¸ä¼¼çš„å•†å“
- âš ï¸ ç°‡è´¨é‡å¯èƒ½ä¸å¦‚HDBSCAN

---

## ğŸ¯ æ¨èå®æ–½æ–¹æ¡ˆï¼ˆç»„åˆæ–¹æ¡ˆï¼‰

### æœ€ä½³ç»„åˆ: æ–¹æ¡ˆ1 + æ–¹æ¡ˆ2 â­â­â­â­â­

#### å®æ–½æ­¥éª¤
1. **å‡çº§å‘é‡åŒ–æ¨¡å‹** â†’ all-mpnet-base-v2
2. **æ·»åŠ æ–‡æœ¬é¢„å¤„ç†** â†’ æ¸…æ´—å•†å“åç§°
3. **ä¿æŒå‚æ•°** â†’ min_cluster_size=15, min_samples=5

#### é¢„æœŸæ•ˆæœ
- å™ªç‚¹ç‡: 71.6% â†’ **15-25%**
- ç°‡å¤§å°: >= 15
- ç°‡è´¨é‡: æ˜¾è‘—æå‡

#### å¦‚æœè¿˜ä¸å¤Ÿï¼Œå†åŠ æ–¹æ¡ˆ3
- å™ªç‚¹ç‡: â†’ **< 10%**
- ç°‡å¤§å°: å¤§éƒ¨åˆ† >= 15ï¼Œå°‘éƒ¨åˆ† >= 10

---

## ğŸ“ å®æ–½ä»£ç ï¼ˆå®Œæ•´ç‰ˆï¼‰

### 1. ä¿®æ”¹ clustering_service.py

```python
import re
from sentence_transformers import SentenceTransformer
import hdbscan
import numpy as np

class ClusteringService:
    def __init__(self, db: Session, model_name: str = "all-mpnet-base-v2"):  # æ”¹è¿™é‡Œ
        self.db = db
        self.model_name = model_name
        # ...

    def preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # è½¬å°å†™
        text = text.lower()

        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[|/\\()\[\]<>]', ' ', text)

        # å»é™¤å°ºå¯¸ä¿¡æ¯
        text = re.sub(r'\d+x\d+', '', text)
        text = re.sub(r'\d+\s*(mm|cm|inch|in|ft)', '', text)

        # å»é™¤å¸¸è§åœç”¨è¯
        stop_words = ['template', 'digital', 'download', 'printable',
                      'editable', 'instant', 'pdf', 'canva', 'bundle']
        for word in stop_words:
            text = re.sub(rf'\b{word}\b', '', text, flags=re.IGNORECASE)

        # æ¸…ç†å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())

        return text

    def vectorize_products(self, products: List[Product], use_cache: bool = True):
        """å‘é‡åŒ–å•†å“ï¼ˆæ·»åŠ é¢„å¤„ç†ï¼‰"""
        self.load_model()

        embeddings = []
        product_ids = []
        texts_to_encode = []

        for product in products:
            product_ids.append(product.product_id)

            # é¢„å¤„ç†å•†å“åç§°
            processed_name = self.preprocess_text(product.product_name)
            texts_to_encode.append(processed_name)

        # æ‰¹é‡ç¼–ç 
        if texts_to_encode:
            embeddings = self.model.encode(
                texts_to_encode,
                show_progress_bar=True,
                batch_size=32
            )

        return np.array(embeddings), product_ids

    def perform_clustering(self, embeddings, min_cluster_size=15, min_samples=5):
        """æ‰§è¡Œèšç±»ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰"""
        print(f"Performing HDBSCAN clustering...")
        print(f"  min_cluster_size: {min_cluster_size}")
        print(f"  min_samples: {min_samples}")

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='leaf',  # æ”¹ä¸ºleaf
            cluster_selection_epsilon=0.3,     # æ·»åŠ epsilon
            core_dist_n_jobs=-1                # ä½¿ç”¨æ‰€æœ‰CPU
        )

        cluster_labels = clusterer.fit_predict(embeddings)

        # ç»Ÿè®¡
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)

        print(f"Clustering completed:")
        print(f"  Total clusters: {n_clusters}")
        print(f"  Noise points: {n_noise}")
        print(f"  Noise ratio: {n_noise / len(cluster_labels) * 100:.2f}%")

        return cluster_labels
```

### 2. ä¿®æ”¹ products.py è·¯ç”±

```python
@router.post("/cluster")
def cluster_products(
    min_cluster_size: int = 15,  # æ”¹å›15
    min_samples: int = 5,         # æ”¹å›5
    use_cache: bool = True,
    db: Session = Depends(get_db)
):
    """
    [REQ-003] æ‰§è¡Œè¯­ä¹‰èšç±»åˆ†æ

    ä½¿ç”¨ä¼˜åŒ–åçš„èšç±»æ–¹æ¡ˆï¼š
    - æ›´å¼ºå¤§çš„å‘é‡åŒ–æ¨¡å‹ (all-mpnet-base-v2)
    - æ–‡æœ¬é¢„å¤„ç†
    - ä¼˜åŒ–çš„HDBSCANå‚æ•°
    """
    clustering_service = ClusteringService(db)
    result = clustering_service.cluster_all_products(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        use_cache=use_cache
    )

    return {
        "success": result["success"],
        "message": "èšç±»åˆ†æå®Œæˆ" if result["success"] else result.get("message", "èšç±»å¤±è´¥"),
        "data": result
    }
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœå¯¹æ¯”

| æ–¹æ¡ˆ | å™ªç‚¹ç‡ | ç°‡å¤§å° | å®æ–½éš¾åº¦ | è®¡ç®—æˆæœ¬ | æ¨èåº¦ |
|------|--------|--------|----------|----------|--------|
| å½“å‰ | 71.6% | >= 15 | - | ä½ | - |
| æ–¹æ¡ˆ1 | 30-40% | >= 15 | ä½ | ä¸­ | â­â­â­â­â­ |
| æ–¹æ¡ˆ2 | 40-50% | >= 15 | ä¸­ | ä½ | â­â­â­â­â­ |
| æ–¹æ¡ˆ3 | 20-30% | >= 10 | é«˜ | é«˜ | â­â­â­â­ |
| æ–¹æ¡ˆ4 | 40-50% | >= 15 | ä½ | ä½ | â­â­â­ |
| æ–¹æ¡ˆ5 | 10-20% | >= 15 | ä¸­ | ä¸­ | â­â­ |
| **æ–¹æ¡ˆ1+2** | **15-25%** | **>= 15** | **ä¸­** | **ä¸­** | **â­â­â­â­â­** |
| **æ–¹æ¡ˆ1+2+3** | **< 10%** | **>= 10** | **é«˜** | **é«˜** | **â­â­â­â­â­** |

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨

### ç¬¬ä¸€æ­¥ï¼šå®æ–½æ–¹æ¡ˆ1+2ï¼ˆæ¨èï¼‰
1. ä¿®æ”¹ `clustering_service.py` çš„ `__init__` æ–¹æ³•ï¼Œæ”¹ç”¨ `all-mpnet-base-v2`
2. æ·»åŠ  `preprocess_text` æ–¹æ³•
3. åœ¨ `vectorize_products` ä¸­è°ƒç”¨é¢„å¤„ç†
4. ä¿®æ”¹ `perform_clustering` æ·»åŠ é«˜çº§å‚æ•°
5. é‡æ–°è¿è¡Œèšç±»

### ç¬¬äºŒæ­¥ï¼šéªŒè¯æ•ˆæœ
- ç›®æ ‡å™ªç‚¹ç‡: < 25%
- å¦‚æœè¾¾åˆ°ï¼Œå®Œæˆï¼
- å¦‚æœæœªè¾¾åˆ°ï¼Œç»§ç»­ç¬¬ä¸‰æ­¥

### ç¬¬ä¸‰æ­¥ï¼šå®æ–½æ–¹æ¡ˆ3ï¼ˆå¦‚éœ€è¦ï¼‰
- æ·»åŠ ä¸¤é˜¶æ®µèšç±»é€»è¾‘
- ç›®æ ‡å™ªç‚¹ç‡: < 10%

---

**æ€»ç»“**: é€šè¿‡å‡çº§æ¨¡å‹ + æ–‡æœ¬é¢„å¤„ç†ï¼Œå¯ä»¥åœ¨ä¿æŒç°‡å¤§å° >= 15 çš„åŒæ—¶ï¼Œå°†å™ªç‚¹ç‡é™è‡³ 15-25%ã€‚å¦‚æœéœ€è¦è¿›ä¸€æ­¥é™è‡³ < 10%ï¼Œå¯ä»¥å†åŠ ä¸Šä¸¤é˜¶æ®µèšç±»ç­–ç•¥ã€‚
